{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nl_sql_pairs(filepath, splits, with_dbs=False):\n",
    "    \"\"\"Gets pairs of natural language and corresponding gold SQL for Michigan.\n",
    "\n",
    "    TODO: This is Google code. Add LICENSE.\n",
    "\n",
    "    From the XSP codebase.\n",
    "    \"\"\"\n",
    "    with open(filepath) as infile:\n",
    "        data = json.load(infile)\n",
    "\n",
    "    pairs = list()\n",
    "\n",
    "    tag = '[' + filepath.split('/')[-1].split('.')[0] + ']'\n",
    "    print('Getting examples with tag ' + tag)\n",
    "\n",
    "    # The UMichigan data is split by anonymized queries, where values are\n",
    "    # anonymized but table/column names are not. However, our experiments are\n",
    "    # performed on the original splits of the data.\n",
    "    for query in data:\n",
    "        # Take the first SQL query only. From their Github documentation:\n",
    "        # \"Note - we only use the first query, but retain the variants for\n",
    "        #  completeness\"\n",
    "        anonymized_sql = query['sql'][0]\n",
    "\n",
    "        # It's also associated with a number of natural language examples, which\n",
    "        # also contain anonymous tokens. Save the de-anonymized utterance and query.\n",
    "        for example in query['sentences']:\n",
    "            if example['question-split'] not in splits:\n",
    "                continue\n",
    "\n",
    "            nl = example['text']\n",
    "            sql = anonymized_sql\n",
    "\n",
    "            # Go through the anonymized values and replace them in both the natural\n",
    "            # language and the SQL.\n",
    "            #\n",
    "            # It's very important to sort these in descending order. If one is a\n",
    "            # substring of the other, it shouldn't be replaced first lest it ruin the\n",
    "            # replacement of the superstring.\n",
    "            for variable_name, value in sorted(\n",
    "                    example['variables'].items(), key=lambda x: len(x[0]), reverse=True):\n",
    "                if not value:\n",
    "                    # TODO(alanesuhr) While the Michigan repo says to use a - here, the\n",
    "                    # thing that works is using a % and replacing = with LIKE.\n",
    "                    #\n",
    "                    # It's possible that I should remove such clauses from the SQL, as\n",
    "                    # long as they lead to the same table result. They don't align well\n",
    "                    # to the natural language at least.\n",
    "                    #\n",
    "                    # See: https://github.com/jkkummerfeld/text2sql-data/tree/master/data\n",
    "                    value = '%'\n",
    "\n",
    "                nl = nl.replace(variable_name, value)\n",
    "                sql = sql.replace(variable_name, value)\n",
    "\n",
    "            # In the case that we replaced an empty anonymized value with %, make it\n",
    "            # compilable new allowing equality with any string.\n",
    "            sql = sql.replace('= \"%\"', 'LIKE \"%\"')\n",
    "            nl = nl.lower()\n",
    "            if with_dbs:\n",
    "                pairs.append((nl, sql, example['table-id']))\n",
    "            else:\n",
    "                pairs.append((nl, sql))\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../../language/language/xsp/data'\n",
    "output_dir = '../../../featurestorage/data/spider-20200607'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'geography',\n",
       " 'query': \"SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX ( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = 'arizona' ) AND CITYalias0.STATE_NAME = 'arizona' ;\",\n",
       " 'query_toks': ['SELECT',\n",
       "  'CITYalias0.CITY_NAME',\n",
       "  'FROM',\n",
       "  'CITY',\n",
       "  'AS',\n",
       "  'CITYalias0',\n",
       "  'WHERE',\n",
       "  'CITYalias0.POPULATION',\n",
       "  '=',\n",
       "  '(',\n",
       "  'SELECT',\n",
       "  'MAX',\n",
       "  '(',\n",
       "  'CITYalias1.POPULATION',\n",
       "  ')',\n",
       "  'FROM',\n",
       "  'CITY',\n",
       "  'AS',\n",
       "  'CITYalias1',\n",
       "  'WHERE',\n",
       "  'CITYalias1.STATE_NAME',\n",
       "  '=',\n",
       "  \"'arizona'\",\n",
       "  ')',\n",
       "  'AND',\n",
       "  'CITYalias0.STATE_NAME',\n",
       "  '=',\n",
       "  \"'arizona'\",\n",
       "  ';'],\n",
       " 'question': 'what is the biggest city in arizona',\n",
       " 'question_toks': ['what', 'is', 'the', 'biggest', 'city', 'in', 'arizona'],\n",
       " 'sql': {'from': {'table_units': [['table_unit', 1]], 'conds': []},\n",
       "  'select': [False, [[0, [0, [0, 3, False], None]]]],\n",
       "  'where': [],\n",
       "  'groupBy': [],\n",
       "  'having': [],\n",
       "  'orderBy': [],\n",
       "  'limit': None,\n",
       "  'intersect': None,\n",
       "  'union': None,\n",
       "  'except': None}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('../../../featurestorage/data/spider-20200607/geography_dev.json','r') as f:\n",
    "    example = json.load(f)[0]\n",
    "    example['sql']['where'] = []\n",
    "    display(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting examples with tag [geoquery]\n"
     ]
    }
   ],
   "source": [
    "pairs = get_nl_sql_pairs(os.path.join(data_dir,'geoquery/geoquery.json'), ['train','dev'])\n",
    "items = []\n",
    "for question, query in pairs:\n",
    "    items.append({\n",
    "        'db_id': 'geography',\n",
    "        'query': query,\n",
    "        'query_toks': query.split(),\n",
    "        'sql': example['sql'],\n",
    "        'question': question,\n",
    "        'question_toks': question.split()\n",
    "    })\n",
    "with open(os.path.join(output_dir,'geography_origin.json'), 'w') as f:\n",
    "    json.dump(items, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting examples with tag [atis]\n"
     ]
    }
   ],
   "source": [
    "pairs = get_nl_sql_pairs(os.path.join(data_dir,'atis/atis.json'), ['dev'])\n",
    "items = []\n",
    "for question, query in pairs:\n",
    "    items.append({\n",
    "        'db_id': 'atis',\n",
    "        'query': query,\n",
    "        'query_toks': query.split(),\n",
    "        'sql': example['sql'],\n",
    "        'question': question,\n",
    "        'question_toks': question.split()\n",
    "    })\n",
    "with open(os.path.join(output_dir,'atis_origin.json'), 'w') as f:\n",
    "    json.dump(items, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting examples with tag [restaurants]\n",
      "Getting examples with tag [academic]\n",
      "Getting examples with tag [yelp]\n",
      "Getting examples with tag [imdb]\n"
     ]
    }
   ],
   "source": [
    "for db_name in ['restaurants','academic','yelp','imdb']:\n",
    "    pairs = get_nl_sql_pairs(os.path.join(data_dir,f'{db_name}/{db_name}.json'), [str(x) for x in range(10)])\n",
    "    items = []\n",
    "    for question, query in pairs:\n",
    "        items.append({\n",
    "            'db_id': db_name,\n",
    "            'query': query,\n",
    "            'query_toks': query.split(),\n",
    "            'sql': example['sql'],\n",
    "            'question': question,\n",
    "            'question_toks': question.split()\n",
    "        })\n",
    "    with open(os.path.join(output_dir,f'{db_name}_origin.json'), 'w') as f:\n",
    "        json.dump(items, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting examples with tag [scholar]\n",
      "Getting examples with tag [advising]\n"
     ]
    }
   ],
   "source": [
    "for db_name in ['scholar','advising']:\n",
    "    pairs = get_nl_sql_pairs(os.path.join(data_dir,f'{db_name}/{db_name}.json'), ['train','dev'])\n",
    "    items = []\n",
    "    for question, query in pairs:\n",
    "        items.append({\n",
    "            'db_id': db_name,\n",
    "            'query': query,\n",
    "            'query_toks': query.split(),\n",
    "            'sql': example['sql'],\n",
    "            'question': question,\n",
    "            'question_toks': question.split()\n",
    "        })\n",
    "    with open(os.path.join(output_dir,f'{db_name}_origin.json'), 'w') as f:\n",
    "        json.dump(items, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_results = [\n",
    "    \"final_nocvlink_bert_large_bs24_decmin10_spideronly_nocolvaluev1_nomaskcol_noencaction_0_0_lr7.44e-4_seed1_warmup5k-step40000\",\n",
    "#     \"final_nocvlink_bert_large_bs24_decmin10_spideronly_nocolvaluev1_nomaskcol_noencaction_0_0_lr7.44e-4_seed3_warmup5k-step40000\",\n",
    "#     \"final_nocvlink_bert_large_bs24_decmin10_spideronly_nocolvaluev1_nomaskcol_noencaction_0_0_lr7.44e-4_seed5_warmup5k-step40000\",\n",
    "#     \"final_nocvlink_neg1_1-1-1_anony5_bs24_decmin10_spideronly_nocolvaluev1_nomaskcol_noencaction_0_0_lr3e-4_seed1_warmup10k-step40000\",\n",
    "#     \"final_nocvlink_neg1_1-1-1_anony5_bs24_decmin10_spideronly_nocolvaluev1_nomaskcol_noencaction_0_0_lr3e-4_seed3_warmup10k-step40000\",\n",
    "#     \"final_nocvlink_neg1_1-1-1_anony5_bs24_decmin10_spideronly_nocolvaluev1_nomaskcol_noencaction_0_0_lr3e-4_seed5_warmup10k-step40000\",\n",
    "#     \"final_nocvlink_unsupervised_neg1_1-1-1_anony5_bs24_decmin10_spideronly_nocolvaluev1_nomaskcol_noencaction_0_0_lr3e-4_seed1_warmup10k-step40000\",\n",
    "#     \"final_nocvlink_unsupervised_neg1_1-1-1_anony5_bs24_decmin10_spideronly_nocolvaluev1_nomaskcol_noencaction_0_0_lr3e-4_seed3_warmup10k-step40000\",\n",
    "#     \"final_nocvlink_unsupervised_neg1_1-1-1_anony5_bs24_decmin10_spideronly_nocolvaluev1_nomaskcol_noencaction_0_0_lr3e-4_seed5_warmup10k-step40000\",\n",
    "#     \"final_bert_large_bs24_decmin10_spideronly_nocolvaluev1_nomaskcol_noencaction_0_0_lr7.44e-4_seed1_warmup5k-step40000\",\n",
    "#     \"final_bert_large_bs24_decmin10_spideronly_nocolvaluev1_nomaskcol_noencaction_0_0_lr7.44e-4_seed3_warmup5k-step40000\",\n",
    "#     \"final_bert_large_bs24_decmin10_spideronly_nocolvaluev1_nomaskcol_noencaction_0_0_lr7.44e-4_seed5_warmup5k-step40000\",\n",
    "#     \"final_neg1_1-1-1_anony5_bs24_decmin10_spideronly_nocolvaluev1_nomaskcol_noencaction_0_0_lr3e-4_seed1_warmup5k-step40000\",\n",
    "#     \"final_neg1_1-1-1_anony5_bs24_decmin10_spideronly_nocolvaluev1_nomaskcol_noencaction_0_0_lr3e-4_seed3_warmup5k-step40000\",\n",
    "#     \"final_neg1_1-1-1_anony5_bs24_decmin10_spideronly_nocolvaluev1_nomaskcol_noencaction_0_0_lr3e-4_seed5_warmup5k-step40000\",\n",
    "#     \"final_unsupervised_neg1_1-1-1_anony5_bs24_decmin10_spideronly_nocolvaluev1_nomaskcol_noencaction_0_0_lr7.44e-4_seed1_warmup10k-step40000\",\n",
    "#     \"final_unsupervised_neg1_1-1-1_anony5_bs24_decmin10_spideronly_nocolvaluev1_nomaskcol_noencaction_0_0_lr7.44e-4_seed3_warmup10k-step40000\",\n",
    "#     \"final_unsupervised_neg1_1-1-1_anony5_bs24_decmin10_spideronly_nocolvaluev1_nomaskcol_noencaction_0_0_lr7.44e-4_seed5_warmup10k-step40000\"\n",
    "]\n",
    "# for inferred_dir in inferred_results:\n",
    "#     os.mkdir(os.path.join('../../../language/language/xsp/output',inferred_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_predictions(origs, inferreds, db_path):\n",
    "    predictions = []\n",
    "    for inferred in inferreds:\n",
    "        orig = origs[inferred['index']]\n",
    "        predictions.append({\n",
    "            'predictions': [x['inferred_code'] for x in inferred['beams']],\n",
    "            'scores': [x['score'] for x in inferred['beams']],\n",
    "            'database_path': os.path.join(db_path,f\"{orig['db_id']}/{orig['db_id']}.sqlite\"),\n",
    "            'gold': orig['query'],\n",
    "            'utterance': orig['question']\n",
    "        })\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = '/datadrive/xiaden/workspace/featurestorage/data/spider-20200607/database/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geo\n",
      "598\n",
      "imdb\n",
      "131\n",
      "atis\n",
      "486\n",
      "restaurants\n",
      "378\n",
      "yelp\n",
      "128\n",
      "advising\n",
      "2858\n",
      "academic\n",
      "196\n",
      "scholar\n",
      "599\n"
     ]
    }
   ],
   "source": [
    "predictions = {}\n",
    "for inferred_dir in inferred_results:\n",
    "    try:\n",
    "        print('geo')\n",
    "        with open(os.path.join('../../../NL2CodeOverData/logdirs',inferred_dir,'val_origin_geography.infer'),'r') as f1,\\\n",
    "            open('../../../featurestorage/data/spider-20200607/geography_origin.json','r') as f2:\n",
    "            origs = json.load(f2)\n",
    "            inferreds = []\n",
    "            for line in f1:\n",
    "                inferreds.append(json.loads(line.strip()))\n",
    "            predictions[inferred_dir] = convert_predictions(origs, inferreds, db_path)\n",
    "            print(len(predictions[inferred_dir]))\n",
    "            with open(os.path.join('../../../language/language/xsp/output',inferred_dir,'geo.infer'),'w') as f:\n",
    "                json.dump(predictions[inferred_dir],f)\n",
    "    except:\n",
    "        print(inferred_dir)\n",
    "\n",
    "    try:\n",
    "        print('imdb')\n",
    "        with open(os.path.join('../../../NL2CodeOverData/logdirs',inferred_dir,'val_origin_imdb.infer'),'r') as f1,\\\n",
    "            open('../../../featurestorage/data/spider-20200607/imdb_origin.json','r') as f2:\n",
    "            origs = json.load(f2)\n",
    "            inferreds = []\n",
    "            for line in f1:\n",
    "                inferreds.append(json.loads(line.strip()))\n",
    "            predictions[inferred_dir] = convert_predictions(origs, inferreds, db_path)\n",
    "            print(len(predictions[inferred_dir]))\n",
    "            with open(os.path.join('../../../language/language/xsp/output',inferred_dir,'imdb.infer'),'w') as f:\n",
    "                json.dump(predictions[inferred_dir],f)\n",
    "    except:\n",
    "        print(inferred_dir)    \n",
    "\n",
    "    try:\n",
    "        print('atis')\n",
    "        with open(os.path.join('../../../NL2CodeOverData/logdirs',inferred_dir,'val_origin_atis.infer'),'r') as f1,\\\n",
    "            open('../../../featurestorage/data/spider-20200607/atis_origin.json','r') as f2:\n",
    "            origs = json.load(f2)\n",
    "            inferreds = []\n",
    "            for line in f1:\n",
    "                inferreds.append(json.loads(line.strip()))\n",
    "            predictions[inferred_dir] = convert_predictions(origs, inferreds, db_path)\n",
    "            print(len(predictions[inferred_dir]))\n",
    "            with open(os.path.join('../../../language/language/xsp/output',inferred_dir,'atis.infer'),'w') as f:\n",
    "                json.dump(predictions[inferred_dir],f)\n",
    "    except:\n",
    "        print(inferred_dir)\n",
    "\n",
    "    try:\n",
    "        print('restaurants')\n",
    "        with open(os.path.join('../../../NL2CodeOverData/logdirs',inferred_dir,'val_origin_restaurants.infer'),'r') as f1,\\\n",
    "            open('../../../featurestorage/data/spider-20200607/restaurants_origin.json','r') as f2:\n",
    "            origs = json.load(f2)\n",
    "            inferreds = []\n",
    "            for line in f1:\n",
    "                inferreds.append(json.loads(line.strip()))\n",
    "            predictions[inferred_dir] = convert_predictions(origs, inferreds, db_path)\n",
    "            print(len(predictions[inferred_dir]))\n",
    "            with open(os.path.join('../../../language/language/xsp/output',inferred_dir,'restaurants.infer'),'w') as f:\n",
    "                json.dump(predictions[inferred_dir],f)\n",
    "    except:\n",
    "        print(inferred_dir)\n",
    "    try:\n",
    "        print('yelp')\n",
    "        with open(os.path.join('../../../NL2CodeOverData/logdirs',inferred_dir,'val_origin_yelp.infer'),'r') as f1,\\\n",
    "            open('../../../featurestorage/data/spider-20200607/yelp_origin.json','r') as f2:\n",
    "            origs = json.load(f2)\n",
    "            inferreds = []\n",
    "            for line in f1:\n",
    "                inferreds.append(json.loads(line.strip()))\n",
    "            predictions[inferred_dir] = convert_predictions(origs, inferreds, db_path)\n",
    "            print(len(predictions[inferred_dir]))\n",
    "            with open(os.path.join('../../../language/language/xsp/output',inferred_dir,'yelp.infer'),'w') as f:\n",
    "                json.dump(predictions[inferred_dir],f)\n",
    "    except:\n",
    "        print(inferred_dir)\n",
    "\n",
    "    try:\n",
    "        print('advising')\n",
    "        with open(os.path.join('../../../NL2CodeOverData/logdirs',inferred_dir,'val_origin_advising.infer'),'r') as f1,\\\n",
    "            open('../../../featurestorage/data/spider-20200607/advising_origin.json','r') as f2:\n",
    "            origs = json.load(f2)\n",
    "            inferreds = []\n",
    "            for line in f1:\n",
    "                inferreds.append(json.loads(line.strip()))\n",
    "            predictions[inferred_dir] = convert_predictions(origs, inferreds, db_path)\n",
    "            print(len(predictions[inferred_dir]))\n",
    "            with open(os.path.join('../../../language/language/xsp/output',inferred_dir,'advising.infer'),'w') as f:\n",
    "                json.dump(predictions[inferred_dir],f)\n",
    "    except:\n",
    "        print(inferred_dir)\n",
    "\n",
    "    try:\n",
    "        print('academic')\n",
    "        with open(os.path.join('../../../NL2CodeOverData/logdirs',inferred_dir,'val_origin_academic.infer'),'r') as f1,\\\n",
    "            open('../../../featurestorage/data/spider-20200607/academic_origin.json','r') as f2:\n",
    "            origs = json.load(f2)\n",
    "            inferreds = []\n",
    "            for line in f1:\n",
    "                inferreds.append(json.loads(line.strip()))\n",
    "            predictions[inferred_dir] = convert_predictions(origs, inferreds, db_path)\n",
    "            print(len(predictions[inferred_dir]))\n",
    "            with open(os.path.join('../../../language/language/xsp/output',inferred_dir,'academic.infer'),'w') as f:\n",
    "                json.dump(predictions[inferred_dir],f)\n",
    "    except:\n",
    "        print(inferred_dir)\n",
    "\n",
    "    try:\n",
    "        print('scholar')\n",
    "        with open(os.path.join('../../../NL2CodeOverData/logdirs',inferred_dir,'val_origin_scholar.infer'),'r') as f1,\\\n",
    "            open('../../../featurestorage/data/spider-20200607/scholar_origin.json','r') as f2:\n",
    "            origs = json.load(f2)\n",
    "            inferreds = []\n",
    "            for line in f1:\n",
    "                inferreds.append(json.loads(line.strip()))\n",
    "            predictions[inferred_dir] = convert_predictions(origs, inferreds, db_path)\n",
    "            print(len(predictions[inferred_dir]))\n",
    "            with open(os.path.join('../../../language/language/xsp/output',inferred_dir,'scholar.infer'),'w') as f:\n",
    "                json.dump(predictions[inferred_dir],f)\n",
    "    except:\n",
    "        print(inferred_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599\n"
     ]
    }
   ],
   "source": [
    "inferred_dir = \"final_nocvlink_bert_large_bs24_decmin10_spideronly_nocolvaluev1_nomaskcol_noencaction_0_0_lr7.44e-4_seed1_warmup5k-step40000\"\n",
    "with open(os.path.join('../../../NL2CodeOverData/logdirs',inferred_dir,'val_origin_scholar.infer'),'r') as f1,\\\n",
    "    open('../../../featurestorage/data/spider-20200607/scholar_origin.json','r') as f2:\n",
    "    origs = json.load(f2)\n",
    "    inferreds = []\n",
    "    for line in f1:\n",
    "        inferreds.append(json.loads(line.strip()))\n",
    "    predictions[inferred_dir] = convert_predictions(origs, inferreds, db_path)\n",
    "    print(len(predictions[inferred_dir]))\n",
    "    with open(os.path.join('../../../language/language/xsp/output',inferred_dir,'scholar.infer'),'w') as f:\n",
    "        json.dump(predictions[inferred_dir],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
