{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data from UMich, including:\n",
    "- ATIS\n",
    "- GEO\n",
    "- Restaurants\n",
    "- Scholar\n",
    "- Academic\n",
    "- IMDB\n",
    "- Yelp\n",
    "- Advising\n",
    "\n",
    "Also create new subset from spider dev, removing column mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import sqlite3\n",
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions for reading the standardised text2sql datasets presented in\n",
    "`\"Improving Text to SQL Evaluation Methodology\" <https://arxiv.org/abs/1806.09029>`_\n",
    "\"\"\"\n",
    "# from allennlp\n",
    "from typing import List, Dict, NamedTuple, Iterable, Tuple, Set\n",
    "from collections import defaultdict\n",
    "\n",
    "class SqlData(NamedTuple):\n",
    "    \"\"\"\n",
    "    A utility class for reading in text2sql data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : ``List[str]``\n",
    "        The tokens in the text of the query.\n",
    "    text_with_variables : ``List[str]``\n",
    "        The tokens in the text of the query with variables\n",
    "        mapped to table names/abstract variables.\n",
    "    variable_tags : ``List[str]``\n",
    "        Labels for each word in ``text`` which correspond to\n",
    "        which variable in the sql the token is linked to. \"O\"\n",
    "        is used to denote no tag.\n",
    "    sql : ``List[str]``\n",
    "        The tokens in the SQL query which corresponds to the text.\n",
    "    text_variables : ``Dict[str, str]``\n",
    "        A dictionary of variables associated with the text, e.g. {\"city_name0\": \"san fransisco\"}\n",
    "    sql_variables : ``Dict[str, Dict[str, str]]``\n",
    "        A dictionary of variables and column references associated with the sql query.\n",
    "    \"\"\"\n",
    "\n",
    "    text: List[str]\n",
    "    text_with_variables: List[str]\n",
    "    variable_tags: List[str]\n",
    "    sql: List[str]\n",
    "    text_variables: Dict[str, str]\n",
    "    sql_variables: Dict[str, Dict[str, str]]\n",
    "\n",
    "\n",
    "class TableColumn(NamedTuple):\n",
    "    name: str\n",
    "    column_type: str\n",
    "    is_primary_key: bool\n",
    "\n",
    "\n",
    "def column_has_string_type(column: TableColumn) -> bool:\n",
    "    if \"varchar\" in column.column_type:\n",
    "        return True\n",
    "    elif column.column_type == \"text\":\n",
    "        return True\n",
    "    elif column.column_type == \"longtext\":\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def column_has_numeric_type(column: TableColumn) -> bool:\n",
    "    if \"int\" in column.column_type:\n",
    "        return True\n",
    "    elif \"float\" in column.column_type:\n",
    "        return True\n",
    "    elif \"double\" in column.column_type:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def replace_variables(\n",
    "    sentence: List[str], sentence_variables: Dict[str, str]\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Replaces abstract variables in text with their concrete counterparts.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    for token in sentence:\n",
    "        if token not in sentence_variables:\n",
    "            tokens.append(token)\n",
    "            tags.append(\"O\")\n",
    "        else:\n",
    "            for word in sentence_variables[token].split():\n",
    "                tokens.append(word)\n",
    "                tags.append(token)\n",
    "    return tokens, tags\n",
    "\n",
    "\n",
    "def split_table_and_column_names(table: str) -> Iterable[str]:\n",
    "    partitioned = [x for x in table.partition(\".\") if x != \"\"]\n",
    "    # Avoid splitting decimal strings.\n",
    "    if partitioned[0].isnumeric() and partitioned[-1].isnumeric():\n",
    "        return [table]\n",
    "    return partitioned\n",
    "\n",
    "\n",
    "def clean_and_split_sql(sql: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Cleans up and unifies a SQL query. This involves unifying quoted strings\n",
    "    and splitting brackets which aren't formatted consistently in the data.\n",
    "    \"\"\"\n",
    "    sql_tokens: List[str] = []\n",
    "    for token in sql.strip().split():\n",
    "        token = token.replace('\"', \"'\")#.replace(\"%\", \"\")\n",
    "        if token.endswith(\"(\") and len(token) > 1:\n",
    "            sql_tokens.extend(split_table_and_column_names(token[:-1]))\n",
    "            sql_tokens.extend(split_table_and_column_names(token[-1]))\n",
    "        else:\n",
    "            sql_tokens.extend(split_table_and_column_names(token))\n",
    "    return sql_tokens\n",
    "\n",
    "\n",
    "def resolve_primary_keys_in_schema(\n",
    "    sql_tokens: List[str], schema: Dict[str, List[TableColumn]]\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Some examples in the text2sql datasets use ID as a column reference to the\n",
    "    column of a table which has a primary key. This causes problems if you are trying\n",
    "    to constrain a grammar to only produce the column names directly, because you don't\n",
    "    know what ID refers to. So instead of dealing with that, we just replace it.\n",
    "    \"\"\"\n",
    "    primary_keys_for_tables = {\n",
    "        name: max(columns, key=lambda x: x.is_primary_key).name for name, columns in schema.items()\n",
    "    }\n",
    "    resolved_tokens = []\n",
    "    for i, token in enumerate(sql_tokens):\n",
    "        if i > 2:\n",
    "            table_name = sql_tokens[i - 2]\n",
    "            if token == \"ID\" and table_name in primary_keys_for_tables.keys():\n",
    "                token = primary_keys_for_tables[table_name]\n",
    "        resolved_tokens.append(token)\n",
    "    return resolved_tokens\n",
    "\n",
    "\n",
    "def clean_unneeded_aliases(sql_tokens: List[str]) -> List[str]:\n",
    "\n",
    "    unneeded_aliases = {}\n",
    "    previous_token = sql_tokens[0]\n",
    "    for (token, next_token) in zip(sql_tokens[1:-1], sql_tokens[2:]):\n",
    "        if token == \"AS\" and previous_token is not None:\n",
    "            # Check to see if the table name without the alias\n",
    "            # is the same.\n",
    "            table_name = next_token[:-6]\n",
    "            if table_name == previous_token:\n",
    "                # If so, store the mapping as a replacement.\n",
    "                unneeded_aliases[next_token] = previous_token\n",
    "\n",
    "        previous_token = token\n",
    "\n",
    "    dealiased_tokens: List[str] = []\n",
    "    for token in sql_tokens:\n",
    "        new_token = unneeded_aliases.get(token, None)\n",
    "\n",
    "        if new_token is not None and dealiased_tokens[-1] == \"AS\":\n",
    "            dealiased_tokens.pop()\n",
    "            continue\n",
    "        elif new_token is None:\n",
    "            new_token = token\n",
    "\n",
    "        dealiased_tokens.append(new_token)\n",
    "\n",
    "    return dealiased_tokens\n",
    "\n",
    "\n",
    "def read_dataset_schema(schema_path: str) -> Dict[str, List[TableColumn]]:\n",
    "    \"\"\"\n",
    "    Reads a schema from the text2sql data, returning a dictionary\n",
    "    mapping table names to their columns and respective types.\n",
    "    This handles columns in an arbitrary order and also allows\n",
    "    either ``{Table, Field}`` or ``{Table, Field} Name`` as headers,\n",
    "    because both appear in the data. It also uppercases table and\n",
    "    column names if they are not already uppercase.\n",
    "    Parameters\n",
    "    ----------\n",
    "    schema_path : ``str``, required.\n",
    "        The path to the csv schema.\n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary mapping table names to typed columns.\n",
    "    \"\"\"\n",
    "    schema: Dict[str, List[TableColumn]] = defaultdict(list)\n",
    "    for i, line in enumerate(open(schema_path, \"r\")):\n",
    "        if i == 0:\n",
    "            header = [x.strip() for x in line.split(\",\")]\n",
    "        elif line[0] == \"-\":\n",
    "            continue\n",
    "        else:\n",
    "            data = {key: value for key, value in zip(header, [x.strip() for x in line.split(\",\")])}\n",
    "\n",
    "            table = data.get(\"Table Name\", None) or data.get(\"Table\")\n",
    "            column = data.get(\"Field Name\", None) or data.get(\"Field\")\n",
    "            is_primary_key = data.get(\"Primary Key\",data.get(\"Is Primary Key\")) == \"y\"\n",
    "            schema[table.upper()].append(TableColumn(column.upper(), data[\"Type\"], is_primary_key))\n",
    "\n",
    "    return {**schema}\n",
    "\n",
    "\n",
    "def process_sql_data(\n",
    "    data: List[Dict],\n",
    "    use_all_sql: bool = False,\n",
    "    use_all_queries: bool = False,\n",
    "    remove_unneeded_aliases: bool = False,\n",
    "    schema: Dict[str, List[TableColumn]] = None,\n",
    ") -> Iterable[SqlData]:\n",
    "    \"\"\"\n",
    "    A utility function for reading in text2sql data. The blob is\n",
    "    the result of loading the json from a file produced by the script\n",
    "    ``scripts/reformat_text2sql_data.py``.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : ``JsonDict``\n",
    "    use_all_sql : ``bool``, optional (default = False)\n",
    "        Whether to use all of the sql queries which have identical semantics,\n",
    "        or whether to just use the first one.\n",
    "    use_all_queries : ``bool``, (default = False)\n",
    "        Whether or not to enforce query sentence uniqueness. If false,\n",
    "        duplicated queries will occur in the dataset as separate instances,\n",
    "        as for a given SQL query, not only are there multiple queries with\n",
    "        the same template, but there are also duplicate queries.\n",
    "    remove_unneeded_aliases : ``bool``, (default = False)\n",
    "        The text2sql data by default creates alias names for `all` tables,\n",
    "        regardless of whether the table is derived or if it is identical to\n",
    "        the original (e.g SELECT TABLEalias0.COLUMN FROM TABLE AS TABLEalias0).\n",
    "        This is not necessary and makes the action sequence and grammar manipulation\n",
    "        much harder in a grammar based decoder. Note that this does not\n",
    "        remove aliases which are legitimately required, such as when a new\n",
    "        table is formed by performing operations on the original table.\n",
    "    schema : ``Dict[str, List[TableColumn]]``, optional, (default = None)\n",
    "        A schema to resolve primary keys against. Converts 'ID' column names\n",
    "        to their actual name with respect to the Primary Key for the table\n",
    "        in the schema.\n",
    "    \"\"\"\n",
    "    for example in data:\n",
    "        seen_sentences: Set[str] = set()\n",
    "        for sent_info in example[\"sentences\"]:\n",
    "            # Loop over the different sql statements with \"equivalent\" semantics\n",
    "            for sql in example[\"sql\"]:\n",
    "                text_with_variables = sent_info[\"text\"].strip().split()\n",
    "                text_vars = sent_info[\"variables\"]\n",
    "\n",
    "                query_tokens, tags = replace_variables(text_with_variables, text_vars)\n",
    "                if not use_all_queries:\n",
    "                    key = \" \".join(query_tokens)\n",
    "                    if key in seen_sentences:\n",
    "                        continue\n",
    "                    else:\n",
    "                        seen_sentences.add(key)\n",
    "\n",
    "                sql_tokens = clean_and_split_sql(sql)\n",
    "                if remove_unneeded_aliases:\n",
    "                    sql_tokens = clean_unneeded_aliases(sql_tokens)\n",
    "                if schema is not None:\n",
    "                    sql_tokens = resolve_primary_keys_in_schema(sql_tokens, schema)\n",
    "\n",
    "                sql_variables = {}\n",
    "                for variable in example[\"variables\"]:\n",
    "                    sql_variables[variable[\"name\"]] = {\n",
    "                        \"text\": variable[\"example\"],\n",
    "                        \"type\": variable[\"type\"],\n",
    "                    }\n",
    "\n",
    "                sql_data = SqlData(\n",
    "                    text=query_tokens,\n",
    "                    text_with_variables=text_with_variables,\n",
    "                    variable_tags=tags,\n",
    "                    sql=sql_tokens,\n",
    "                    text_variables=text_vars,\n",
    "                    sql_variables=sql_variables,\n",
    "                )\n",
    "                yield sql_data\n",
    "\n",
    "                # Some questions might have multiple equivalent SQL statements.\n",
    "                # By default, we just use the first one. TODO(Mark): Use the shortest?\n",
    "                if not use_all_sql:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Official evaluation script for natural language to SQL datasets.\n",
    "\n",
    "Arguments:\n",
    "    predictions_filepath (str): Path to a predictions file (in JSON format).\n",
    "    output_filepath (str): Path to the file where the result of execution is\n",
    "        saved.\n",
    "    cache_filepath (str): Path to a JSON file containing a mapping from gold SQL\n",
    "        queries to cached resulting tables.  Should be ran locally. All filepaths\n",
    "        above should refer to the local filesystem.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import timeout_decorator\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Maximum allowable timeout for executing predicted and gold queries.\n",
    "TIMEOUT = 30\n",
    "\n",
    "# Maximum number of candidates we should consider\n",
    "MAX_CANDIDATE = 20\n",
    "\n",
    "\n",
    "# These are substrings of exceptions from sqlite3 that indicate certain classes\n",
    "# of schema and syntax errors.\n",
    "SCHEMA_INCOHERENCE_STRINGS = {\n",
    "        'no such table', 'no such column', 'ambiguous column name'\n",
    "}\n",
    "SYNTAX_INCORRECTNESS_STRINGS = {\n",
    "        'bad syntax', 'unrecognized token', 'incomplete input',\n",
    "        'misuse of aggregate', 'left and right', 'wrong number of arguments',\n",
    "        'sub-select returns', '1st order by term does not match any column',\n",
    "        'no such function', 'clause is required before',\n",
    "        'incorrect number of bindings', 'datatype mismatch', 'syntax error'\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_sql_str(string):\n",
    "    \"\"\"Normalizes the format of a SQL string for string comparison.\"\"\"\n",
    "    string = string.lower()\n",
    "    while '  ' in string:\n",
    "        string = string.replace('  ', ' ')\n",
    "    string = string.strip()\n",
    "    string = string.replace('( ', '(').replace(' )', ')')\n",
    "    string = string.replace(' ;', ';')\n",
    "    string = string.replace('\"', '\\'')\n",
    "\n",
    "    if ';' not in string:\n",
    "        string += ';'\n",
    "    return string\n",
    "\n",
    "\n",
    "def string_acc(s1, s2):\n",
    "    \"\"\"Computes string accuracy between two SQL queries.\"\"\"\n",
    "    return normalize_sql_str(s1) == normalize_sql_str(s2)\n",
    "\n",
    "\n",
    "def result_table_to_string(table):\n",
    "    \"\"\"Converts a resulting SQL table to a human-readable string.\"\"\"\n",
    "    string_val = '\\t' + '\\n\\t'.join(\n",
    "            [str(row) for row in table[:min(len(table), 5)]]) + '\\n'\n",
    "    if len(table) > 5:\n",
    "        string_val += '... and %d more rows.\\n' % (len(table) - 5)\n",
    "    return string_val\n",
    "\n",
    "\n",
    "def try_executing_query(prediction, cursor, case_sensitive=True, verbose=False):\n",
    "    \"\"\"Attempts to execute a SQL query against a database given a cursor.\"\"\"\n",
    "    exception_str = None\n",
    "\n",
    "    prediction_str = prediction[:]\n",
    "    prediction_str = prediction_str.replace(';', '').strip()\n",
    "#     print('Current prediction:' + prediction_str)\n",
    "\n",
    "    try:\n",
    "        if not case_sensitive:\n",
    "            new_prediction = ''\n",
    "            last_quote = ''\n",
    "            for char in prediction:\n",
    "                new_prediction += char\n",
    "                if char in {'\"', '\\''} and not last_quote:\n",
    "                    last_quote = char\n",
    "                elif char == last_quote:\n",
    "                    last_quote = ''\n",
    "                    new_prediction += ' COLLATE NOCASE'\n",
    "            prediction = new_prediction\n",
    "\n",
    "            if verbose:\n",
    "                print('Executing case-insensitive query:')\n",
    "                print(new_prediction)\n",
    "        pred_results = timeout_execute(cursor, prediction)\n",
    "    except timeout_decorator.timeout_decorator.TimeoutError:\n",
    "        print('!time out!')\n",
    "        pred_results = []\n",
    "        exception_str = 'timeout'\n",
    "    except (sqlite3.Warning, sqlite3.Error, sqlite3.DatabaseError,\n",
    "                    sqlite3.IntegrityError, sqlite3.ProgrammingError,\n",
    "                    sqlite3.OperationalError, sqlite3.NotSupportedError) as e:\n",
    "        exception_str = str(e).lower()\n",
    "        pred_results = []\n",
    "\n",
    "    return pred_results, exception_str\n",
    "\n",
    "\n",
    "@timeout_decorator.timeout(seconds=TIMEOUT, use_signals=False)\n",
    "def timeout_execute(cursor, prediction):\n",
    "    cursor.execute(prediction)\n",
    "    pred_results = cursor.fetchall()\n",
    "    pred_results = [list(result) for result in pred_results]\n",
    "    return pred_results\n",
    "\n",
    "\n",
    "def find_used_entities_in_string(query, columns, tables):\n",
    "    \"\"\"Heuristically finds schema entities included in a SQL query.\"\"\"\n",
    "    used_columns = set()\n",
    "    used_tables = set()\n",
    "\n",
    "    nopunct_query = query.replace('.', ' ').replace('(', ' ').replace(')', ' ')\n",
    "\n",
    "    for token in nopunct_query.split(' '):\n",
    "        if token.lower() in columns:\n",
    "            used_columns.add(token.lower())\n",
    "        if token.lower() in tables:\n",
    "            used_tables.add(token.lower())\n",
    "    return used_columns, used_tables\n",
    "\n",
    "\n",
    "def compute_f1(precision, recall):\n",
    "    if precision + recall > 0.:\n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def compute_set_f1(pred_set, gold_set):\n",
    "    \"\"\"Computes F1 of items given two sets of items.\"\"\"\n",
    "    prec = 1.\n",
    "    if pred_set:\n",
    "        prec = float(len(pred_set & gold_set)) / len(pred_set)\n",
    "\n",
    "    rec = 1.\n",
    "    if gold_set:\n",
    "        rec = float(len(pred_set & gold_set)) / len(gold_set)\n",
    "    return compute_f1(prec, rec)\n",
    "\n",
    "\n",
    "def col_tab_f1(schema, gold_query, predicted_query):\n",
    "    \"\"\"Computes the F1 of tables and columns mentioned in the two queries.\"\"\"\n",
    "\n",
    "    # Get the schema entities.\n",
    "    db_columns = set()\n",
    "    db_tables = set()\n",
    "    for name, cols in schema.items():\n",
    "        for col in cols:\n",
    "            db_columns.add(col['field name'].lower())\n",
    "        db_tables.add(name.lower())\n",
    "\n",
    "    # Heuristically find the entities used in the gold and predicted queries.\n",
    "    pred_columns, pred_tables = find_used_entities_in_string(\n",
    "            predicted_query, db_columns, db_tables)\n",
    "    gold_columns, gold_tables = find_used_entities_in_string(\n",
    "            gold_query, db_columns, db_tables)\n",
    "\n",
    "    # Compute and return column and table F1.\n",
    "    return (compute_set_f1(pred_columns,gold_columns), compute_set_f1(pred_tables,gold_tables))\n",
    "\n",
    "\n",
    "def execute_prediction(prediction, empty_table_cursor, cursor, case_sensitive,verbose):\n",
    "    \"\"\"Executes a single example's prediction(s).\n",
    "\n",
    "    If more than one prediction is available, the most likely executable\n",
    "    prediction is used as the \"official\" prediction.\n",
    "\n",
    "    Args:\n",
    "        prediction: A dictionary containing information for a single example's\n",
    "            prediction.\n",
    "        empty_table_cursor: The cursor to a database containing no records, to be\n",
    "            used only to determine whether a query is executable in the database.\n",
    "        cursor: The sqlite3 database cursor to execute queries on.\n",
    "        case_sensitive: Boolean indicating whether the execution should be case\n",
    "            sensitive with respect to string values.\n",
    "        verbose: Whether to print details about what queries are being executed.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing the highest-ranked executable query, the resulting table,\n",
    "        and any exception string associated with executing this query.\n",
    "    \"\"\"\n",
    "\n",
    "    # Go through predictions in order of probability and test their executability\n",
    "    # until you get an executable prediction. If you don't find one, just\n",
    "    # \"predict\" the most probable one.\n",
    "    paired_preds_and_scores = zip(prediction['predictions'], prediction['scores'])\n",
    "    sorted_by_scores = sorted(\n",
    "            paired_preds_and_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    best_prediction = None\n",
    "    pred_results = None\n",
    "    exception_str = None\n",
    "\n",
    "    if len(sorted_by_scores) > MAX_CANDIDATE:\n",
    "        sorted_by_scores = sorted_by_scores[:MAX_CANDIDATE]\n",
    "\n",
    "    for i, (pred, _) in enumerate(sorted_by_scores):\n",
    "        # Try predicting\n",
    "        if verbose:\n",
    "            print('Trying to execute query:\\n\\t' + pred)\n",
    "            print('... on empty database')\n",
    "        temp_exception_str = try_executing_query(pred, empty_table_cursor,case_sensitive, verbose)[1]\n",
    "\n",
    "        if temp_exception_str:\n",
    "            if i == 0:\n",
    "                # By default, set the prediction to the first (highest-scoring)\n",
    "                # one.\n",
    "                best_prediction = pred\n",
    "\n",
    "                # Get the actual results\n",
    "                if verbose:\n",
    "                    print('... on actual database')\n",
    "                pred_results, exception_str = try_executing_query(\n",
    "                        pred, cursor, case_sensitive, verbose)\n",
    "            if exception_str == 'timeout':\n",
    "                # Technically, this query didn't have a syntax problem, so\n",
    "                # continue and set this as the best prediction.\n",
    "                best_prediction = pred\n",
    "\n",
    "                if verbose:\n",
    "                    print('... on actual database')\n",
    "                pred_results, exception_str = try_executing_query(\n",
    "                        pred, cursor, case_sensitive, verbose)\n",
    "                break\n",
    "        else:\n",
    "            best_prediction = pred\n",
    "            exception_str = None\n",
    "\n",
    "            if verbose:\n",
    "                print('No exception... on actual database')\n",
    "            pred_results = try_executing_query(pred, cursor, case_sensitive,verbose)[0]\n",
    "            break\n",
    "\n",
    "    return best_prediction, pred_results, exception_str\n",
    "\n",
    "\n",
    "def _convert_to_unicode_string(value):\n",
    "    if isinstance(value, int) or isinstance(value, float):\n",
    "        return str(value).decode('utf-8', 'ignore')\n",
    "    elif isinstance(value, unicode):\n",
    "        return value\n",
    "    elif isinstance(value, str):\n",
    "        return value.decode('utf-8', 'ignore')\n",
    "    else:\n",
    "        return str(value).decode('utf-8', 'ignore')\n",
    "\n",
    "\n",
    "def execute_predictions(predictions, cache_dict, ofile, case_sensitive, verbose,update_cache):\n",
    "    \"\"\"Executes predicted/gold queries and computes performance.\n",
    "\n",
    "    Writes results to ofile.\n",
    "\n",
    "    Args:\n",
    "        predictions: A list of dictionaries defining the predictions made by a\n",
    "            model.\n",
    "        cache_dict: A dictionary mapping from gold queries to the resulting tables.\n",
    "        ofile: A file pointer to be written to.\n",
    "        case_sensitive: A Boolean indicating whether execution of queries should be\n",
    "            case sensitive with respect to strings.\n",
    "        verbose: Whether to print detailed information about evaluation (e.g., for\n",
    "            debugging).\n",
    "        update_cache: Whether to execute and cache gold queries.\n",
    "    \"\"\"\n",
    "    # Keeps tracks of metrics throughout all of the evaluation.\n",
    "    exec_results_same = list()\n",
    "    string_same = list()\n",
    "\n",
    "    precision = list()\n",
    "    recall = list()\n",
    "\n",
    "    column_f1s = list()\n",
    "    table_f1s = list()\n",
    "\n",
    "    conversion_errors = 0\n",
    "\n",
    "    schema_errors = 0\n",
    "    syntax_errors = 0\n",
    "    timeouts = 0\n",
    "\n",
    "    gold_error = 0\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    predictions_iterator = tqdm\n",
    "    if verbose:\n",
    "        # Don't use TQDM if verbose: it might mess up the verbose messages\n",
    "        predictions_iterator = lambda x: x\n",
    "\n",
    "    for prediction in predictions_iterator(predictions):\n",
    "        # Attempt to connect to the database for executing.\n",
    "        try:\n",
    "            conn = sqlite3.connect(prediction['database_path'])\n",
    "            conn.text_factory = str\n",
    "        except sqlite3.OperationalError as e:\n",
    "            print(e)\n",
    "            print(prediction['database_path'])\n",
    "            exit()\n",
    "\n",
    "        empty_path = prediction['empty_database_path']\n",
    "        try:\n",
    "            empty_conn = sqlite3.connect(empty_path)\n",
    "            empty_conn.text_factory = str\n",
    "        except sqlite3.OperationalError as e:\n",
    "            print(e)\n",
    "            print(empty_path)\n",
    "            exit()\n",
    "\n",
    "        empty_cursor = empty_conn.cursor()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        ofile.write('Example #' + str(i) + '\\n')\n",
    "        printable_utterance = u''.join(\n",
    "                prediction['utterance']).encode('utf-8').strip()\n",
    "        ofile.write(printable_utterance + '\\n')\n",
    "\n",
    "        if verbose:\n",
    "            print('Finding the highest-rated prediction for utterance:\\n\\t' +\n",
    "                        printable_utterance)\n",
    "\n",
    "        best_prediction, pred_results, exception_str = execute_prediction(\n",
    "                prediction, empty_cursor, cursor, case_sensitive, verbose)\n",
    "\n",
    "        ofile.write('Predicted query:\\n')\n",
    "        if best_prediction:\n",
    "            ofile.write('\\t' + u''.join(best_prediction).encode('utf-8').strip() +\n",
    "                                    '\\n')\n",
    "        else:\n",
    "            ofile.write('ERROR: Cannot write prediction %r\\n' % best_prediction)\n",
    "\n",
    "        # If it didn't execute correctly, check why.\n",
    "        if exception_str:\n",
    "            ofile.write(exception_str + '\\n')\n",
    "\n",
    "            found_error = False\n",
    "            for substring in SCHEMA_INCOHERENCE_STRINGS:\n",
    "                if substring in exception_str.lower():\n",
    "                    schema_errors += 1\n",
    "                    found_error = True\n",
    "                    break\n",
    "\n",
    "            if not found_error:\n",
    "                for substring in SYNTAX_INCORRECTNESS_STRINGS:\n",
    "                    if substring in exception_str.lower():\n",
    "                        syntax_errors += 1\n",
    "                        found_error = True\n",
    "                        break\n",
    "\n",
    "            if not found_error and 'timeout' in exception_str:\n",
    "                ofile.write('Execution (predicted) took too long.\\n')\n",
    "                found_error = True\n",
    "                timeouts += 1\n",
    "\n",
    "            # If the error type hasn't been identified, exit and report it.\n",
    "            if not found_error:\n",
    "                print(best_prediction)\n",
    "                print(exception_str)\n",
    "                exit(1)\n",
    "\n",
    "            # Predicted table should be empty for all of these cases.\n",
    "            pred_results = []\n",
    "\n",
    "        # Compare to gold and update metrics\n",
    "        gold_query = prediction['gold']\n",
    "\n",
    "        ofile.write('Gold query:\\n')\n",
    "        ofile.write('\\t' + u''.join(gold_query).encode('utf-8').strip() + '\\n')\n",
    "\n",
    "        # Get the gold results\n",
    "        if cache_dict is None or gold_query not in cache_dict:\n",
    "            if printable_utterance not in cache_dict:\n",
    "                if update_cache:\n",
    "                    if verbose:\n",
    "                        print('Trying to execute the gold query:\\n\\t' + gold_query)\n",
    "                    gold_results, gold_exception_str = try_executing_query(\n",
    "                            gold_query, cursor, case_sensitive, verbose)\n",
    "\n",
    "                    if gold_exception_str:\n",
    "                        gold_error += 1\n",
    "                        gold_results = []\n",
    "                    elif cache_dict is not None:\n",
    "                        cache_dict[u''.join(gold_query).decode('utf-8')] = gold_results\n",
    "                else:\n",
    "                    print(gold_query)\n",
    "                    print(printable_utterance)\n",
    "                    raise ValueError('Cache miss!')\n",
    "\n",
    "            else:\n",
    "                gold_results = cache_dict[cache_dict[printable_utterance]]\n",
    "        else:\n",
    "            gold_results = cache_dict[gold_query]\n",
    "\n",
    "        if best_prediction:\n",
    "            string_same.append(string_acc(gold_query, best_prediction))\n",
    "            col_f1, tab_f1 = col_tab_f1(prediction['schema'], gold_query,\n",
    "                                                                    best_prediction)\n",
    "            column_f1s.append(col_f1)\n",
    "            table_f1s.append(tab_f1)\n",
    "            ofile.write('Column F1: %f\\n' % col_f1)\n",
    "            ofile.write('Table F1: %f\\n' % tab_f1)\n",
    "\n",
    "            if 'order by' in gold_query:\n",
    "                results_equivalent = pred_results == gold_results\n",
    "            else:\n",
    "                pred_set = set()\n",
    "                gold_set = set()\n",
    "                for pred in pred_results:\n",
    "                    if isinstance(pred, list):\n",
    "                        pred_set.add(u' '.join(\n",
    "                                [_convert_to_unicode_string(item) for item in pred]))\n",
    "                    else:\n",
    "                        pred_set.add(pred)\n",
    "                for gold in gold_results:\n",
    "                    if isinstance(gold, list):\n",
    "                        gold_set.add(u' '.join(\n",
    "                                [_convert_to_unicode_string(item) for item in gold]))\n",
    "                    else:\n",
    "                        gold_set.add(gold)\n",
    "\n",
    "                results_equivalent = pred_set == gold_set\n",
    "\n",
    "        else:\n",
    "            string_same.append(0.)\n",
    "            ofile.write('Column F1: 0.')\n",
    "            ofile.write('Table F1: 0.')\n",
    "            column_f1s.append(0.)\n",
    "            table_f1s.append(0.)\n",
    "\n",
    "            conversion_errors += 1\n",
    "\n",
    "            # Only consider correct if the gold table was empty.\n",
    "            results_equivalent = gold_results == list()\n",
    "\n",
    "        exec_results_same.append(int(results_equivalent))\n",
    "        ofile.write('Execution was correct? ' + str(results_equivalent) + '\\n')\n",
    "\n",
    "        # Add some debugging information about the tables, and compute the\n",
    "        # precisions.\n",
    "        if pred_results:\n",
    "            if not results_equivalent:\n",
    "                ofile.write('Predicted table:\\n')\n",
    "                ofile.write(result_table_to_string(pred_results))\n",
    "\n",
    "            precision.append(int(results_equivalent))\n",
    "        elif best_prediction is None or not results_equivalent:\n",
    "            ofile.write('Predicted table was EMPTY!\\n')\n",
    "\n",
    "        if gold_results:\n",
    "            ofile.write('Gold table:\\n')\n",
    "            ofile.write(result_table_to_string(gold_results))\n",
    "\n",
    "            recall.append(int(results_equivalent))\n",
    "        else:\n",
    "            ofile.write('Gold table was EMPTY!\\n')\n",
    "\n",
    "        ofile.write('\\n')\n",
    "        ofile.flush()\n",
    "\n",
    "        conn.close()\n",
    "        empty_conn.close()\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Write the overall metrics to the file.\n",
    "    num_empty_pred = len(precision)\n",
    "    num_empty_gold = len(recall)\n",
    "\n",
    "    precision = np.mean(np.array(precision))\n",
    "    recall = np.mean(np.array(recall))\n",
    "\n",
    "    execution_f1 = compute_f1(precision, recall)\n",
    "\n",
    "    ofile.write('String accuracy: ' +\n",
    "                            '{0:.2f}'.format(100. * np.mean(np.array(string_same))) + '\\n')\n",
    "    ofile.write('Accuracy: ' +\n",
    "                            '{0:.2f}'.format(100. * np.mean(np.array(exec_results_same))) +\n",
    "                            '\\n')\n",
    "    ofile.write('Precision: ' + '{0:.2f}'.format(100. * precision) + ' ; ' +\n",
    "                            str(num_empty_pred) + ' nonempty predicted tables' + '\\n')\n",
    "    ofile.write('Recall: ' + '{0:.2f}'.format(100. * recall) + ' ; ' +\n",
    "                            str(num_empty_gold) + ' nonempty gold tables' + '\\n')\n",
    "    ofile.write('Execution F1: ' + '{0:.2f}'.format(100. * execution_f1) + '\\n')\n",
    "    ofile.write('Timeout: ' +\n",
    "                            '{0:.2f}'.format(timeouts * 100. / len(predictions)) + '\\n')\n",
    "    ofile.write('Gold did not execute: ' +\n",
    "                            '{0:.2f}'.format(gold_error * 100. / len(predictions)) + '\\n')\n",
    "    ofile.write('Average column F1: ' +\n",
    "                            '{0:.2f}'.format(100. * np.mean(np.array(column_f1s))) + '\\n')\n",
    "    ofile.write('Average table F1: ' +\n",
    "                            '{0:.2f}'.format(100. * np.mean(np.array(table_f1s))) + '\\n')\n",
    "    ofile.write('Schema errors: ' +\n",
    "                            '{0:.2f}'.format((schema_errors) * 100. / len(predictions)) +\n",
    "                            '\\n')\n",
    "    ofile.write('Syntax errors:  ' +\n",
    "                            '{0:.2f}'.format((syntax_errors) * 100. / len(predictions)) +\n",
    "                            '\\n')\n",
    "    ofile.write('Conversion errors: ' +\n",
    "                            '{0:.2f}'.format((conversion_errors * 100.) / len(predictions)) +\n",
    "                            '\\n')\n",
    "\n",
    "\n",
    "def main(predictions_filepath, output_filepath, cache_filepath, verbose,\n",
    "                 update_cache):\n",
    "    # Load the predictions filepath.\n",
    "    with open(predictions_filepath) as infile:\n",
    "        predictions = json.load(infile)\n",
    "    print('Loaded %d predictions.' % len(predictions))\n",
    "\n",
    "    # Load or create the cache dictionary mapping from gold queries to resulting\n",
    "    # tables.\n",
    "    cache_dict = None\n",
    "\n",
    "    # Only instantiate the cache dict if using Spider.\n",
    "    print('cache path: ' + cache_filepath)\n",
    "\n",
    "    basefilename = os.path.basename(predictions_filepath).lower()\n",
    "\n",
    "    if 'spider' not in basefilename:\n",
    "        cache_dict = dict()\n",
    "        if os.path.exists(cache_filepath):\n",
    "            print('Loading cache from %s' % cache_filepath)\n",
    "            with open(cache_filepath) as infile:\n",
    "                cache_dict = json.load(infile)\n",
    "            print('Loaded %d cached queries' % len(cache_dict))\n",
    "\n",
    "    # Create the text file that results will be written to.\n",
    "    with open(output_filepath, 'w') as ofile:\n",
    "        execute_predictions(predictions, cache_dict, ofile,\n",
    "                                                'scholar' not in basefilename, verbose, update_cache)\n",
    "\n",
    "    if 'spider' not in basefilename:\n",
    "        try:\n",
    "            cache_str = json.dumps(cache_dict)\n",
    "            with open(cache_filepath, 'w') as ofile:\n",
    "                ofile.write(cache_str)\n",
    "        except UnicodeDecodeError as e:\n",
    "            print('Could not save the cache dict. Exception:')\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_in_suhr(example, verbose=False):\n",
    "    # Filter out examples with empty gold tables.\n",
    "    if example['query_exception'] is not None and example['query_exception']:\n",
    "        return example['query_exception']\n",
    "    empty = False\n",
    "    if example['result'] is None or len(example['result'])==0:\n",
    "        empty = True\n",
    "\n",
    "    # Filter out examples with a result of [0] and that require a count.\n",
    "    if (example['result'] == [0] and\n",
    "      (example['query'].lower().startswith('select count') or\n",
    "       example['query'].lower().startswith('select distinct count'))):\n",
    "        empty = True\n",
    "\n",
    "    # Filter out examples that require copying values that can't be copied.\n",
    "    prev_value = ''\n",
    "    last_quote = ''\n",
    "    utterance = example['question'].lower()\n",
    "    copiable = True\n",
    "    in_equality = False\n",
    "    numerical_value = ''\n",
    "    handled_prefix = False\n",
    "    too_many_selects = False\n",
    "    gold_query = example['query'].lower()\n",
    "        \n",
    "    for i, char in enumerate(gold_query):\n",
    "        # Check that it's only selecting a single table at the top\n",
    "        if (not handled_prefix and i - 4 >= 0 and gold_query[i - 4:i].lower() == 'from'):\n",
    "            handled_prefix = True\n",
    "            if gold_query[:i].count(',') > 0:\n",
    "                too_many_selects = True\n",
    "        if char == last_quote:\n",
    "            last_quote = ''\n",
    "            prev_value = prev_value.replace('%', '').strip()\n",
    "            if prev_value not in utterance:\n",
    "                if verbose:\n",
    "                    print(prev_value)\n",
    "                copiable = False\n",
    "            prev_value = ''\n",
    "        elif last_quote:\n",
    "            prev_value += char\n",
    "        elif char in {'\"', '\\''}:\n",
    "            last_quote = char\n",
    "        if char in {'=', '>', '<'}:\n",
    "            in_equality = True\n",
    "            equality_used = False\n",
    "        elif in_equality:\n",
    "            if char.isdigit() or char == '.':\n",
    "                if numerical_value or (not prev_value and gold_query[i - 1] == ' '):\n",
    "                    numerical_value += char\n",
    "            if char == ' ' and numerical_value:\n",
    "                in_equality = False\n",
    "                if numerical_value not in utterance:\n",
    "                    if verbose:\n",
    "                        print(numerical_value)\n",
    "                    copiable = False\n",
    "                numerical_value = ''\n",
    "            if char != ' ':\n",
    "                equality_used = True\n",
    "            if char == ' ' and not last_quote and equality_used:\n",
    "                in_equality = False\n",
    "    if not copiable:\n",
    "        return 'not copiable'\n",
    "    if empty:\n",
    "        return 'empty result'\n",
    "    if too_many_selects:\n",
    "        return 'too many selects'\n",
    "    return 'pass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# the mock-0.3.1 dir contains testcase.py, testutils.py & mock.py\n",
    "sys.path.append('/home/t-xiaden/workspace/NL2CodeOverData')\n",
    "from third_party.spider.preprocess.get_tables import dump_db_json_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from third_party.spider.process_sql import get_sql\n",
    "from third_party.spider.preprocess.schema import Schema\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "def get_schemas_from_json(data):\n",
    "    db_names = [db['db_id'] for db in data]\n",
    "\n",
    "    tables = {}\n",
    "    schemas = {}\n",
    "    for db in data:\n",
    "        db_id = db['db_id']\n",
    "        schema = {} #{'table': [col.lower, ..., ]} * -> __all__\n",
    "        column_names_original = db['column_names_original']\n",
    "        table_names_original = db['table_names_original']\n",
    "        tables[db_id] = {'column_names_original': column_names_original, 'table_names_original': table_names_original}\n",
    "        for i, tabn in enumerate(table_names_original):\n",
    "            table = str(tabn.lower())\n",
    "            cols = [str(col.lower()) for td, col in column_names_original if td == i]\n",
    "            schema[table] = cols\n",
    "        schemas[db_id] = schema\n",
    "\n",
    "    return schemas, db_names, tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAUSE_KEYWORDS = ('select', 'from', 'where', 'group', 'order', 'limit', 'intersect', 'union', 'except')\n",
    "JOIN_KEYWORDS = ('join', 'on', 'as')\n",
    "\n",
    "WHERE_OPS = ('not', 'between', '=', '>', '<', '>=', '<=', '!=', 'in', 'like', 'is', 'exists')\n",
    "UNIT_OPS = ('none', '-', '+', \"*\", '/')\n",
    "AGG_OPS = ('none', 'max', 'min', 'count', 'sum', 'avg')\n",
    "TABLE_TYPE = {\n",
    "    'sql': \"sql\",\n",
    "    'table_unit': \"table_unit\",\n",
    "}\n",
    "\n",
    "COND_OPS = ('and', 'or')\n",
    "SQL_OPS = ('intersect', 'union', 'except')\n",
    "ORDER_OPS = ('desc', 'asc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_ops = {'max', 'min', 'count', 'sum', 'avg', 'between', '=', '>', '<', '>=', '<=', '!=', 'in', 'like', 'is', 'exists'}\n",
    "def try_clean_brackets(sql):\n",
    "    sql = sql.split()\n",
    "    p1_idx = []\n",
    "    p = []\n",
    "    for i, token in enumerate(sql):\n",
    "        if token == '(':\n",
    "            if sql[i-1].lower() in legal_ops or \\\n",
    "                sql[i+1].lower() == 'select':\n",
    "                p1_idx.append([i, 'keep'])\n",
    "            else:\n",
    "                p1_idx.append([i, 'remove', ' '.join(sql[i-2:i+4])])\n",
    "        if token == ')':\n",
    "            p.append([i,p1_idx[-1]])\n",
    "            p1_idx = p1_idx[:-1]\n",
    "    for idx2, idx1 in p:\n",
    "        if idx1[1] == 'remove':\n",
    "            sql[idx1[0]] = ''\n",
    "            sql[idx2] = ''\n",
    "    sql = ' '.join([x for x in sql if x])\n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'atis':['dev'],\n",
    "            'geography':['train','dev'],\n",
    "            'restaurants':[str(x) for x in range(10)],\n",
    "            'scholar':['train','dev'],\n",
    "            'imdb':[str(x) for x in range(10)],\n",
    "            'yelp':[str(x) for x in range(10)],\n",
    "            'advising':['train','dev'],\n",
    "            'academic':[str(x) for x in range(10)]}\n",
    "datadir = '/home/t-xiaden/workspace/text2sql-data/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('/home/t-xiaden/workspace/text2sql-data/data/%s.db'%'imdb')\n",
    "conn.text_factory = lambda x:str(x, 'latin1')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main processing\n",
    "i. The purpose of the filtering is to remove those cases where current model (trained on spider) always fails as such structure/operation is never seen in training. So we can have a better set for evaluation\n",
    "\n",
    "ii. Remove parentheses. Test the execution result, remove the example if the result changes\n",
    "    \n",
    "    a. (A AND B) AND (C AND D) is ok, but (A AND B) OR (C AND D) will be removed\n",
    "    \n",
    "    b. DISTINCT()\n",
    "\n",
    "iii. JOIN ON and COND in WHERE, some datasets use different format to represent join on, change spider parser to support it. Also change all types of join (inner, left,â€¦) to join, remove if the execution result changes\n",
    "\n",
    "iv. Remove examples contain derived table/column, as spider assumes that we only selects column from the database\n",
    "\n",
    "v. Boolean result, such as SELECT COUNT(*)>0\n",
    "\n",
    "vi. The original dataset is released with value anonymized. Some value that cannot find reference in question (in any form) will be replaced as LIKE '%', remove such condition\n",
    "\n",
    "vii. Calculation in sql\n",
    "\"SELECT DISTINCT COURSEalias0.DEPARTMENT , COURSEalias0.NAME , COURSEalias0.NUMBER , SEMESTERalias0.SEMESTER FROM COURSE AS COURSEalias0 , COURSE_OFFERING AS COURSE_OFFERINGalias0 , SEMESTER AS SEMESTERalias0 WHERE COURSEalias0.COURSE_ID = COURSE_OFFERINGalias0.COURSE_ID AND COURSEalias0.DEPARTMENT LIKE '%' AND COURSEalias0.NUMBER BETWEEN 500 AND 500 + 100 AND SEMESTERalias0.SEMESTER IN ( 'FA' , 'WN' ) AND SEMESTERalias0.SEMESTER_ID = COURSE_OFFERINGalias0.SEMESTER AND SEMESTERalias0.YEAR = 2016 ;\" \n",
    "\n",
    "viii. Multiple columns in COUNT\n",
    "SELECT COUNT ( DISTINCT COURSEalias1.DEPARTMENT , COURSEalias0.NUMBER ) FROM COURSE\n",
    "\n",
    "ix. COUNT ( 1 ) -> COUNT( * )\n",
    "\n",
    "x. COL IN (VAL1, VAL2) -> COL = VAL1 OR COL = VAL2\n",
    "\n",
    "xi. Filter in suhr paper:\n",
    "    \n",
    "    a. Remove not copiable\n",
    "    \n",
    "    b. Remove empty. keep empty but report non-empty exec results\n",
    "    \n",
    "    c. Remove too many select. Keep query with multiple select, report oracle select result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–        | 139/947 [02:51<07:57,  1.69it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|â–ˆâ–ˆâ–       | 202/947 [04:13<30:06,  2.42s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–‰       | 274/947 [04:54<17:38,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|â–ˆâ–ˆâ–‰       | 277/947 [05:54<1:19:51,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 456/947 [07:06<09:35,  1.17s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 648/947 [08:02<10:21,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 828/947 [08:59<10:00,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 836/947 [10:15<20:52, 11.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 840/947 [10:47<21:35, 12.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 843/947 [10:48<07:48,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 844/947 [11:49<36:34, 21.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 847/947 [12:20<27:45, 16.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 849/947 [12:21<13:48,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 850/947 [13:22<38:55, 24.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 854/947 [13:56<24:21, 15.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 864/947 [14:50<15:05, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 868/947 [15:29<17:09, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 869/947 [15:29<12:03,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 870/947 [16:30<31:37, 24.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 872/947 [17:01<26:40, 21.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 947/947 [17:04<00:00,  1.08s/it]\n",
      "  0%|          | 0/246 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486 474 12\n",
      "geography\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 246/246 [02:36<00:00,  1.57it/s]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598 584 14\n",
      "restaurants\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [01:37<00:00,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378 378 0\n",
      "scholar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 20/193 [01:29<18:28,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n",
      "!time out!\n",
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|â–ˆ         | 21/193 [03:31<1:57:53, 41.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 38/193 [06:17<49:17, 19.08s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|â–ˆâ–ˆ        | 39/193 [08:19<2:08:16, 49.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 46/193 [08:39<14:10,  5.79s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|â–ˆâ–ˆâ–       | 47/193 [11:11<2:01:16, 49.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 86/193 [17:38<04:15,  2.39s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 87/193 [18:39<35:18, 19.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 96/193 [19:05<04:09,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n",
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 97/193 [20:36<46:51, 29.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 113/193 [21:16<02:26,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 114/193 [22:18<25:50, 19.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 115/193 [22:18<18:01, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 116/193 [23:19<35:57, 28.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 121/193 [24:00<18:58, 15.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 122/193 [24:31<23:50, 20.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 145/193 [25:18<07:33,  9.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 148/193 [26:25<13:13, 17.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 151/193 [27:42<15:33, 22.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 152/193 [27:42<10:41, 15.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 153/193 [28:43<19:25, 29.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 160/193 [29:18<06:31, 11.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 193/193 [29:18<00:00,  9.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599 581 18\n",
      "imdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [01:33<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 128 3\n",
      "yelp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 64/110 [01:09<00:28,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n",
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 65/110 [03:08<26:59, 35.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 66/110 [04:09<31:49, 43.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 75/110 [04:19<01:28,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 76/110 [05:19<11:18, 19.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 77/110 [05:20<07:48, 14.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 78/110 [06:21<15:01, 28.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 79/110 [06:23<10:30, 20.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 80/110 [07:24<16:12, 32.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [09:15<00:00,  5.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 122 6\n",
      "advising\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/205 [00:23<46:12, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|â–         | 3/205 [08:04<8:17:35, 147.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 15/205 [09:25<30:08,  9.52s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|â–Š         | 16/205 [16:34<7:06:59, 135.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 23/205 [17:35<1:00:13, 19.85s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|â–ˆâ–        | 24/205 [23:44<6:15:35, 124.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 53/205 [28:43<31:11, 12.31s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|â–ˆâ–ˆâ–‹       | 54/205 [34:52<4:59:47, 119.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 85/205 [38:31<12:39,  6.33s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 86/205 [46:43<5:01:14, 151.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 99/205 [50:07<19:21, 10.96s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 100/205 [56:46<3:42:51, 127.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 110/205 [57:43<14:07,  8.92s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 111/205 [1:06:56<4:29:31, 172.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 124/205 [1:08:14<11:38,  8.63s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 125/205 [1:13:21<2:10:48, 98.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 146/205 [1:15:45<07:46,  7.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 147/205 [1:23:26<2:18:56, 143.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 190/205 [1:30:03<02:15,  9.01s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 191/205 [1:36:12<27:17, 116.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 192/205 [1:41:19<37:42, 174.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 193/205 [1:50:01<55:40, 278.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 195/205 [1:50:20<23:33, 141.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n",
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 196/205 [1:56:29<31:26, 209.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [2:05:40<00:00, 36.78s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2858 1948 910\n",
      "academic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 19/185 [00:56<26:47,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 50/185 [01:45<21:26,  9.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 97/185 [04:15<01:39,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 98/185 [05:16<27:37, 19.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 104/185 [05:27<04:41,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 105/185 [06:28<27:34, 20.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 129/185 [06:46<00:31,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 130/185 [07:47<17:05, 18.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 138/185 [08:20<08:12, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 142/185 [08:24<02:16,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 143/185 [09:24<14:19, 20.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!time out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [12:37<00:00,  4.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196 181 15\n"
     ]
    }
   ],
   "source": [
    "final_data = {}\n",
    "for dataset_id in datasets:\n",
    "    print(dataset_id)\n",
    "    db = dump_db_json_schema('/home/t-xiaden/workspace/text2sql-data/data/%s.db'%dataset_id,dataset_id)\n",
    "    db = [db]\n",
    "    schemas, db_names, tables = get_schemas_from_json(db)\n",
    "    schema = schemas[dataset_id]\n",
    "    table = tables[dataset_id]\n",
    "    schema = Schema(schema, table)\n",
    "    schema_1 = read_dataset_schema('/home/t-xiaden/workspace/text2sql-data/data/%s-schema.csv'%dataset_id)\n",
    "    processed_data = []\n",
    "\n",
    "    error_sqls = []\n",
    "    with sqlite3.connect('/home/t-xiaden/workspace/text2sql-data/data/%s.db'%dataset_id) as source:\n",
    "        dest = sqlite3.connect(':memory:')\n",
    "        source.backup(dest)\n",
    "        conn = dest\n",
    "        conn.text_factory = lambda x:str(x, 'latin1')\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "    with open(os.path.join(datadir, '{}.json'.format(dataset_id))) as f:\n",
    "        data = json.load(f)\n",
    "        pairs = list()\n",
    "\n",
    "    # The UMichigan data is split by anonymized queries, where values are\n",
    "    # anonymized but table/column names are not. However, our experiments are\n",
    "    # performed on the original splits of the data.\n",
    "    for query in tqdm(data):\n",
    "        # Take the first SQL query only. From their Github documentation:\n",
    "        # \"Note - we only use the first query, but retain the variants for\n",
    "        #  completeness\"\n",
    "        anonymized_sql = query['sql'][0]\n",
    "\n",
    "        # It's also associated with a number of natural language examples, which\n",
    "        # also contain anonymous tokens. Save the de-anonymized utterance and query.\n",
    "        for example in query['sentences']:\n",
    "            if example['question-split'] not in datasets[dataset_id]:\n",
    "                continue\n",
    "\n",
    "            nl = example['text']\n",
    "            sql = anonymized_sql\n",
    "            sql_tokens = clean_and_split_sql(sql)\n",
    "    #         sql_tokens = clean_unneeded_aliases(sql_tokens)\n",
    "            sql_tokens = resolve_primary_keys_in_schema(sql_tokens, schema_1)\n",
    "            sql = ' '.join(sql_tokens).replace(' . ','.')\n",
    "\n",
    "            # Go through the anonymized values and replace them in both the natural\n",
    "            # language and the SQL.\n",
    "            #\n",
    "            # It's very important to sort these in descending order. If one is a\n",
    "            # substring of the other, it shouldn't be replaced first lest it ruin the\n",
    "            # replacement of the superstring.\n",
    "            for variable_name, value in sorted(\n",
    "                  example['variables'].items(), key=lambda x: len(x[0]), reverse=True):\n",
    "                if not value:\n",
    "                # TODO(alanesuhr) While the Michigan repo says to use a - here, the\n",
    "                # thing that works is using a % and replacing = with LIKE.\n",
    "                #\n",
    "                # It's possible that I should remove such clauses from the SQL, as\n",
    "                # long as they lead to the same table result. They don't align well\n",
    "                # to the natural language at least.\n",
    "                #\n",
    "                # See: https://github.com/jkkummerfeld/text2sql-data/tree/master/data\n",
    "                    value = '%'\n",
    "\n",
    "                nl = nl.replace(variable_name, value)\n",
    "                sql = sql.replace(variable_name, value)\n",
    "\n",
    "            # In the case that we replaced an empty anonymized value with %, make it\n",
    "            # compilable new allowing equality with any string.\n",
    "            sql = re.sub('= \\'?%\\'?','LIKE \\'%\\'',sql)\n",
    "            # remove the wildcard matching\n",
    "            new_sql = sql\n",
    "            all_wildcards = re.findall(r'(AND|OR)? (\\w+\\.\\w+ (=|LIKE) (\\'|\\\")?%(\\'|\\\")?) (AND|OR)?',new_sql)\n",
    "            for before_and, wildcard,_,_,_,after_and in all_wildcards:\n",
    "                if before_and:\n",
    "                    wildcard = before_and +' '+wildcard\n",
    "                elif after_and:\n",
    "                    wildcard = wildcard +' '+after_and\n",
    "                new_sql = new_sql.replace(wildcard, '')\n",
    "                \n",
    "            # col IN (val, +) -> col = val OR col = val\n",
    "            for in_clause, col,_, vals,_ in re.findall(r'((\\w+\\.\\w+) IN (\\(( (\\'.+\\'|[0-9\\.]+) ,?)+\\)))',new_sql):\n",
    "                new_clause = []\n",
    "                for val in vals.split(','):\n",
    "                    val = val.strip()\n",
    "                    new_clause.append(col+' = '+val)\n",
    "                new_clause = ' OR '.join(new_clause)\n",
    "                new_sql = new_sql.replace(in_clause, new_clause)\n",
    "                \n",
    "            # COUNT ( 1 ) -> COUNT(*)\n",
    "            new_sql = new_sql.replace('COUNT ( 1 )', 'COUNT ( * )')\n",
    "            \n",
    "            \n",
    "            new_sql = new_sql.replace('<>', '!=')\n",
    "            \n",
    "            new_sql = re.sub(r'(INNER JOIN)|(LEFT JOIN)|(OUTER JOIN)|(LEFT OUTER JOIN)','JOIN',new_sql, flags=re.I)\n",
    "            new_sql = try_clean_brackets(new_sql)\n",
    "            \n",
    "            pred_results, exception_str = try_executing_query(sql, cursor, False, False)\n",
    "            if new_sql != sql:\n",
    "                new_pred_results, new_exception_str = try_executing_query(new_sql, cursor, False, False)\n",
    "            else:\n",
    "                new_pred_results = pred_results\n",
    "                new_exception_str = exception_str\n",
    "            try:\n",
    "                if new_sql != sql:\n",
    "                    # keep only queries that have the same result after tranformation\n",
    "                    assert not exception_str and len(pred_results) == len(new_pred_results) and pred_results == new_pred_results, 'result change'\n",
    "                processed_data.append({\n",
    "                \"db_id\": dataset_id,\n",
    "                \"query\": sql,\n",
    "                \"query_toks\": sql.split(),\n",
    "                \"question\": nl,\n",
    "                \"question_toks\": nl.split(),\n",
    "                \"sql\": get_sql(schema, new_sql),\n",
    "                \"result\": pred_results,\n",
    "                \"query_exception\": exception_str}) \n",
    "                \n",
    "            except Exception as e:\n",
    "                if 'Assert' in str(e):\n",
    "                    break\n",
    "                error_sqls.append({\n",
    "                \"db_id\": dataset_id,\n",
    "                \"query\": sql,\n",
    "                \"query_toks\": sql.split(),\n",
    "                \"question\": nl,\n",
    "                \"question_toks\": nl.split(),\n",
    "                \"sql\": {},\n",
    "                \"error\": str(e),\n",
    "                \"result\": pred_results,\n",
    "                \"query_exception\": exception_str})\n",
    "    final_data[dataset_id] = [processed_data, error_sqls]\n",
    "    print(len(processed_data)+len(error_sqls), len(processed_data), len(error_sqls))\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/home/t-xiaden/workspace/featurestorage/data/spider-20200607/cached_processed_michigan_data.pkl', 'wb') as f:\n",
    "    pickle.dump(final_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_fix(sql):\n",
    "    new_sql = sql\n",
    "    all_wildcards = re.findall(r'(AND|OR)? (\\w+\\.\\w+ (=|LIKE) (\\'|\\\")?%(\\'|\\\")?) (AND|OR)?',new_sql)\n",
    "    for before_and, wildcard,_,_,_,after_and in all_wildcards:\n",
    "        if before_and:\n",
    "            wildcard = before_and +' '+wildcard\n",
    "        elif after_and:\n",
    "            wildcard = wildcard +' '+after_and\n",
    "        new_sql = new_sql.replace(wildcard, '')\n",
    "\n",
    "    # col IN (val, +) -> col = val OR col = val\n",
    "    for in_clause, col,_, vals,_ in re.findall(r'((\\w+\\.\\w+) IN (\\(( (\\'.+\\'|[0-9\\.]+) ,?)+\\)))',new_sql):\n",
    "        new_clause = []\n",
    "        for val in vals.split(','):\n",
    "            val = val.strip()\n",
    "            new_clause.append(col+' = '+val)\n",
    "        new_clause = ' OR '.join(new_clause)\n",
    "        new_sql = new_sql.replace(in_clause, new_clause)\n",
    "\n",
    "    # COUNT ( 1 ) -> COUNT(*)\n",
    "    new_sql = new_sql.replace('COUNT ( 1 )', 'COUNT ( * )')\n",
    "\n",
    "\n",
    "    new_sql = new_sql.replace('<>', '!=')\n",
    "\n",
    "    new_sql = re.sub(r'(INNER JOIN)|(LEFT JOIN)|(OUTER JOIN)|(LEFT OUTER JOIN)','JOIN',new_sql, flags=re.I)\n",
    "    new_sql = try_clean_brackets(new_sql)\n",
    "    return new_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atis\n",
      "486\n",
      "474 286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('pass', 275),\n",
       " ('not copiable', 143),\n",
       " ('empty result', 39),\n",
       " ('too many selects', 5)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('result change', 12)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "geography\n",
      "598\n",
      "584 525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('pass', 525), ('not copiable', 43), ('empty result', 16)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('Error col: as', 10),\n",
       " ('Error col: /', 1),\n",
       " (\"'as'\", 1),\n",
       " ('Error col: all', 1),\n",
       " ('result change', 1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "restaurants\n",
      "378\n",
      "378 39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('not copiable', 210),\n",
       " ('too many selects', 114),\n",
       " ('pass', 39),\n",
       " ('empty result', 15)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "scholar\n",
      "599\n",
      "581 396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('pass', 396),\n",
       " ('too many selects', 118),\n",
       " ('empty result', 32),\n",
       " ('not copiable', 28),\n",
       " ('timeout', 4),\n",
       " ('no such function: curdate', 3)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('result change', 14),\n",
       " ('Error col: year0', 1),\n",
       " ('Error condition: idx: 24, tok: ==', 1),\n",
       " ('Error col: as', 1),\n",
       " (\"'field'\", 1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "imdb\n",
      "131\n",
      "128 111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('pass', 111),\n",
       " ('empty result', 13),\n",
       " ('too many selects', 3),\n",
       " ('not copiable', 1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('result change', 2), ('Error col: as', 1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "yelp\n",
      "128\n",
      "122 68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('pass', 68), ('empty result', 52), ('too many selects', 2)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('result change', 5), (\"'neighborhood.name'\", 1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "advising\n",
      "2858\n",
      "1948 281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('not copiable', 1062),\n",
       " ('empty result', 308),\n",
       " ('too many selects', 297),\n",
       " ('pass', 281)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('result change', 322),\n",
       " ('Error col: >', 288),\n",
       " ('Error col: =', 83),\n",
       " ('Error col: +', 77),\n",
       " (\"'as'\", 61),\n",
       " ('Error col: year0', 13),\n",
       " ('Error col: course_offeringalias0', 11),\n",
       " ('Error col: )', 11),\n",
       " ('Error col: select', 11),\n",
       " ('Error col: ;', 11),\n",
       " ('Error col: lower', 11),\n",
       " ('Error col: as', 9),\n",
       " ('Unexpected quote', 2)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "academic\n",
      "196\n",
      "181 167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('pass', 167), ('empty result', 6), ('too many selects', 6), ('timeout', 2)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(\"'as'\", 9), ('result change', 6)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "errors_for_debug = []\n",
    "for dataset_id, data in final_data.items():\n",
    "    print(dataset_id)\n",
    "    print(len(data[0])+len(data[1]))\n",
    "    print(len(data[0]), len([x for x in data[0] if filter_in_suhr(x)=='pass']))\n",
    "    filter_result = collections.Counter()\n",
    "    filter_errors = [filter_in_suhr(x) for x in data[0] if ' IS ' not in x['query']]\n",
    "    filter_result.update(filter_errors)\n",
    "    display(filter_result.most_common(100))\n",
    "    parse_errors = collections.Counter()\n",
    "    parse_errors.update([x['error'] for x in data[1]])\n",
    "    errors_for_debug.extend([x['query'] for x in data[1] if x['error']==\"'as'\"])\n",
    "    display(parse_errors.most_common(100))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"SELECT DISTINCT FLIGHTalias0.FLIGHT_ID FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias1 , CITY AS CITYalias0 , CITY AS CITYalias1 , DATE_DAY AS DATE_DAYalias0 , DAYS AS DAYSalias0 , FLIGHT AS FLIGHTalias0 WHERE ( CITYalias1.CITY_CODE = AIRPORT_SERVICEalias1.CITY_CODE AND CITYalias1.CITY_NAME = 'PHILADELPHIA' AND DATE_DAYalias0.DAY_NUMBER = 20 AND DATE_DAYalias0.MONTH_NUMBER = 1 AND DATE_DAYalias0.YEAR = 1991 AND DAYSalias0.DAY_NAME = DATE_DAYalias0.DAY_NAME AND FLIGHTalias0.FLIGHT_DAYS = DAYSalias0.DAYS_CODE AND FLIGHTalias0.TO_AIRPORT = AIRPORT_SERVICEalias1.AIRPORT_CODE ) AND CITYalias0.CITY_CODE = AIRPORT_SERVICEalias0.CITY_CODE AND CITYalias0.CITY_NAME = 'DENVER' AND FLIGHTalias0.FROM_AIRPORT = AIRPORT_SERVICEalias0.AIRPORT_CODE ;\",\n",
       "  'what flights are available tomorrow from DENVER to PHILADELPHIA'],\n",
       " [\"SELECT DISTINCT FLIGHTalias0.FLIGHT_ID FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias1 , CITY AS CITYalias0 , CITY AS CITYalias1 , DATE_DAY AS DATE_DAYalias0 , DAYS AS DAYSalias0 , FLIGHT AS FLIGHTalias0 WHERE ( CITYalias1.CITY_CODE = AIRPORT_SERVICEalias1.CITY_CODE AND CITYalias1.CITY_NAME = 'BOSTON' AND DATE_DAYalias0.DAY_NUMBER = 8 AND DATE_DAYalias0.MONTH_NUMBER = 8 AND DATE_DAYalias0.YEAR = 1991 AND DAYSalias0.DAY_NAME = DATE_DAYalias0.DAY_NAME AND FLIGHTalias0.FLIGHT_DAYS = DAYSalias0.DAYS_CODE AND FLIGHTalias0.TO_AIRPORT = AIRPORT_SERVICEalias1.AIRPORT_CODE ) AND CITYalias0.CITY_CODE = AIRPORT_SERVICEalias0.CITY_CODE AND CITYalias0.CITY_NAME = 'SAN FRANCISCO' AND FLIGHTalias0.FROM_AIRPORT = AIRPORT_SERVICEalias0.AIRPORT_CODE ;\",\n",
       "  'display all flights from SAN FRANCISCO to BOSTON on 8 8'],\n",
       " [\"SELECT DISTINCT FLIGHTalias0.FLIGHT_ID FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias1 , CITY AS CITYalias0 , CITY AS CITYalias1 , DATE_DAY AS DATE_DAYalias0 , DAYS AS DAYSalias0 , FLIGHT AS FLIGHTalias0 WHERE ( CITYalias1.CITY_CODE = AIRPORT_SERVICEalias1.CITY_CODE AND CITYalias1.CITY_NAME = 'DENVER' AND DATE_DAYalias0.DAY_NUMBER = 27 AND DATE_DAYalias0.MONTH_NUMBER = 8 AND DATE_DAYalias0.YEAR = 1991 AND DAYSalias0.DAY_NAME = DATE_DAYalias0.DAY_NAME AND FLIGHTalias0.FLIGHT_DAYS = DAYSalias0.DAYS_CODE AND FLIGHTalias0.TO_AIRPORT = AIRPORT_SERVICEalias1.AIRPORT_CODE ) AND CITYalias0.CITY_CODE = AIRPORT_SERVICEalias0.CITY_CODE AND CITYalias0.CITY_NAME = 'PHILADELPHIA' AND FLIGHTalias0.FROM_AIRPORT = AIRPORT_SERVICEalias0.AIRPORT_CODE ;\",\n",
       "  'give me a flight from PHILADELPHIA to DENVER on sunday']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[\"SELECT LAKEalias0.LAKE_NAME FROM LAKE AS LAKEalias0 WHERE LAKEalias0.AREA > 750 AND LAKEalias0.STATE_NAME = 'michigan' ;\",\n",
       "  'name the major lakes in michigan'],\n",
       " ['SELECT HIGHLOWalias0.HIGHEST_POINT , HIGHLOWalias0.STATE_NAME FROM HIGHLOW AS HIGHLOWalias0 WHERE HIGHLOWalias0.LOWEST_ELEVATION = 0 ;',\n",
       "  'what is the highest point in each state whose lowest point is sea level'],\n",
       " ['SELECT COUNT ( CITYalias0.CITY_NAME ) FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION > 150000 ;',\n",
       "  'how many major cities are there']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[\"SELECT LOCATIONalias0.HOUSE_NUMBER , RESTAURANTalias0.NAME FROM LOCATION AS LOCATIONalias0 , RESTAURANT AS RESTAURANTalias0 WHERE LOCATIONalias0.CITY_NAME = 'bethel island' AND LOCATIONalias0.STREET_NAME = 'bethel island rd' AND RESTAURANTalias0.ID = LOCATIONalias0.RESTAURANT_ID AND RESTAURANTalias0.RATING > 2.5 ;\",\n",
       "  'what are some good restaurants on bethel island rd in bethel island ?'],\n",
       " [\"SELECT LOCATIONalias0.HOUSE_NUMBER , RESTAURANTalias0.NAME FROM LOCATION AS LOCATIONalias0 , RESTAURANT AS RESTAURANTalias0 WHERE LOCATIONalias0.CITY_NAME = 'bethel island' AND LOCATIONalias0.STREET_NAME = 'bethel island rd' AND RESTAURANTalias0.ID = LOCATIONalias0.RESTAURANT_ID AND RESTAURANTalias0.RATING > 2.5 ;\",\n",
       "  'give me some good restaurants on bethel island rd in bethel island ?'],\n",
       " [\"SELECT LOCATIONalias0.HOUSE_NUMBER , RESTAURANTalias0.NAME FROM LOCATION AS LOCATIONalias0 , RESTAURANT AS RESTAURANTalias0 WHERE LOCATIONalias0.CITY_NAME = 'bethel island' AND LOCATIONalias0.STREET_NAME = 'bethel island rd' AND RESTAURANTalias0.ID = LOCATIONalias0.RESTAURANT_ID AND RESTAURANTalias0.RATING > 2.5 ;\",\n",
       "  'give me a good restaurant on bethel island rd in bethel island ?']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[\"SELECT DISTINCT PAPERalias0.PAPERID FROM DATASET AS DATASETalias0 , PAPER AS PAPERalias0 , PAPERDATASET AS PAPERDATASETalias0 , VENUE AS VENUEalias0 WHERE DATASETalias0.DATASETNAME = 'RGB-D Object Dataset' AND PAPERDATASETalias0.DATASETID = DATASETalias0.DATASETID AND PAPERalias0.PAPERID = PAPERDATASETalias0.PAPERID AND PAPERalias0.TITLE = 'Class consistent multi-modal fusion with binary features' AND PAPERalias0.YEAR = 2016 AND VENUEalias0.VENUEID = PAPERalias0.VENUEID AND VENUEalias0.VENUENAME = 'CVPR' ;\",\n",
       "  \"What papers were published at CVPR '16 about Class consistent multi-modal fusion with binary features applied to RGB-D Object Dataset ?\"],\n",
       " [\"SELECT DISTINCT PAPERalias0.PAPERID FROM KEYPHRASE AS KEYPHRASEalias0 , PAPER AS PAPERalias0 , PAPERKEYPHRASE AS PAPERKEYPHRASEalias0 , VENUE AS VENUEalias0 WHERE KEYPHRASEalias0.KEYPHRASENAME = 'semantic spaces' AND PAPERKEYPHRASEalias0.KEYPHRASEID = KEYPHRASEalias0.KEYPHRASEID AND PAPERalias0.PAPERID = PAPERKEYPHRASEalias0.PAPERID AND PAPERalias0.YEAR = 2015 AND VENUEalias0.VENUEID = PAPERalias0.VENUEID AND VENUEalias0.VENUENAME = 'arxiv' ;\",\n",
       "  'papers on semantic spaces appeared at arxiv last year'],\n",
       " [\"SELECT DISTINCT PAPERKEYPHRASEalias0.KEYPHRASEID FROM AUTHOR AS AUTHORalias0 , PAPER AS PAPERalias0 , PAPERKEYPHRASE AS PAPERKEYPHRASEalias0 , WRITES AS WRITESalias0 WHERE AUTHORalias0.AUTHORNAME = 'emina torlak' AND PAPERalias0.PAPERID = PAPERKEYPHRASEalias0.PAPERID AND PAPERalias0.YEAR = 2015 AND WRITESalias0.AUTHORID = AUTHORalias0.AUTHORID AND WRITESalias0.PAPERID = PAPERalias0.PAPERID ;\",\n",
       "  'keyphrases emina torlak used in papers written last year']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[\"SELECT MOVIEalias0.TITLE FROM COMPANY AS COMPANYalias0 , COPYRIGHT AS COPYRIGHTalias0 , MOVIE AS MOVIEalias0 WHERE COMPANYalias0.NAME = 'company_name0' AND COPYRIGHTalias0.CID = COMPANYalias0.ID AND MOVIEalias0.MID = COPYRIGHTalias0.MSID AND MOVIEalias0.RELEASE_YEAR > 2010 ;\",\n",
       "  'Find all movies produced by \" Walt Disney \" after 2010']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[\"SELECT DISTINCT COURSEalias0.DEPARTMENT , COURSEalias0.NAME , COURSEalias0.NUMBER FROM COURSE AS COURSEalias0 , COURSE_OFFERING AS COURSE_OFFERINGalias0 , PROGRAM_COURSE AS PROGRAM_COURSEalias0 , SEMESTER AS SEMESTERalias0 WHERE COURSEalias0.COURSE_ID = COURSE_OFFERINGalias0.COURSE_ID AND PROGRAM_COURSEalias0.CATEGORY LIKE '%ULCS%' AND PROGRAM_COURSEalias0.COURSE_ID = COURSEalias0.COURSE_ID AND SEMESTERalias0.SEMESTER = 'FA' AND SEMESTERalias0.SEMESTER_ID = COURSE_OFFERINGalias0.SEMESTER AND SEMESTERalias0.YEAR = 2016 GROUP BY COURSEalias0.DEPARTMENT , COURSEalias0.NUMBER ;\",\n",
       "  'What classes next semester are available as ULCS ?'],\n",
       " [\"SELECT DISTINCT COURSEalias0.DEPARTMENT , COURSEalias0.NAME , COURSEalias0.NUMBER FROM COURSE AS COURSEalias0 , COURSE_OFFERING AS COURSE_OFFERINGalias0 , PROGRAM_COURSE AS PROGRAM_COURSEalias0 , SEMESTER AS SEMESTERalias0 WHERE COURSEalias0.COURSE_ID = COURSE_OFFERINGalias0.COURSE_ID AND PROGRAM_COURSEalias0.CATEGORY LIKE '%ULCS%' AND PROGRAM_COURSEalias0.COURSE_ID = COURSEalias0.COURSE_ID AND SEMESTERalias0.SEMESTER = 'FA' AND SEMESTERalias0.SEMESTER_ID = COURSE_OFFERINGalias0.SEMESTER AND SEMESTERalias0.YEAR = 2016 GROUP BY COURSEalias0.DEPARTMENT , COURSEalias0.NUMBER ;\",\n",
       "  'Any available ULCS next semester ?'],\n",
       " [\"SELECT DISTINCT COURSEalias0.DEPARTMENT , COURSEalias0.NAME , COURSEalias0.NUMBER FROM COURSE AS COURSEalias0 , COURSE_OFFERING AS COURSE_OFFERINGalias0 , PROGRAM_COURSE AS PROGRAM_COURSEalias0 , SEMESTER AS SEMESTERalias0 WHERE COURSEalias0.COURSE_ID = COURSE_OFFERINGalias0.COURSE_ID AND PROGRAM_COURSEalias0.CATEGORY LIKE '%PreMajor%' AND PROGRAM_COURSEalias0.COURSE_ID = COURSEalias0.COURSE_ID AND SEMESTERalias0.SEMESTER = 'FA' AND SEMESTERalias0.SEMESTER_ID = COURSE_OFFERINGalias0.SEMESTER AND SEMESTERalias0.YEAR = 2016 GROUP BY COURSEalias0.DEPARTMENT , COURSEalias0.NUMBER ;\",\n",
       "  'Are any classes available next semester for PreMajor fulfillment ?']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# not copiable queries are usually caused by some dataset convention, not synonym\n",
    "# for atis, caused by date transformations and default dates\n",
    "display([[x['query'], x['question']] for x in final_data['atis'][0] if filter_in_suhr(x)=='not copiable'][:3])\n",
    "# for geography, caused by convention such as major city means city with population > 150000\n",
    "display([[x['query'], x['question']] for x in final_data['geography'][0] if filter_in_suhr(x)=='not copiable'][:3])\n",
    "# for restaurants, caused by convention such as good restaurants means rating > 2.5\n",
    "display([[x['query'], x['question']] for x in final_data['restaurants'][0] if filter_in_suhr(x)=='not copiable'][:3])\n",
    "# for scholar, caused by convention such as last year\n",
    "display([[x['query'], x['question']] for x in final_data['scholar'][0] if filter_in_suhr(x)=='not copiable'][:3])\n",
    "# for imdb, only one\n",
    "display([[x['query'], x['question']] for x in final_data['imdb'][0] if filter_in_suhr(x)=='not copiable'][:3])\n",
    "# for advising, caused by convention such as next semester is 2016 Fall\n",
    "display([[x['query'], x['question']] for x in final_data['advising'][0] if filter_in_suhr(x)=='not copiable'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atis 319\n",
      "geography 541\n",
      "restaurants 168\n",
      "scholar 546\n",
      "imdb 127\n",
      "yelp 122\n",
      "advising 886\n",
      "academic 179\n"
     ]
    }
   ],
   "source": [
    "for dataset_id, data in final_data.items():\n",
    "    data_to_dump = []\n",
    "    db = dump_db_json_schema('/home/t-xiaden/workspace/text2sql-data/data/%s.db'%dataset_id,dataset_id)\n",
    "    db = [db]\n",
    "#     with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/%s_tables.json'%dataset_id, 'w') as f:\n",
    "#         json.dump(db, f)\n",
    "    for example in data[0]:\n",
    "        if filter_in_suhr(example) in ['pass', 'empty result', 'too many selects']:\n",
    "            fixed_sql = all_fix(example['query'])\n",
    "            # remove \"is null, is not null\", only affect few examples in atis\n",
    "            if ' IS ' in fixed_sql:\n",
    "                continue\n",
    "            data_to_dump.append({\n",
    "                \"db_id\": dataset_id,\n",
    "                \"query\": fixed_sql,\n",
    "                \"query_toks\": fixed_sql.split(),\n",
    "                \"question\": example['question'],\n",
    "                \"question_toks\": example['question_toks'],\n",
    "                \"sql\": example['sql']})\n",
    "    print(dataset_id, len(data_to_dump))\n",
    "    with open('/home/t-xiaden/workspace/featurestorage/data/spider-20200607/%s_dev.json'%dataset_id, 'w') as f:\n",
    "        json.dump(data_to_dump, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create index when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atis\n",
      "idx_aircraft_aircraft_code\n",
      "idx_aircraft_aircraft_description\n",
      "idx_aircraft_manufacturer\n",
      "idx_aircraft_basic_type\n",
      "idx_aircraft_engines\n",
      "idx_aircraft_propulsion\n",
      "idx_aircraft_wide_body\n",
      "idx_aircraft_wing_span\n",
      "idx_aircraft_length\n",
      "idx_aircraft_weight\n",
      "idx_aircraft_capacity\n",
      "idx_aircraft_pay_load\n",
      "idx_aircraft_cruising_speed\n",
      "idx_aircraft_range_miles\n",
      "idx_aircraft_pressurized\n",
      "idx_airline_airline_code\n",
      "idx_airline_airline_name\n",
      "idx_airline_note\n",
      "idx_airport_airport_code\n",
      "idx_airport_airport_name\n",
      "idx_airport_airport_location\n",
      "idx_airport_state_code\n",
      "idx_airport_country_name\n",
      "idx_airport_time_zone_code\n",
      "idx_airport_minimum_connect_time\n",
      "idx_airport_service_city_code\n",
      "idx_airport_service_airport_code\n",
      "idx_airport_service_miles_distant\n",
      "idx_airport_service_direction\n",
      "idx_airport_service_minutes_distant\n",
      "idx_city_city_code\n",
      "idx_city_city_name\n",
      "idx_city_state_code\n",
      "idx_city_country_name\n",
      "idx_city_time_zone_code\n",
      "idx_class_of_service_rank\n",
      "idx_class_of_service_class_description\n",
      "idx_code_description_description\n",
      "idx_compartment_class_compartment\n",
      "idx_compartment_class_class_type\n",
      "idx_date_day_month_number\n",
      "idx_date_day_day_number\n",
      "idx_date_day_year\n",
      "idx_date_day_day_name\n",
      "idx_days_days_code\n",
      "idx_days_day_name\n",
      "idx_dual_carrier_main_airline\n",
      "idx_dual_carrier_low_flight_number\n",
      "idx_dual_carrier_high_flight_number\n",
      "idx_dual_carrier_dual_airline\n",
      "idx_dual_carrier_service_name\n",
      "idx_equipment_sequence_aircraft_code_sequence\n",
      "idx_equipment_sequence_aircraft_code\n",
      "idx_fare_from_airport\n",
      "idx_fare_to_airport\n",
      "idx_fare_fare_basis_code\n",
      "idx_fare_fare_airline\n",
      "idx_fare_restriction_code\n",
      "idx_fare_one_direction_cost\n",
      "idx_fare_round_trip_cost\n",
      "idx_fare_round_trip_required\n",
      "idx_fare_basis_fare_basis_code\n",
      "idx_fare_basis_booking_class\n",
      "idx_fare_basis_class_type\n",
      "idx_fare_basis_premium\n",
      "idx_fare_basis_economy\n",
      "idx_fare_basis_discounted\n",
      "idx_fare_basis_night\n",
      "idx_fare_basis_season\n",
      "idx_fare_basis_basis_days\n",
      "idx_flight_flight_days\n",
      "idx_flight_from_airport\n",
      "idx_flight_to_airport\n",
      "idx_flight_departure_time\n",
      "idx_flight_arrival_time\n",
      "idx_flight_airline_flight\n",
      "idx_flight_airline_code\n",
      "idx_flight_flight_number\n",
      "idx_flight_aircraft_code_sequence\n",
      "idx_flight_meal_code\n",
      "idx_flight_stops\n",
      "idx_flight_connections\n",
      "idx_flight_dual_carrier\n",
      "idx_flight_time_elapsed\n",
      "idx_flight_fare_flight_id\n",
      "idx_flight_fare_fare_id\n",
      "idx_flight_leg_flight_id\n",
      "idx_flight_leg_leg_number\n",
      "idx_flight_leg_leg_flight\n",
      "idx_flight_stop_flight_id\n",
      "idx_flight_stop_stop_number\n",
      "idx_flight_stop_stop_days\n",
      "idx_flight_stop_stop_airport\n",
      "idx_flight_stop_arrival_time\n",
      "idx_flight_stop_arrival_airline\n",
      "idx_flight_stop_arrival_flight_number\n",
      "idx_flight_stop_departure_time\n",
      "idx_flight_stop_departure_airline\n",
      "idx_flight_stop_departure_flight_number\n",
      "idx_flight_stop_stop_time\n",
      "idx_food_service_meal_code\n",
      "idx_food_service_meal_number\n",
      "idx_food_service_compartment\n",
      "idx_food_service_meal_description\n",
      "idx_ground_service_city_code\n",
      "idx_ground_service_airport_code\n",
      "idx_ground_service_transport_type\n",
      "idx_ground_service_ground_fare\n",
      "idx_month_month_number\n",
      "idx_month_month_name\n",
      "idx_restriction_restriction_code\n",
      "idx_restriction_advance_purchase\n",
      "idx_restriction_stopovers\n",
      "idx_restriction_saturday_stay_required\n",
      "idx_restriction_minimum_stay\n",
      "idx_restriction_maximum_stay\n",
      "idx_restriction_application\n",
      "idx_restriction_no_discounts\n",
      "idx_state_state_code\n",
      "idx_state_state_name\n",
      "idx_state_country_name\n",
      "idx_time_interval_period\n",
      "idx_time_interval_begin_time\n",
      "idx_time_interval_end_time\n",
      "idx_time_zone_time_zone_code\n",
      "idx_time_zone_time_zone_name\n",
      "idx_time_zone_hours_from_gmt\n",
      "geography\n",
      "idx_border_info_state_name\n",
      "idx_border_info_border\n",
      "idx_city_city_name\n",
      "idx_city_population\n",
      "idx_city_country_name\n",
      "idx_city_state_name\n",
      "idx_highlow_state_name\n",
      "idx_highlow_highest_elevation\n",
      "idx_highlow_lowest_point\n",
      "idx_highlow_highest_point\n",
      "idx_highlow_lowest_elevation\n",
      "idx_lake_lake_name\n",
      "idx_lake_area\n",
      "idx_lake_country_name\n",
      "idx_lake_state_name\n",
      "idx_mountain_mountain_name\n",
      "idx_mountain_mountain_altitude\n",
      "idx_mountain_country_name\n",
      "idx_mountain_state_name\n",
      "idx_river_river_name\n",
      "idx_river_length\n",
      "idx_river_country_name\n",
      "idx_river_traverse\n",
      "idx_state_state_name\n",
      "idx_state_population\n",
      "idx_state_area\n",
      "idx_state_country_name\n",
      "idx_state_capital\n",
      "idx_state_density\n",
      "restaurants\n",
      "idx_GEOGRAPHIC_COUNTY\n",
      "idx_GEOGRAPHIC_REGION\n",
      "idx_RESTAURANT_NAME\n",
      "idx_RESTAURANT_FOOD_TYPE\n",
      "idx_RESTAURANT_CITY_NAME\n",
      "idx_RESTAURANT_RATING\n",
      "idx_LOCATION_HOUSE_NUMBER\n",
      "idx_LOCATION_STREET_NAME\n",
      "idx_LOCATION_CITY_NAME\n",
      "scholar\n",
      "idx_author_authorName\n",
      "idx_dataset_datasetName\n",
      "idx_journal_journalName\n",
      "idx_keyphrase_keyphraseName\n",
      "idx_paper_title\n",
      "idx_paper_venueId\n",
      "idx_paper_year\n",
      "idx_paper_numCiting\n",
      "idx_paper_numCitedBy\n",
      "idx_paper_journalId\n",
      "idx_paperDataset_paperId\n",
      "idx_paperDataset_datasetId\n",
      "idx_paperKeyphrase_paperId\n",
      "idx_paperKeyphrase_keyphraseId\n",
      "idx_venue_venueId\n",
      "idx_venue_venueName\n",
      "idx_writes_paperId\n",
      "idx_writes_authorId\n",
      "imdb\n",
      "idx_actor_gender\n",
      "idx_actor_name\n",
      "idx_actor_nationality\n",
      "idx_actor_birth_city\n",
      "idx_actor_birth_year\n",
      "idx_cast_msid\n",
      "idx_cast_aid\n",
      "idx_cast_role\n",
      "idx_sqlite_sequence_name\n",
      "table sqlite_sequence may not be indexed\n",
      "idx_sqlite_sequence_seq\n",
      "table sqlite_sequence may not be indexed\n",
      "idx_classification_msid\n",
      "idx_classification_gid\n",
      "idx_company_name\n",
      "idx_company_country_code\n",
      "idx_copyright_msid\n",
      "idx_copyright_cid\n",
      "idx_directed_by_msid\n",
      "idx_directed_by_did\n",
      "idx_director_gender\n",
      "idx_director_name\n",
      "idx_director_nationality\n",
      "idx_director_birth_city\n",
      "idx_director_birth_year\n",
      "idx_genre_genre\n",
      "idx_keyword_keyword\n",
      "idx_made_by_msid\n",
      "idx_made_by_pid\n",
      "idx_movie_title\n",
      "idx_movie_release_year\n",
      "idx_movie_title_aka\n",
      "idx_movie_budget\n",
      "idx_producer_gender\n",
      "idx_producer_name\n",
      "idx_producer_nationality\n",
      "idx_producer_birth_city\n",
      "idx_producer_birth_year\n",
      "idx_tags_msid\n",
      "idx_tags_kid\n",
      "idx_tv_series_title\n",
      "idx_tv_series_release_year\n",
      "idx_tv_series_num_of_seasons\n",
      "idx_tv_series_num_of_episodes\n",
      "idx_tv_series_title_aka\n",
      "idx_tv_series_budget\n",
      "idx_writer_gender\n",
      "idx_writer_name\n",
      "idx_writer_nationality\n",
      "idx_writer_birth_city\n",
      "idx_writer_birth_year\n",
      "idx_written_by_msid\n",
      "idx_written_by_wid\n",
      "yelp\n",
      "idx_business_business_id\n",
      "idx_business_name\n",
      "idx_business_full_address\n",
      "idx_business_city\n",
      "idx_business_latitude\n",
      "idx_business_longitude\n",
      "idx_business_review_count\n",
      "idx_business_is_open\n",
      "idx_business_rating\n",
      "idx_business_state\n",
      "idx_sqlite_sequence_name\n",
      "table sqlite_sequence may not be indexed\n",
      "idx_sqlite_sequence_seq\n",
      "table sqlite_sequence may not be indexed\n",
      "idx_category_business_id\n",
      "idx_category_category_name\n",
      "idx_checkin_business_id\n",
      "idx_checkin_count\n",
      "idx_checkin_day\n",
      "idx_neighborhood_business_id\n",
      "idx_neighborhood_neighborhood_name\n",
      "idx_review_business_id\n",
      "idx_review_user_id\n",
      "idx_review_rating\n",
      "idx_review_text\n",
      "idx_review_year\n",
      "idx_review_month\n",
      "idx_tip_business_id\n",
      "idx_tip_text\n",
      "idx_tip_user_id\n",
      "idx_tip_likes\n",
      "idx_tip_year\n",
      "idx_tip_month\n",
      "idx_user_user_id\n",
      "idx_user_name\n",
      "advising\n",
      "idx_AREA_course_id\n",
      "idx_AREA_area\n",
      "idx_COMMENT_INSTRUCTOR_score\n",
      "idx_COMMENT_INSTRUCTOR_comment_text\n",
      "idx_COURSE_NAME\n",
      "idx_COURSE_DEPARTMENT\n",
      "idx_COURSE_NUMBER\n",
      "idx_COURSE_CREDITS\n",
      "idx_COURSE_ADVISORY_REQUIREMENT\n",
      "idx_COURSE_ENFORCED_REQUIREMENT\n",
      "idx_COURSE_DESCRIPTION\n",
      "idx_COURSE_NUM_SEMESTERS\n",
      "idx_COURSE_NUM_ENROLLED\n",
      "idx_COURSE_HAS_DISCUSSION\n",
      "idx_COURSE_HAS_LAB\n",
      "idx_COURSE_HAS_PROJECTS\n",
      "idx_COURSE_HAS_EXAMS\n",
      "idx_COURSE_NUM_REVIEWS\n",
      "idx_COURSE_CLARITY_SCORE\n",
      "idx_COURSE_EASINESS_SCORE\n",
      "idx_COURSE_HELPFULNESS_SCORE\n",
      "idx_COURSE_OFFERING_COURSE_ID\n",
      "idx_COURSE_OFFERING_SEMESTER\n",
      "idx_COURSE_OFFERING_SECTION_NUMBER\n",
      "idx_COURSE_OFFERING_START_TIME\n",
      "idx_COURSE_OFFERING_END_TIME\n",
      "idx_COURSE_OFFERING_MONDAY\n",
      "idx_COURSE_OFFERING_TUESDAY\n",
      "idx_COURSE_OFFERING_WEDNESDAY\n",
      "idx_COURSE_OFFERING_THURSDAY\n",
      "idx_COURSE_OFFERING_FRIDAY\n",
      "idx_COURSE_OFFERING_SATURDAY\n",
      "idx_COURSE_OFFERING_SUNDAY\n",
      "idx_COURSE_OFFERING_HAS_FINAL_PROJECT\n",
      "idx_COURSE_OFFERING_HAS_FINAL_EXAM\n",
      "idx_COURSE_OFFERING_TEXTBOOK\n",
      "idx_COURSE_OFFERING_CLASS_ADDRESS\n",
      "idx_COURSE_OFFERING_ALLOW_AUDIT\n",
      "idx_COURSE_TAGS_COUNT_CLEAR_GRADING\n",
      "idx_COURSE_TAGS_COUNT_POP_QUIZ\n",
      "idx_COURSE_TAGS_COUNT_GROUP_PROJECTS\n",
      "idx_COURSE_TAGS_COUNT_INSPIRATIONAL\n",
      "idx_COURSE_TAGS_COUNT_LONG_LECTURES\n",
      "idx_COURSE_TAGS_COUNT_EXTRA_CREDIT\n",
      "idx_COURSE_TAGS_COUNT_FEW_TESTS\n",
      "idx_COURSE_TAGS_COUNT_GOOD_FEEDBACK\n",
      "idx_COURSE_TAGS_COUNT_TOUGH_TESTS\n",
      "idx_COURSE_TAGS_COUNT_HEAVY_PAPERS\n",
      "idx_COURSE_TAGS_COUNT_CARES_FOR_STUDENTS\n",
      "idx_COURSE_TAGS_COUNT_HEAVY_ASSIGNMENTS\n",
      "idx_COURSE_TAGS_COUNT_RESPECTED\n",
      "idx_COURSE_TAGS_COUNT_PARTICIPATION\n",
      "idx_COURSE_TAGS_COUNT_HEAVY_READING\n",
      "idx_COURSE_TAGS_COUNT_TOUGH_GRADER\n",
      "idx_COURSE_TAGS_COUNT_HILARIOUS\n",
      "idx_COURSE_TAGS_COUNT_WOULD_TAKE_AGAIN\n",
      "idx_COURSE_TAGS_COUNT_GOOD_LECTURE\n",
      "idx_COURSE_TAGS_COUNT_NO_SKIP\n",
      "idx_INSTRUCTOR_NAME\n",
      "idx_INSTRUCTOR_UNIQNAME\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_OFFERING_INSTRUCTOR_OFFERING_ID\n",
      "idx_OFFERING_INSTRUCTOR_INSTRUCTOR_ID\n",
      "idx_PROGRAM_name\n",
      "idx_PROGRAM_college\n",
      "idx_PROGRAM_introduction\n",
      "idx_PROGRAM_COURSE_workload\n",
      "idx_PROGRAM_REQUIREMENT_min_credit\n",
      "idx_PROGRAM_REQUIREMENT_additional_req\n",
      "idx_SEMESTER_semester\n",
      "idx_SEMESTER_year\n",
      "idx_STUDENT_lastname\n",
      "idx_STUDENT_firstname\n",
      "idx_STUDENT_program_id\n",
      "idx_STUDENT_declare_major\n",
      "idx_STUDENT_total_credit\n",
      "idx_STUDENT_total_gpa\n",
      "idx_STUDENT_entered_as\n",
      "idx_STUDENT_admit_term\n",
      "idx_STUDENT_predicted_graduation_semester\n",
      "idx_STUDENT_degree\n",
      "idx_STUDENT_minor\n",
      "idx_STUDENT_internship\n",
      "idx_STUDENT_RECORD_semester\n",
      "idx_STUDENT_RECORD_grade\n",
      "idx_STUDENT_RECORD_how\n",
      "idx_STUDENT_RECORD_transfer_source\n",
      "idx_STUDENT_RECORD_repeat_term\n",
      "idx_STUDENT_RECORD_test_id\n",
      "idx_STUDENT_RECORD_offering_id\n",
      "academic\n",
      "idx_author_name\n",
      "idx_author_oid\n",
      "idx_author_homepage\n",
      "idx_author_photo\n",
      "idx_conference_name\n",
      "idx_conference_full_name\n",
      "idx_conference_homepage\n",
      "idx_domain_name\n",
      "idx_ids_exist\n",
      "idx_journal_name\n",
      "idx_journal_full_name\n",
      "idx_journal_homepage\n",
      "idx_keyword_keyword\n",
      "idx_keyword_keyword_short\n",
      "idx_organization_name\n",
      "idx_organization_continent\n",
      "idx_organization_homepage\n",
      "idx_publication_title\n",
      "idx_publication_abstract\n",
      "idx_publication_year\n",
      "idx_publication_cid\n",
      "idx_publication_jid\n",
      "idx_publication_reference_num\n",
      "idx_publication_citation_num\n",
      "idx_publication_doi\n"
     ]
    }
   ],
   "source": [
    "for dataset_id in datasets:\n",
    "    print(dataset_id)\n",
    "    with sqlite3.connect('/home/t-xiaden/workspace/text2sql-data/data/%s.db'%dataset_id) as conn:\n",
    "        conn.text_factory = lambda x:str(x, 'latin1')\n",
    "        cursor = conn.cursor()\n",
    "        meta = cursor.execute(\"SELECT * FROM sqlite_master\").fetchall()\n",
    "        tables = [x[2] for x in meta if x[0]=='table']\n",
    "        try:\n",
    "            indexes = [(x[2],re.findall(r'`\\w+`',x[4])[0].strip('`')) for x in meta if x[0]=='index' and x[4] is not None]\n",
    "        except:\n",
    "            indexes = [(x[2],re.findall(r'\"\\w+\"',x[4])[0].strip('\"')) for x in meta if x[0]=='index' and x[4] is not None]\n",
    "        for table in tables:\n",
    "            columns = cursor.execute(\"pragma table_info(%s)\"%table).fetchall()\n",
    "            columns = [x[1] for x in columns if x[-1]==0]\n",
    "            for x in columns:\n",
    "                index_name = 'idx_{}_{}'.format(table,x)\n",
    "                print(index_name)\n",
    "                try:\n",
    "                    cursor.execute(\"CREATE INDEX IF NOT EXISTS \\\"{}\\\" ON \\\"{}\\\" (`{}`)\".format(index_name, table, x))\n",
    "                    cursor.execute(\"CREATE INDEX IF NOT EXISTS \\\"{}\\\" ON \\\"{}\\\" (`{}` COLLATE NOCASE)\".format(index_name+'_nocase', table, x))\n",
    "                except Exception as e:\n",
    "                    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'paperId', 'integer', 1, None, 1),\n",
       " (1, 'title', 'varchar(300)', 0, 'NULL', 0),\n",
       " (2, 'venueId', 'integer', 0, 'NULL', 0),\n",
       " (3, 'year', 'integer', 0, 'NULL', 0),\n",
       " (4, 'numCiting', 'integer', 0, 'NULL', 0),\n",
       " (5, 'numCitedBy', 'integer', 0, 'NULL', 0),\n",
       " (6, 'journalId', 'integer', 0, 'NULL', 0)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"pragma table_info(%s)\"%\"paper\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuild foreign key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atis\n",
      "geography\n",
      "restaurants\n",
      "scholar\n",
      "imdb\n",
      "yelp\n",
      "advising\n",
      "academic\n"
     ]
    }
   ],
   "source": [
    "dbs = {}\n",
    "for dataset_id in datasets:\n",
    "    print(dataset_id)\n",
    "    db = dump_db_json_schema('/home/t-xiaden/workspace/text2sql-data/data/%s.db'%dataset_id,dataset_id)\n",
    "    db = [db]\n",
    "    dbs[dataset_id] = db\n",
    "fk_key = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dbs['atis'][0]\n",
    "column_mapping = {}\n",
    "for i, (tab_id, col_name) in enumerate(db['column_names_original']):\n",
    "    if col_name not in column_mapping:\n",
    "        column_mapping[col_name] = []\n",
    "    column_mapping[col_name].append([0, i, db['table_names_original'][tab_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreign key relation for atis\n",
    "fk_key['atis'] = {\n",
    "    'aircraft_code': [[1, 1, 'aircraft'], [0, 55, 'equipment_sequence']],\n",
    "    'airline_code': [[1, 16, 'airline'], [0, 81, 'flight']],\n",
    "    'airport_code': [[1, 19, 'airport'],\n",
    "                    [0, 27, 'airport_service'],\n",
    "                    [0, 110, 'ground_service']],\n",
    "    'state_code': [[0, 22, 'airport'], [0, 33, 'city'], [1, 123, 'state']],\n",
    "    'country_name': [[0, 23, 'airport'], [0, 34, 'city'], [1, 125, 'state']],\n",
    "    'time_zone_code': [[0, 24, 'airport'],\n",
    "                    [0, 35, 'city'],\n",
    "                    [1, 129, 'time_zone']],\n",
    "    'city_code': [[0, 26, 'airport_service'],\n",
    "                [1, 31, 'city'],\n",
    "                [0, 109, 'ground_service']],\n",
    "    'booking_class': [[1, 36, 'class_of_service'], [0, 66, 'fare_basis']],\n",
    "    'code': [[0, 39, 'code_description']],\n",
    "    'compartment': [[1, 41, 'compartment_class'], [0, 107, 'food_service']],\n",
    "    'class_type': [[1, 42, 'compartment_class'], [0, 67, 'fare_basis']],\n",
    "    'month_number': [[0, 43, 'date_day'], [1, 113, 'month']],\n",
    "    'day_number': [[0, 44, 'date_day']],\n",
    "    'year': [[0, 45, 'date_day']],\n",
    "    'day_name': [[0, 46, 'date_day'], [1, 48, 'days']],\n",
    "    'days_code': [[0, 47, 'days']],\n",
    "    'main_airline': [[1, 16, 'airline'], [0, 49, 'dual_carrier']],\n",
    "    'low_flight_number': [[1, 82, 'flight'], [0, 50, 'dual_carrier']],\n",
    "    'high_flight_number': [[1, 82, 'flight'], [0, 51, 'dual_carrier']],\n",
    "    'dual_airline': [[1, 16, 'airline'], [0, 52, 'dual_carrier']],\n",
    "    'aircraft_code_sequence': [[1, 54, 'equipment_sequence'], [0, 83, 'flight']],\n",
    "    'fare_id': [[1, 56, 'fare'], [0, 90, 'flight_fare']],\n",
    "    'from_airport': [[1, 19, 'airport'], [0, 57, 'fare'], [0, 76, 'flight']],\n",
    "    'to_airport': [[1, 19, 'airport'], [0, 58, 'fare'], [0, 77, 'flight']],\n",
    "    'fare_basis_code': [[0, 59, 'fare'], [1, 65, 'fare_basis']],\n",
    "    'fare_airline': [[1, 16, 'airline'], [0, 60, 'fare']],\n",
    "    'restriction_code': [[0, 61, 'fare'], [1, 115, 'restriction']],\n",
    "    'basis_days': [[1, 47, 'days'], [0, 73, 'fare_basis']],\n",
    "    'flight_id': [[1, 74, 'flight'],\n",
    "                    [0, 89, 'flight_fare'],\n",
    "                    [0, 91, 'flight_leg'],\n",
    "                    [0, 94, 'flight_stop']],\n",
    "    'flight_days': [[1, 47, 'days'], [0, 75, 'flight']],\n",
    "    'departure_time': [[0, 78, 'flight'], [0, 101, 'flight_stop']],\n",
    "    'arrival_time': [[0, 79, 'flight'], [0, 98, 'flight_stop']],\n",
    "    'airline_flight': [[0, 80, 'flight']],\n",
    "    'flight_number': [[0, 82, 'flight']],\n",
    "    'meal_code': [[0, 84, 'flight'], [1, 105, 'food_service']],\n",
    "    'leg_flight': [[1, 74, 'flight'], [0, 93, 'flight_leg']],\n",
    "    'stop_days': [[1, 47, 'days'], [0, 96, 'flight_stop']],\n",
    "    'stop_airport': [[1, 19, 'airport'], [0, 97, 'flight_stop']],\n",
    "    'arrival_airline': [[1, 16, 'airline'], [0, 99, 'flight_stop']],\n",
    "    'arrival_flight_number': [[1, 82, 'flight'], [0, 100, 'flight_stop']],\n",
    "    'departure_airline': [[1, 16, 'airline'], [0, 102, 'flight_stop']],\n",
    "    'departure_flight_number': [[1, 82, 'flight'], [0, 103, 'flight_stop']],\n",
    "    'period': [[0, 126, 'time_interval']],\n",
    "    'begin_time': [[0, 127, 'time_interval']],\n",
    "    'end_time': [[0, 128, 'time_interval']],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, cols in fk_key['atis'].items():\n",
    "    main_key = [x[1] for x in cols if x[0]==1]\n",
    "    assert len(main_key) <= 1\n",
    "    if len(main_key) == 1:\n",
    "        main_key = main_key[0]\n",
    "        for _, col_id, _ in cols:\n",
    "            if col_id != main_key:\n",
    "                dbs['atis'][0]['foreign_keys'].append([col_id, main_key])\n",
    "with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/%s_tables.json'%'atis', 'w') as f:\n",
    "    json.dump(dbs['atis'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dbs['advising'][0]\n",
    "column_mapping = {}\n",
    "for i, (tab_id, col_name) in enumerate(db['column_names_original']):\n",
    "    col_name = col_name.lower()\n",
    "    if col_name not in column_mapping:\n",
    "        column_mapping[col_name] = []\n",
    "    column_mapping[col_name].append([0, i, db['table_names_original'][tab_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "fk_key['advising'] = {\n",
    " 'course_id': [[0, 1, 'AREA'], [1, 7, 'COURSE'], [0, 26, 'COURSE_OFFERING'], [0, 44, 'COURSE_PREREQUISITE'], \n",
    "               [0, 45, 'COURSE_TAGS_COUNT'], [0, 79, 'PROGRAM_COURSE'], [0, 103, 'STUDENT_RECORD']],\n",
    " 'area': [[0, 2, 'AREA']],\n",
    " 'instructor_id': [[0, 3, 'COMMENT_INSTRUCTOR'], [1, 68, 'INSTRUCTOR'], [0, 73, 'OFFERING_INSTRUCTOR']],\n",
    " 'student_id': [[0, 4, 'COMMENT_INSTRUCTOR'], [0, 67, 'GSI'], [1, 89, 'STUDENT'], [0, 102, 'STUDENT_RECORD']],\n",
    " 'score': [[0, 5, 'COMMENT_INSTRUCTOR']],\n",
    " 'comment_text': [[0, 6, 'COMMENT_INSTRUCTOR']],\n",
    " 'name': [[0, 8, 'COURSE'], [0, 69, 'INSTRUCTOR'], [0, 75, 'PROGRAM']],\n",
    " 'department': [[0, 9, 'COURSE']],\n",
    " 'number': [[0, 10, 'COURSE']],\n",
    " 'credits': [[0, 11, 'COURSE']],\n",
    " 'advisory_requirement': [[0, 12, 'COURSE']],\n",
    " 'enforced_requirement': [[0, 13, 'COURSE']],\n",
    " 'description': [[0, 14, 'COURSE']],\n",
    " 'num_semesters': [[0, 15, 'COURSE']],\n",
    " 'num_enrolled': [[0, 16, 'COURSE']],\n",
    " 'has_discussion': [[0, 17, 'COURSE']],\n",
    " 'has_lab': [[0, 18, 'COURSE']],\n",
    " 'has_projects': [[0, 19, 'COURSE']],\n",
    " 'has_exams': [[0, 20, 'COURSE']],\n",
    " 'num_reviews': [[0, 21, 'COURSE']],\n",
    " 'clarity_score': [[0, 22, 'COURSE']],\n",
    " 'easiness_score': [[0, 23, 'COURSE']],\n",
    " 'helpfulness_score': [[0, 24, 'COURSE']],\n",
    " 'offering_id': [[1, 25, 'COURSE_OFFERING'], [0, 72, 'OFFERING_INSTRUCTOR'], [0, 111, 'STUDENT_RECORD']],\n",
    " 'semester': [[0, 27, 'COURSE_OFFERING'], [1, 87, 'SEMESTER'], [0, 104, 'STUDENT_RECORD']],\n",
    " 'section_number': [[0, 28, 'COURSE_OFFERING']],\n",
    " 'start_time': [[0, 29, 'COURSE_OFFERING']],\n",
    " 'end_time': [[0, 30, 'COURSE_OFFERING']],\n",
    " 'monday': [[0, 31, 'COURSE_OFFERING']],\n",
    " 'tuesday': [[0, 32, 'COURSE_OFFERING']],\n",
    " 'wednesday': [[0, 33, 'COURSE_OFFERING']],\n",
    " 'thursday': [[0, 34, 'COURSE_OFFERING']],\n",
    " 'friday': [[0, 35, 'COURSE_OFFERING']],\n",
    " 'saturday': [[0, 36, 'COURSE_OFFERING']],\n",
    " 'sunday': [[0, 37, 'COURSE_OFFERING']],\n",
    " 'has_final_project': [[0, 38, 'COURSE_OFFERING']],\n",
    " 'has_final_exam': [[0, 39, 'COURSE_OFFERING']],\n",
    " 'textbook': [[0, 40, 'COURSE_OFFERING']],\n",
    " 'class_address': [[0, 41, 'COURSE_OFFERING']],\n",
    " 'allow_audit': [[0, 42, 'COURSE_OFFERING']],\n",
    " 'pre_course_id': [[0, 43, 'COURSE_PREREQUISITE']],\n",
    " 'clear_grading': [[0, 46, 'COURSE_TAGS_COUNT']],\n",
    " 'pop_quiz': [[0, 47, 'COURSE_TAGS_COUNT']],\n",
    " 'group_projects': [[0, 48, 'COURSE_TAGS_COUNT']],\n",
    " 'inspirational': [[0, 49, 'COURSE_TAGS_COUNT']],\n",
    " 'long_lectures': [[0, 50, 'COURSE_TAGS_COUNT']],\n",
    " 'extra_credit': [[0, 51, 'COURSE_TAGS_COUNT']],\n",
    " 'few_tests': [[0, 52, 'COURSE_TAGS_COUNT']],\n",
    " 'good_feedback': [[0, 53, 'COURSE_TAGS_COUNT']],\n",
    " 'tough_tests': [[0, 54, 'COURSE_TAGS_COUNT']],\n",
    " 'heavy_papers': [[0, 55, 'COURSE_TAGS_COUNT']],\n",
    " 'cares_for_students': [[0, 56, 'COURSE_TAGS_COUNT']],\n",
    " 'heavy_assignments': [[0, 57, 'COURSE_TAGS_COUNT']],\n",
    " 'respected': [[0, 58, 'COURSE_TAGS_COUNT']],\n",
    " 'participation': [[0, 59, 'COURSE_TAGS_COUNT']],\n",
    " 'heavy_reading': [[0, 60, 'COURSE_TAGS_COUNT']],\n",
    " 'tough_grader': [[0, 61, 'COURSE_TAGS_COUNT']],\n",
    " 'hilarious': [[0, 62, 'COURSE_TAGS_COUNT']],\n",
    " 'would_take_again': [[0, 63, 'COURSE_TAGS_COUNT']],\n",
    " 'good_lecture': [[0, 64, 'COURSE_TAGS_COUNT']],\n",
    " 'no_skip': [[0, 65, 'COURSE_TAGS_COUNT']],\n",
    " 'course_offering_id': [[0, 66, 'GSI']],\n",
    " 'uniqname': [[0, 70, 'INSTRUCTOR']],\n",
    " 'offering_instructor_id': [[0, 71, 'OFFERING_INSTRUCTOR']],\n",
    " 'program_id': [[1, 74, 'PROGRAM'], [0, 78, 'PROGRAM_COURSE'], [0, 82, 'PROGRAM_REQUIREMENT'], [0, 92, 'STUDENT']],\n",
    " 'college': [[0, 76, 'PROGRAM']],\n",
    " 'introduction': [[0, 77, 'PROGRAM']],\n",
    " 'workload': [[0, 80, 'PROGRAM_COURSE']],\n",
    " 'category': [[0, 81, 'PROGRAM_COURSE'], [0, 83, 'PROGRAM_REQUIREMENT']],\n",
    " 'min_credit': [[0, 84, 'PROGRAM_REQUIREMENT']],\n",
    " 'additional_req': [[0, 85, 'PROGRAM_REQUIREMENT']],\n",
    " 'semester_id': [[0, 86, 'SEMESTER']],\n",
    " 'year': [[0, 88, 'SEMESTER']],\n",
    " 'lastname': [[0, 90, 'STUDENT']],\n",
    " 'firstname': [[0, 91, 'STUDENT']],\n",
    " 'declare_major': [[0, 93, 'STUDENT']],\n",
    " 'total_credit': [[0, 94, 'STUDENT']],\n",
    " 'total_gpa': [[0, 95, 'STUDENT']],\n",
    " 'entered_as': [[0, 96, 'STUDENT']],\n",
    " 'admit_term': [[0, 97, 'STUDENT']],\n",
    " 'predicted_graduation_semester': [[0, 98, 'STUDENT']],\n",
    " 'degree': [[0, 99, 'STUDENT']],\n",
    " 'minor': [[0, 100, 'STUDENT']],\n",
    " 'internship': [[0, 101, 'STUDENT']],\n",
    " 'grade': [[0, 105, 'STUDENT_RECORD']],\n",
    " 'how': [[0, 106, 'STUDENT_RECORD']],\n",
    " 'transfer_source': [[0, 107, 'STUDENT_RECORD']],\n",
    " 'earn_credit': [[0, 108, 'STUDENT_RECORD']],\n",
    " 'repeat_term': [[0, 109, 'STUDENT_RECORD']],\n",
    " 'test_id': [[0, 110, 'STUDENT_RECORD']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, cols in fk_key['advising'].items():\n",
    "    main_key = [x[1] for x in cols if x[0]==1]\n",
    "    assert len(main_key) <= 1\n",
    "    if len(main_key) == 1:\n",
    "        main_key = main_key[0]\n",
    "        for _, col_id, _ in cols:\n",
    "            if col_id != main_key:\n",
    "                dbs['advising'][0]['foreign_keys'].append([col_id, main_key])\n",
    "with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/%s_tables.json'%'advising', 'w') as f:\n",
    "    json.dump(dbs['advising'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dbs['geography'][0]\n",
    "column_mapping = {}\n",
    "for i, (tab_id, col_name) in enumerate(db['column_names_original']):\n",
    "    col_name = col_name.lower()\n",
    "    if col_name not in column_mapping:\n",
    "        column_mapping[col_name] = []\n",
    "    column_mapping[col_name].append([0, i, db['table_names_original'][tab_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "fk_key['geography'] = {\n",
    " 'state_name': [[0, 1, 'border_info'], [0, 6, 'city'], [0, 7, 'highlow'], [0, 15, 'lake'], \n",
    "                [0, 19, 'mountain'], [1, 24, 'state']],\n",
    " 'border': [[0, 2, 'border_info']],\n",
    " 'city_name': [[0, 3, 'city']],\n",
    " 'population': [[0, 4, 'city'], [0, 25, 'state']],\n",
    " 'country_name': [[0, 5, 'city'],\n",
    "  [0, 14, 'lake'],\n",
    "  [0, 18, 'mountain'],\n",
    "  [0, 22, 'river'],\n",
    "  [0, 27, 'state']],\n",
    " 'highest_elevation': [[0, 8, 'highlow']],\n",
    " 'lowest_point': [[0, 9, 'highlow']],\n",
    " 'highest_point': [[0, 10, 'highlow']],\n",
    " 'lowest_elevation': [[0, 11, 'highlow']],\n",
    " 'lake_name': [[0, 12, 'lake']],\n",
    " 'area': [[0, 13, 'lake'], [0, 26, 'state']],\n",
    " 'mountain_name': [[0, 16, 'mountain']],\n",
    " 'mountain_altitude': [[0, 17, 'mountain']],\n",
    " 'river_name': [[0, 20, 'river']],\n",
    " 'length': [[0, 21, 'river']],\n",
    " 'traverse': [[0, 23, 'river'], [1, 24, 'state']],\n",
    " 'capital': [[0, 28, 'state']],\n",
    " 'density': [[0, 29, 'state']]}\n",
    "\n",
    "for _, cols in fk_key['geography'].items():\n",
    "    main_key = [x[1] for x in cols if x[0]==1]\n",
    "    assert len(main_key) <= 1\n",
    "    if len(main_key) == 1:\n",
    "        main_key = main_key[0]\n",
    "        for _, col_id, _ in cols:\n",
    "            if col_id != main_key:\n",
    "                dbs['geography'][0]['foreign_keys'].append([col_id, main_key])\n",
    "with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/%s_tables.json'%'geography', 'w') as f:\n",
    "    json.dump(dbs['geography'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dbs['imdb'][0]\n",
    "column_mapping = {}\n",
    "for i, (tab_id, col_name) in enumerate(db['column_names_original']):\n",
    "    col_name = col_name.lower()\n",
    "    if col_name not in column_mapping:\n",
    "        column_mapping[col_name] = []\n",
    "    column_mapping[col_name].append([0, i, db['table_names_original'][tab_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "fk_key['imdb'] = {\n",
    " 'aid': [[1, 1, 'actor'], [0, 9, 'cast']],\n",
    " 'gender': [[0, 2, 'actor'], [0, 26, 'director'], [0, 44, 'producer'], [0, 60, 'writer']],\n",
    " 'name': [[0, 3, 'actor'], [0, 11, 'sqlite_sequence'], [0, 17, 'company'], [0, 27, 'director'],\n",
    "          [0, 45, 'producer'], [0, 61, 'writer']],\n",
    " 'nationality': [[0, 4, 'actor'], [0, 28, 'director'], [0, 46, 'producer'], [0, 62, 'writer']],\n",
    " 'birth_city': [[0, 5, 'actor'], [0, 29, 'director'], [0, 47, 'producer'], [0, 63, 'writer']],\n",
    " 'birth_year': [[0, 6, 'actor'], [0, 30, 'director'], [0, 48, 'producer'], [0, 64, 'writer']],\n",
    " 'id': [[0, 7, 'cast'], [0, 13, 'classification'], [0, 19, 'copyright'],\n",
    "        [0, 22, 'directed_by'],[0, 33, 'keyword'], [0, 35, 'made_by'], [0, 49, 'tags'], [0, 65, 'written_by']],\n",
    " 'msid': [[1, 52, 'tv_series'],[1, 38, 'movie'],[0, 8, 'cast'], [0, 14, 'classification'], [0, 20, 'copyright'], [0, 23, 'directed_by'],\n",
    "          [0, 36, 'made_by'], [0, 50, 'tags'], [0, 66, 'written_by']],\n",
    " 'role': [[0, 10, 'cast']],\n",
    " 'gid': [[0, 15, 'classification'], [1, 31, 'genre']],\n",
    " 'country_code': [[0, 18, 'company']],\n",
    " 'cid': [[0, 21, 'copyright'], [1, 16, 'company']],\n",
    " 'did': [[0, 24, 'directed_by'], [1, 25, 'director']],\n",
    " 'genre': [[0, 32, 'genre']],\n",
    " 'keyword': [[0, 34, 'keyword']],\n",
    " 'pid': [[0, 37, 'made_by'], [1, 43, 'producer']],\n",
    " 'mid': [[0, 38, 'movie']],\n",
    " 'title': [[0, 39, 'movie'], [0, 53, 'tv_series']],\n",
    " 'release_year': [[0, 40, 'movie'], [0, 54, 'tv_series']],\n",
    " 'title_aka': [[0, 41, 'movie'], [0, 57, 'tv_series']],\n",
    " 'budget': [[0, 42, 'movie'], [0, 58, 'tv_series']],\n",
    " 'kid': [[1, 33, 'keyword'], [0, 51, 'tags']],\n",
    " 'sid': [[0, 52, 'tv_series']],\n",
    " 'num_of_seasons': [[0, 55, 'tv_series']],\n",
    " 'num_of_episodes': [[0, 56, 'tv_series']],\n",
    " 'wid': [[1, 59, 'writer'], [0, 67, 'written_by']]}\n",
    "\n",
    "for _, cols in fk_key['imdb'].items():\n",
    "    main_keys = [x[1] for x in cols if x[0]==1]\n",
    "    if len(main_keys) != 0:\n",
    "        for main_key in main_keys:\n",
    "            for is_main, col_id, _ in cols:\n",
    "                if is_main != 1:\n",
    "                    dbs['imdb'][0]['foreign_keys'].append([col_id, main_key])\n",
    "with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/%s_tables.json'%'imdb', 'w') as f:\n",
    "    json.dump(dbs['imdb'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dbs['yelp'][0]\n",
    "column_mapping = {}\n",
    "for i, (tab_id, col_name) in enumerate(db['column_names_original']):\n",
    "    col_name = col_name.lower()\n",
    "    if col_name not in column_mapping:\n",
    "        column_mapping[col_name] = []\n",
    "    column_mapping[col_name].append([0, i, db['table_names_original'][tab_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "fk_key['yelp'] = {\n",
    " 'bid': [[0, 1, 'business']],\n",
    " 'business_id': [[1, 2, 'business'], [0, 15, 'category'], [0, 18, 'checkin'],\n",
    "                 [0, 22, 'neighborhood'], [0, 25, 'review'], [0, 32, 'tip']],\n",
    " 'name': [[0, 3, 'business'], [0, 12, 'sqlite_sequence'], [0, 40, 'user']],\n",
    " 'full_address': [[0, 4, 'business']],\n",
    " 'city': [[0, 5, 'business']],\n",
    " 'latitude': [[0, 6, 'business']],\n",
    " 'longitude': [[0, 7, 'business']],\n",
    " 'review_count': [[0, 8, 'business']],\n",
    " 'is_open': [[0, 9, 'business']],\n",
    " 'rating': [[0, 10, 'business'], [0, 27, 'review']],\n",
    " 'state': [[0, 11, 'business']],\n",
    " 'seq': [[0, 13, 'sqlite_sequence']],\n",
    " 'id': [[0, 14, 'category'], [0, 21, 'neighborhood']],\n",
    " 'category_name': [[0, 16, 'category']],\n",
    " 'cid': [[0, 17, 'checkin']],\n",
    " 'count': [[0, 19, 'checkin']],\n",
    " 'day': [[0, 20, 'checkin']],\n",
    " 'neighborhood_name': [[0, 23, 'neighborhood']],\n",
    " 'rid': [[0, 24, 'review']],\n",
    " 'user_id': [[0, 26, 'review'], [0, 34, 'tip'], [1, 39, 'user']],\n",
    " 'text': [[0, 28, 'review'], [0, 33, 'tip']],\n",
    " 'year': [[0, 29, 'review'], [0, 36, 'tip']],\n",
    " 'month': [[0, 30, 'review'], [0, 37, 'tip']],\n",
    " 'tip_id': [[0, 31, 'tip']],\n",
    " 'likes': [[0, 35, 'tip']],\n",
    " 'uid': [[0, 38, 'user']]}\n",
    "\n",
    "for _, cols in fk_key['yelp'].items():\n",
    "    main_key = [x[1] for x in cols if x[0]==1]\n",
    "    assert len(main_key) <= 1\n",
    "    if len(main_key) == 1:\n",
    "        main_key = main_key[0]\n",
    "        for _, col_id, _ in cols:\n",
    "            if col_id != main_key:\n",
    "                dbs['yelp'][0]['foreign_keys'].append([col_id, main_key])\n",
    "with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/%s_tables.json'%'yelp', 'w') as f:\n",
    "    json.dump(dbs['yelp'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dbs['academic'][0]\n",
    "column_mapping = {}\n",
    "for i, (tab_id, col_name) in enumerate(db['column_names_original']):\n",
    "    col_name = col_name.lower()\n",
    "    if col_name not in column_mapping:\n",
    "        column_mapping[col_name] = []\n",
    "    column_mapping[col_name].append([0, i, db['table_names_original'][tab_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "fk_key['academic'] = {\n",
    " 'aid': [[1, 1, 'author'], [0, 14, 'domain_author'], [0, 52, 'writes']],\n",
    " 'name': [[0, 2, 'author'], [0, 9, 'conference'], [0, 13, 'domain'], [0, 29, 'journal'], [0, 38, 'organization']],\n",
    " 'oid': [[0, 3, 'author'], [1, 37, 'organization']],\n",
    " 'homepage': [[0, 4, 'author'], [0, 11, 'conference'], [0, 31, 'journal'], [0, 40, 'organization']],\n",
    " 'photo': [[0, 5, 'author']],\n",
    " 'citing': [[0, 6, 'cite']],\n",
    " 'cited': [[0, 7, 'cite']],\n",
    " 'cid': [[1, 8, 'conference'], [0, 16, 'domain_conference'], [0, 45, 'publication']],\n",
    " 'full_name': [[0, 10, 'conference'], [0, 30, 'journal']],\n",
    " 'did': [[1, 12, 'domain'], [0, 15, 'domain_author'], [0, 17, 'domain_conference'],\n",
    "         [0, 19, 'domain_journal'], [0, 21, 'domain_keyword'], [0, 24, 'domain_publication']],\n",
    " 'jid': [[0, 18, 'domain_journal'], [1, 28, 'journal'], [0, 46, 'publication']],\n",
    " 'kid': [[0, 20, 'domain_keyword'], [1, 32, 'keyword'], [0, 35, 'keyword_variations'], [0, 51, 'publication_keyword']],\n",
    " 'rank': [[0, 22, 'domain_keyword']],\n",
    " 'pid': [[0, 23, 'domain_publication'], [1, 41, 'publication'], [0, 50, 'publication_keyword'], [0, 53, 'writes']],\n",
    " 'relation': [[0, 25, 'ids']],\n",
    " 'id': [[0, 26, 'ids']],\n",
    " 'exist': [[0, 27, 'ids']],\n",
    " 'keyword': [[0, 33, 'keyword']],\n",
    " 'keyword_short': [[0, 34, 'keyword']],\n",
    " 'variation': [[0, 36, 'keyword_variations']],\n",
    " 'continent': [[0, 39, 'organization']],\n",
    " 'title': [[0, 42, 'publication']],\n",
    " 'abstract': [[0, 43, 'publication']],\n",
    " 'year': [[0, 44, 'publication']],\n",
    " 'reference_num': [[0, 47, 'publication']],\n",
    " 'citation_num': [[0, 48, 'publication']],\n",
    " 'doi': [[0, 49, 'publication']]}\n",
    "\n",
    "for _, cols in fk_key['academic'].items():\n",
    "    main_key = [x[1] for x in cols if x[0]==1]\n",
    "    assert len(main_key) <= 1\n",
    "    if len(main_key) == 1:\n",
    "        main_key = main_key[0]\n",
    "        for _, col_id, _ in cols:\n",
    "            if col_id != main_key:\n",
    "                dbs['academic'][0]['foreign_keys'].append([col_id, main_key])\n",
    "with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/%s_tables.json'%'academic', 'w') as f:\n",
    "    json.dump(dbs['academic'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dbs['scholar'][0]\n",
    "column_mapping = {}\n",
    "for i, (tab_id, col_name) in enumerate(db['column_names_original']):\n",
    "    col_name = col_name.lower()\n",
    "    if col_name not in column_mapping:\n",
    "        column_mapping[col_name] = []\n",
    "    column_mapping[col_name].append([0, i, db['table_names_original'][tab_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "fk_key['scholar'] = {\n",
    " 'authorid': [[1, 1, 'author'], [0, 25, 'writes']],\n",
    " 'authorname': [[0, 2, 'author']],\n",
    " 'citingpaperid': [[1, 11, 'paper'], [0, 3, 'cite']],\n",
    " 'citedpaperid': [[1, 11, 'paper'], [0, 4, 'cite']],\n",
    " 'datasetid': [[1, 5, 'dataset'], [0, 19, 'paperDataset']],\n",
    " 'datasetname': [[0, 6, 'dataset']],\n",
    " 'journalid': [[1, 7, 'journal'], [0, 17, 'paper']],\n",
    " 'journalname': [[0, 8, 'journal']],\n",
    " 'keyphraseid': [[1, 9, 'keyphrase'], [0, 21, 'paperKeyphrase']],\n",
    " 'keyphrasename': [[0, 10, 'keyphrase']],\n",
    " 'paperid': [[1, 11, 'paper'],\n",
    "  [0, 18, 'paperDataset'],\n",
    "  [0, 20, 'paperKeyphrase'],\n",
    "  [0, 24, 'writes']],\n",
    " 'title': [[0, 12, 'paper']],\n",
    " 'venueid': [[0, 13, 'paper'], [1, 22, 'venue']],\n",
    " 'year': [[0, 14, 'paper']],\n",
    " 'numciting': [[0, 15, 'paper']],\n",
    " 'numcitedby': [[0, 16, 'paper']],\n",
    " 'venuename': [[0, 23, 'venue']]}\n",
    "\n",
    "for _, cols in fk_key['scholar'].items():\n",
    "    main_key = [x[1] for x in cols if x[0]==1]\n",
    "    assert len(main_key) <= 1\n",
    "    if len(main_key) == 1:\n",
    "        main_key = main_key[0]\n",
    "        for _, col_id, _ in cols:\n",
    "            if col_id != main_key:\n",
    "                dbs['scholar'][0]['foreign_keys'].append([col_id, main_key])\n",
    "with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/%s_tables.json'%'scholar', 'w') as f:\n",
    "    json.dump(dbs['scholar'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dbs['restaurants'][0]\n",
    "column_mapping = {}\n",
    "for i, (tab_id, col_name) in enumerate(db['column_names_original']):\n",
    "    col_name = col_name.lower()\n",
    "    if col_name not in column_mapping:\n",
    "        column_mapping[col_name] = []\n",
    "    column_mapping[col_name].append([0, i, db['table_names_original'][tab_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "fk_key['restaurants'] = {\n",
    " 'city_name': [[1, 1, 'GEOGRAPHIC'], [0, 7, 'RESTAURANT'], [0, 12, 'LOCATION']],\n",
    " 'county': [[0, 2, 'GEOGRAPHIC']],\n",
    " 'region': [[0, 3, 'GEOGRAPHIC']],\n",
    " 'id': [[0, 4, 'RESTAURANT']],\n",
    " 'name': [[0, 5, 'RESTAURANT']],\n",
    " 'food_type': [[0, 6, 'RESTAURANT']],\n",
    " 'rating': [[0, 8, 'RESTAURANT']],\n",
    " 'restaurant_id': [[1, 4, 'RESTAURANT'], [0, 9, 'LOCATION']],\n",
    " 'house_number': [[0, 10, 'LOCATION']],\n",
    " 'street_name': [[0, 11, 'LOCATION']]}\n",
    "\n",
    "for _, cols in fk_key['restaurants'].items():\n",
    "    main_key = [x[1] for x in cols if x[0]==1]\n",
    "    assert len(main_key) <= 1\n",
    "    if len(main_key) == 1:\n",
    "        main_key = main_key[0]\n",
    "        for _, col_id, _ in cols:\n",
    "            if col_id != main_key:\n",
    "                dbs['restaurants'][0]['foreign_keys'].append([col_id, main_key])\n",
    "with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/%s_tables.json'%'restaurants', 'w') as f:\n",
    "    json.dump(dbs['restaurants'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'restaurants',\n",
       " 'table_names_original': ['GEOGRAPHIC', 'RESTAURANT', 'LOCATION'],\n",
       " 'table_names': ['geographic', 'restaurant', 'location'],\n",
       " 'column_names_original': [(-1, '*'),\n",
       "  (0, 'CITY_NAME'),\n",
       "  (0, 'COUNTY'),\n",
       "  (0, 'REGION'),\n",
       "  (1, 'ID'),\n",
       "  (1, 'NAME'),\n",
       "  (1, 'FOOD_TYPE'),\n",
       "  (1, 'CITY_NAME'),\n",
       "  (1, 'RATING'),\n",
       "  (2, 'RESTAURANT_ID'),\n",
       "  (2, 'HOUSE_NUMBER'),\n",
       "  (2, 'STREET_NAME'),\n",
       "  (2, 'CITY_NAME')],\n",
       " 'column_names': [(-1, '*'),\n",
       "  (0, 'city name'),\n",
       "  (0, 'county'),\n",
       "  (0, 'region'),\n",
       "  (1, 'id'),\n",
       "  (1, 'name'),\n",
       "  (1, 'food type'),\n",
       "  (1, 'city name'),\n",
       "  (1, 'rating'),\n",
       "  (2, 'restaurant id'),\n",
       "  (2, 'house number'),\n",
       "  (2, 'street name'),\n",
       "  (2, 'city name')],\n",
       " 'column_types': ['text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text'],\n",
       " 'primary_keys': [1, 4, 9],\n",
       " 'foreign_keys': [[7, 1], [12, 1]]}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create spider subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/t-xiaden/workspace/featurestorage/data/spider-20200607/dev.json','r') as f:\n",
    "    dev_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_where_cond(sql):\n",
    "    conds = []\n",
    "    for unit in sql['where']:\n",
    "        if isinstance(unit, list):\n",
    "            col = unit[2][1][1]\n",
    "            val1 = unit[3]\n",
    "            val2 = unit[4]\n",
    "            if isinstance(val1, dict):\n",
    "                conds += get_where_cond(val1)\n",
    "                val1 = None\n",
    "            if isinstance(val2, dict):\n",
    "                conds += get_where_cond(val2)\n",
    "                val2 = None\n",
    "            conds.append([col, val1, val2])\n",
    "    if sql['intersect'] is not None:\n",
    "        conds += get_where_cond(sql['intersect'])\n",
    "    if sql['union'] is not None:\n",
    "        conds += get_where_cond(sql['union'])\n",
    "    if sql['except'] is not None:\n",
    "        conds += get_where_cond(sql['except'])\n",
    "    return conds\n",
    "def get_select_col(sql):\n",
    "    return [x[1][1][1] for x in sql['select'][1]]\n",
    "def get_orderby(sql):\n",
    "    cols = []\n",
    "    if sql['orderBy']:\n",
    "        cols += [x[1][1] for x in sql['orderBy'][1]]\n",
    "    if sql['intersect'] is not None:\n",
    "        cols += get_orderby(sql['intersect'])\n",
    "    if sql['union'] is not None:\n",
    "        cols += get_orderby(sql['union'])\n",
    "    if sql['except'] is not None:\n",
    "        cols += get_orderby(sql['except'])\n",
    "    return cols\n",
    "def get_groupby(sql):\n",
    "    cols = []\n",
    "    if sql['groupBy']:\n",
    "        cols += [x[1] for x in sql['groupBy']]\n",
    "    if sql['intersect'] is not None:\n",
    "        cols += get_groupby(sql['intersect'])\n",
    "    if sql['union'] is not None:\n",
    "        cols += get_groupby(sql['union'])\n",
    "    if sql['except'] is not None:\n",
    "        cols += get_groupby(sql['except'])\n",
    "    return cols\n",
    "def get_having_cond(sql):\n",
    "    conds = []\n",
    "    for unit in sql['having']:\n",
    "        if isinstance(unit, list):\n",
    "            col = unit[2][1][1]\n",
    "            val1 = unit[3]\n",
    "            val2 = unit[4]\n",
    "            if isinstance(val1, dict):\n",
    "                conds += get_having_cond(val1)\n",
    "                val1 = None\n",
    "            if isinstance(val2, dict):\n",
    "                conds += get_having_cond(val2)\n",
    "                val2 = None\n",
    "            conds.append([col, val1, val2])\n",
    "    if sql['intersect'] is not None:\n",
    "        conds += get_having_cond(sql['intersect'])\n",
    "    if sql['union'] is not None:\n",
    "        conds += get_having_cond(sql['union'])\n",
    "    if sql['except'] is not None:\n",
    "        conds += get_having_cond(sql['except'])\n",
    "    return conds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isnumber(x):\n",
    "    try:\n",
    "        if isinstance(x, str):\n",
    "            x = x.strip().strip('\"')\n",
    "        float(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spider_dev_tomodify.tsv', 'w') as f:\n",
    "    for i,x in enumerate(dev_data):\n",
    "        where_conds = get_where_cond(x['sql'])\n",
    "        groupby = get_groupby(x['sql'])\n",
    "        orderby = get_orderby(x['sql'])\n",
    "        having_conds = get_having_cond(x['sql'])\n",
    "        orig_q = x['question']\n",
    "        modified_q = dev_modified.get(i,['',''])[1] # reload previous modified question\n",
    "        if where_conds or having_conds or groupby or orderby:\n",
    "            f.write('{}\\t{}\\t{}\\t{}\\n'.format(i, orig_q, modified_q, x['query']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "dev_modified_b = []\n",
    "for idx, x in dev_modified.items():\n",
    "    if x[1]:\n",
    "        dev_modified_b.append(copy.deepcopy(dev_data[idx]))\n",
    "        dev_modified_b[-1]['question'] = x[1]\n",
    "        dev_modified_b[-1]['question_toks'] = x[1].split()\n",
    "with open('/home/t-xiaden/workspace/featurestorage/data/spider-20200607/spider_modified_b_dev.json', 'w') as f:\n",
    "    json.dump(dev_modified_b, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 947/947 [00:00<00:00, 340331.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 246/246 [00:00<00:00, 199728.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geography\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:00<00:00, 24299.49it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 193/193 [00:00<00:00, 138946.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurants\n",
      "scholar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [00:00<00:00, 179813.61it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [00:00<00:00, 198611.04it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [00:00<00:00, 30148.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb\n",
      "yelp\n",
      "advising\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185/185 [00:00<00:00, 256003.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "academic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "link_analysis = {}\n",
    "for dataset_id in datasets:\n",
    "#     if dataset_id == 'scholar':\n",
    "#         continue\n",
    "    print(dataset_id)\n",
    "\n",
    "    with open(os.path.join(datadir, '{}.json'.format(dataset_id))) as f:\n",
    "        data = json.load(f)\n",
    "        pairs = list()\n",
    "\n",
    "    # The UMichigan data is split by anonymized queries, where values are\n",
    "    # anonymized but table/column names are not. However, our experiments are\n",
    "    # performed on the original splits of the data.\n",
    "    nl_to_queryid = {}\n",
    "    with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/%s_tables.json'%dataset_id,'r') as f:\n",
    "        schema = json.load(f)\n",
    "        columns = ['{}.{}'.format(schema[0][\"table_names_original\"][tab_id], col_name) for tab_id, col_name in schema[0][\"column_names_original\"]]\n",
    "    for i, query in enumerate(tqdm(data)):\n",
    "        for example in query['sentences']:\n",
    "            if example['question-split'] not in datasets[dataset_id]:\n",
    "                continue\n",
    "            nl = example['text']\n",
    "            for variable_name, value in sorted(\n",
    "                  example['variables'].items(), key=lambda x: len(x[0]), reverse=True):\n",
    "                if not value:\n",
    "                # TODO(alanesuhr) While the Michigan repo says to use a - here, the\n",
    "                # thing that works is using a % and replacing = with LIKE.\n",
    "                #\n",
    "                # It's possible that I should remove such clauses from the SQL, as\n",
    "                # long as they lead to the same table result. They don't align well\n",
    "                # to the natural language at least.\n",
    "                #\n",
    "                # See: https://github.com/jkkummerfeld/text2sql-data/tree/master/data\n",
    "                    value = '%'\n",
    "\n",
    "                nl = nl.replace(variable_name, value)\n",
    "            nl_to_queryid[nl] = i\n",
    "    with open('../../logdirs/%s_bert_value_run_0_true_1_new_nocolvalue-step40000.eval'%dataset_id,'r') as f:\n",
    "        results = json.load(f)\n",
    "    with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/%s_dev.json'%dataset_id,'r') as f:\n",
    "        origs = json.load(f)\n",
    "    link_analysis[dataset_id] = []\n",
    "    with open('../../logdirs/%s_bert_value_run_0_true_1_new_nocolvalue-step40000.infer'%dataset_id,'r') as f:\n",
    "        with open('../../logdirs/%s_analysis.tsv'%dataset_id, 'w') as f_out:\n",
    "            for i, line in enumerate(f):\n",
    "                inferred = json.loads(line.strip())['beams'][0]\n",
    "                if i == 0:\n",
    "                    f_out.write('query_id\\tquestion\\tgold_query\\tpred_query\\texec\\texact\\t')\n",
    "                    for j,col in enumerate(columns):\n",
    "                        if j!=0:\n",
    "                            f_out.write('{}\\t'.format(' '.join(col)))\n",
    "                    f_out.write('\\n')\n",
    "                pred_query = inferred['inferred_code']\n",
    "                orig = origs[i]\n",
    "                q = orig['question']\n",
    "                select_cols = get_select_col(orig['sql'])\n",
    "                where_cols = [cond[0] for cond in get_where_cond(orig['sql'])]\n",
    "                f_out.write('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t'.format(nl_to_queryid[q],q,orig[\"query\"],pred_query,results['per_item'][i]['exec'],results['per_item'][i]['exact']))\n",
    "                q_map = inferred['preproc_item'][\"question_for_copy\"]\n",
    "                column_links = {j:[] for j in range(len(inferred['preproc_item']['columns']))}\n",
    "                all_links = {j:[] for j in range(len(inferred['preproc_item']['columns']))}\n",
    "                for link, link_type in inferred['preproc_item']['sc_link']['q_col_match'].items():\n",
    "                    q_loc, c_loc = link.split(',')\n",
    "                    column_links[int(c_loc)].append('{}: {}'.format(q_map[int(q_loc)], link_type))\n",
    "                    all_links[int(c_loc)].append(link_type)\n",
    "                for link, link_type in inferred['preproc_item']['cv_link']['num_date_match'].items():\n",
    "                    q_loc, c_loc = link.split(',')\n",
    "                    column_links[int(c_loc)].append('{}: {}'.format(q_map[int(q_loc)], link_type))\n",
    "                    all_links[int(c_loc)].append(link_type)\n",
    "                for link, link_type in inferred['preproc_item']['cv_link']['cell_match'].items():\n",
    "                    q_loc, c_loc = link.split(',')\n",
    "                    column_links[int(c_loc)].append('{}: {}'.format(q_map[int(q_loc)], link_type))\n",
    "                    all_links[int(c_loc)].append(link_type)\n",
    "                for j,col in enumerate(inferred['preproc_item']['columns']):\n",
    "                    if j != 0:\n",
    "                        f_out.write('{}\\t'.format('; '.join(column_links[j])))\n",
    "                f_out.write('\\n')\n",
    "                link_analysis[dataset_id].append([all_links, select_cols, where_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'spider_val'\n",
    "with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/dev.json','r') as f:\n",
    "    origs = json.load(f)\n",
    "link_analysis[dataset_id] = []\n",
    "with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/nl2code-1115,output_from=true,fs=2,emb=bert,cvlink,value,dec_min_freq=10,spideronly,newvalue/enc/val.jsonl','r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        preproc_item = json.loads(line.strip())\n",
    "        orig = origs[i]\n",
    "        db_id = orig['db_id']\n",
    "        select_cols = get_select_col(orig['sql'])\n",
    "        where_cols = [cond[0] for cond in get_where_cond(orig['sql'])]\n",
    "        all_links = {j:[] for j in range(len(preproc_item['columns']))}\n",
    "        for link, link_type in preproc_item['sc_link']['q_col_match'].items():\n",
    "            q_loc, c_loc = link.split(',')\n",
    "            all_links[int(c_loc)].append(link_type)\n",
    "        for link, link_type in preproc_item['cv_link']['num_date_match'].items():\n",
    "            q_loc, c_loc = link.split(',')\n",
    "            all_links[int(c_loc)].append(link_type)\n",
    "        for link, link_type in preproc_item['cv_link']['cell_match'].items():\n",
    "            q_loc, c_loc = link.split(',')\n",
    "            all_links[int(c_loc)].append(link_type)\n",
    "        link_analysis[dataset_id].append([all_links, select_cols, where_cols])\n",
    "        if dataset_id+'_'+db_id not in link_analysis:\n",
    "            link_analysis[dataset_id+'_'+db_id] = []\n",
    "        link_analysis[dataset_id+'_'+db_id].append([all_links, select_cols, where_cols])\n",
    "dataset_id = 'spider_train'\n",
    "with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/train_spider.json','r') as f:\n",
    "    origs = json.load(f)\n",
    "link_analysis[dataset_id] = []\n",
    "with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/nl2code-1115,output_from=true,fs=2,emb=bert,cvlink,value,dec_min_freq=10,spideronly,newvalue/enc/train.jsonl','r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        preproc_item = json.loads(line.strip())\n",
    "        orig = origs[i]\n",
    "        select_cols = get_select_col(orig['sql'])\n",
    "        where_cols = [cond[0] for cond in get_where_cond(orig['sql'])]\n",
    "        all_links = {j:[] for j in range(len(preproc_item['columns']))}\n",
    "        for link, link_type in preproc_item['sc_link']['q_col_match'].items():\n",
    "            q_loc, c_loc = link.split(',')\n",
    "            all_links[int(c_loc)].append(link_type)\n",
    "        for link, link_type in preproc_item['cv_link']['num_date_match'].items():\n",
    "            q_loc, c_loc = link.split(',')\n",
    "            all_links[int(c_loc)].append(link_type)\n",
    "        for link, link_type in preproc_item['cv_link']['cell_match'].items():\n",
    "            q_loc, c_loc = link.split(',')\n",
    "            all_links[int(c_loc)].append(link_type)\n",
    "        link_analysis[dataset_id].append([all_links, select_cols, where_cols])\n",
    "for dataset_id in datasets:\n",
    "    with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/%s_dev.json'%dataset_id,'r') as f:\n",
    "        origs = json.load(f)\n",
    "    link_analysis[dataset_id] = []\n",
    "    with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/nl2code-1115,output_from=true,fs=2,emb=bert,cvlink,value,dec_min_freq=10,spideronly,newvalue/enc/val_%s.jsonl'%dataset_id,'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            preproc_item = json.loads(line.strip())\n",
    "            orig = origs[i]\n",
    "            select_cols = get_select_col(orig['sql'])\n",
    "            where_cols = [cond[0] for cond in get_where_cond(orig['sql'])]\n",
    "            all_links = {j:[] for j in range(len(preproc_item['columns']))}\n",
    "            for link, link_type in preproc_item['sc_link']['q_col_match'].items():\n",
    "                q_loc, c_loc = link.split(',')\n",
    "                all_links[int(c_loc)].append(link_type)\n",
    "            for link, link_type in preproc_item['cv_link']['num_date_match'].items():\n",
    "                q_loc, c_loc = link.split(',')\n",
    "                all_links[int(c_loc)].append(link_type)\n",
    "            for link, link_type in preproc_item['cv_link']['cell_match'].items():\n",
    "                q_loc, c_loc = link.split(',')\n",
    "                all_links[int(c_loc)].append(link_type)\n",
    "            link_analysis[dataset_id].append([all_links, select_cols, where_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'treqs'\n",
    "with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/%s_dev.json'%dataset_id,'r') as f:\n",
    "    origs = json.load(f)\n",
    "link_analysis[dataset_id] = []\n",
    "with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/nl2code-1115,output_from=true,fs=2,emb=bert,cvlink,value,dec_min_freq=10,spideronly,newvalue/enc/val_%s.jsonl'%dataset_id,'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        preproc_item = json.loads(line.strip())\n",
    "        orig = origs[i]\n",
    "        select_cols = get_select_col(orig['sql'])\n",
    "        where_cols = [cond[0] for cond in get_where_cond(orig['sql'])]\n",
    "        all_links = {j:[] for j in range(len(preproc_item['columns']))}\n",
    "        for link, link_type in preproc_item['sc_link']['q_col_match'].items():\n",
    "            q_loc, c_loc = link.split(',')\n",
    "            all_links[int(c_loc)].append(link_type)\n",
    "        for link, link_type in preproc_item['cv_link']['num_date_match'].items():\n",
    "            q_loc, c_loc = link.split(',')\n",
    "            all_links[int(c_loc)].append(link_type)\n",
    "        for link, link_type in preproc_item['cv_link']['cell_match'].items():\n",
    "            q_loc, c_loc = link.split(',')\n",
    "            all_links[int(c_loc)].append(link_type)\n",
    "        link_analysis[dataset_id].append([all_links, select_cols, where_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_where_cond(sql):\n",
    "    conds = []\n",
    "    for unit in sql['where']:\n",
    "        if isinstance(unit, list):\n",
    "            col = unit[2][1][1]\n",
    "            val1 = unit[3]\n",
    "            val2 = unit[4]\n",
    "            if isinstance(val1, dict):\n",
    "                conds += get_where_cond(val1)\n",
    "                val1 = None\n",
    "            if isinstance(val2, dict):\n",
    "                conds += get_where_cond(val2)\n",
    "                val2 = None\n",
    "            conds.append([col, val1, val2])\n",
    "    return conds\n",
    "def get_select_col(sql):\n",
    "    return [x[1][1][1] for x in sql['select'][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_select_col(orig['sql'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13, '\"Databases\"', None], [38, '\"University of Michigan\"', None]]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_where_cond(orig['sql'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atis\tCPM 0.7176\tCEM 0.0000\tANYCELLMATCH 0.0802\tCELLMATCH 0.0802\tCELLTOKENMATCH 0.0000\tNUMBER 0.0000\tTIME 0.0000\tCPM 0.0600\tCEM 0.0030\tANYCELLMATCH 0.4708\tCELLMATCH 0.4708\tCELLTOKENMATCH 0.0030\tNUMBER 0.0465\tTIME 0.0000\tall 0.0495, 0.0880\tCPM 0.0702, 0.0149\tCEM 0.0000, 0.1333\tANYCELLMATCH 0.0167, 0.2500\tCELLMATCH 0.0277, 0.4137\tCELLTOKENMATCH 0.0000, 0.0040\tNUMBER 0.0000, 0.0764\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "geography\tCPM 0.4624\tCEM 0.3237\tANYCELLMATCH 0.2601\tCELLMATCH 0.1561\tCELLTOKENMATCH 0.1098\tNUMBER 0.0000\tTIME 0.0000\tCPM 0.2461\tCEM 0.1061\tANYCELLMATCH 0.5205\tCELLMATCH 0.5205\tCELLTOKENMATCH 0.0113\tNUMBER 0.0000\tTIME 0.0000\tall 0.0988, 0.1151\tCPM 0.1164, 0.0844\tCEM 0.3953, 0.1765\tANYCELLMATCH 0.0557, 0.1519\tCELLMATCH 0.0394, 0.1788\tCELLTOKENMATCH 0.1469, 0.0206\tNUMBER 0.0000, 0.0000\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "restaurants\tCPM 0.0000\tCEM 0.0000\tANYCELLMATCH 0.0000\tCELLMATCH 0.0000\tCELLTOKENMATCH 0.0000\tNUMBER 0.0000\tTIME 0.0000\tCPM 0.0769\tCEM 0.0769\tANYCELLMATCH 0.9615\tCELLMATCH 0.9231\tCELLTOKENMATCH 0.0769\tNUMBER 0.0000\tTIME 0.0000\tall 0.0000, 0.4643\tCPM 0.0000, 0.2857\tCEM 0.0000, 1.0000\tANYCELLMATCH 0.0000, 0.5000\tCELLMATCH 0.0000, 0.6667\tCELLTOKENMATCH 0.0000, 0.1333\tNUMBER 0.0000, 0.0000\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "scholar\tCPM 0.2239\tCEM 0.0102\tANYCELLMATCH 0.0153\tCELLMATCH 0.0102\tCELLTOKENMATCH 0.0051\tNUMBER 0.2417\tTIME 0.0000\tCPM 0.0321\tCEM 0.0018\tANYCELLMATCH 0.7897\tCELLMATCH 0.7861\tCELLTOKENMATCH 0.4742\tNUMBER 0.1622\tTIME 0.0000\tall 0.0468, 0.1392\tCPM 0.2023, 0.0414\tCEM 0.8000, 0.2000\tANYCELLMATCH 0.0032, 0.2358\tCELLMATCH 0.0049, 0.5424\tCELLTOKENMATCH 0.0012, 0.1628\tNUMBER 0.0588, 0.0563\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "imdb\tCPM 0.0667\tCEM 0.0857\tANYCELLMATCH 0.8190\tCELLMATCH 0.5429\tCELLTOKENMATCH 0.7810\tNUMBER 0.0000\tTIME 0.0000\tCPM 0.0483\tCEM 0.0069\tANYCELLMATCH 0.7862\tCELLMATCH 0.7793\tCELLTOKENMATCH 0.4828\tNUMBER 0.1586\tTIME 0.0000\tall 0.0740, 0.1034\tCPM 0.1029, 0.1029\tCEM 0.1875, 0.0208\tANYCELLMATCH 0.0842, 0.1117\tCELLMATCH 0.0864, 0.1712\tCELLTOKENMATCH 0.1001, 0.0855\tNUMBER 0.0000, 0.1022\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "yelp\tCPM 0.0492\tCEM 0.1639\tANYCELLMATCH 0.6721\tCELLMATCH 0.1967\tCELLTOKENMATCH 0.6066\tNUMBER 0.0000\tTIME 0.0000\tCPM 0.0472\tCEM 0.0660\tANYCELLMATCH 0.7170\tCELLMATCH 0.7170\tCELLTOKENMATCH 0.1981\tNUMBER 0.1887\tTIME 0.0000\tall 0.0802, 0.1452\tCPM 0.0221, 0.0368\tCEM 0.3030, 0.2121\tANYCELLMATCH 0.0870, 0.1614\tCELLMATCH 0.0710, 0.4497\tCELLTOKENMATCH 0.0941, 0.0534\tNUMBER 0.0000, 0.2174\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "advising\tCPM 0.1494\tCEM 0.1533\tANYCELLMATCH 0.2682\tCELLMATCH 0.0690\tCELLTOKENMATCH 0.1992\tNUMBER 0.2950\tTIME 0.0000\tCPM 0.0060\tCEM 0.0558\tANYCELLMATCH 0.8088\tCELLMATCH 0.7390\tCELLTOKENMATCH 0.0857\tNUMBER 0.0299\tTIME 0.0000\tall 0.0383, 0.0880\tCPM 0.0545, 0.0042\tCEM 0.2116, 0.1481\tANYCELLMATCH 0.0473, 0.2741\tCELLMATCH 0.0409, 0.8432\tCELLTOKENMATCH 0.0495, 0.0410\tNUMBER 0.0353, 0.0069\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "academic\tCPM 0.0966\tCEM 0.1586\tANYCELLMATCH 0.6966\tCELLMATCH 0.4690\tCELLTOKENMATCH 0.6690\tNUMBER 0.0276\tTIME 0.0000\tCPM 0.0405\tCEM 0.0541\tANYCELLMATCH 0.7387\tCELLMATCH 0.7387\tCELLTOKENMATCH 0.4414\tNUMBER 0.2162\tTIME 0.0000\tall 0.0457, 0.0801\tCPM 0.2414, 0.1552\tCEM 0.4894, 0.2553\tANYCELLMATCH 0.0862, 0.1399\tCELLMATCH 0.1948, 0.4699\tCELLTOKENMATCH 0.0906, 0.0915\tNUMBER 0.0028, 0.0339\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "treqs\tCPM 0.0622\tCEM 0.8693\tANYCELLMATCH 0.0494\tCELLMATCH 0.0009\tCELLTOKENMATCH 0.0484\tNUMBER 0.2605\tTIME 0.0000\tCPM 0.1575\tCEM 0.2855\tANYCELLMATCH 0.6333\tCELLMATCH 0.5787\tCELLTOKENMATCH 0.2215\tNUMBER 0.2860\tTIME 0.0000\tall 0.0944, 0.1539\tCPM 0.0263, 0.1095\tCEM 0.4475, 0.2414\tANYCELLMATCH 0.0103, 0.2165\tCELLMATCH 0.0008, 0.8234\tCELLTOKENMATCH 0.0122, 0.0916\tNUMBER 0.0990, 0.1785\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "spider_val\tCPM 0.3018\tCEM 0.5785\tANYCELLMATCH 0.0710\tCELLMATCH 0.0108\tCELLTOKENMATCH 0.0636\tNUMBER 0.0244\tTIME 0.0210\tCPM 0.3225\tCEM 0.3515\tANYCELLMATCH 0.5461\tCELLMATCH 0.5324\tCELLTOKENMATCH 0.0683\tNUMBER 0.1536\tTIME 0.0000\tall 0.1415, 0.0606\tCPM 0.0877, 0.0372\tCEM 0.4737, 0.1141\tANYCELLMATCH 0.1167, 0.3556\tCELLMATCH 0.0328, 0.6393\tCELLTOKENMATCH 0.2089, 0.0889\tNUMBER 0.0455, 0.1138\tTIME 0.0304, 0.0000\t\n",
      "\n",
      "spider_val_concert_singer\tCPM 0.0779\tCEM 0.6364\tANYCELLMATCH 0.0909\tCELLMATCH 0.0000\tCELLTOKENMATCH 0.0909\tNUMBER 0.0260\tTIME 0.0000\tCPM 0.1739\tCEM 0.4348\tANYCELLMATCH 0.4348\tCELLMATCH 0.3478\tCELLTOKENMATCH 0.0870\tNUMBER 0.1304\tTIME 0.0000\tall 0.1633, 0.0525\tCPM 0.0309, 0.0206\tCEM 0.4537, 0.0926\tANYCELLMATCH 0.2258, 0.3226\tCELLMATCH 0.0000, 0.4000\tCELLTOKENMATCH 0.6364, 0.1818\tNUMBER 0.0606, 0.0909\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "spider_val_pets_1\tCPM 0.2969\tCEM 0.4844\tANYCELLMATCH 0.0000\tCELLMATCH 0.0000\tCELLTOKENMATCH 0.0000\tNUMBER 0.0000\tTIME 0.0000\tCPM 0.5294\tCEM 0.1765\tANYCELLMATCH 0.4118\tCELLMATCH 0.4118\tCELLTOKENMATCH 0.0000\tNUMBER 0.2353\tTIME 0.0000\tall 0.1818, 0.1098\tCPM 0.0872, 0.0826\tCEM 0.7381, 0.1429\tANYCELLMATCH 0.0000, 0.8235\tCELLMATCH 0.0000, 0.8235\tCELLTOKENMATCH 0.0000, 0.0000\tNUMBER 0.0000, 1.0000\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "spider_val_car_1\tCPM 0.1429\tCEM 0.5873\tANYCELLMATCH 0.0635\tCELLMATCH 0.0000\tCELLTOKENMATCH 0.0635\tNUMBER 0.0794\tTIME 0.0000\tCPM 0.0000\tCEM 0.5312\tANYCELLMATCH 0.2656\tCELLMATCH 0.2656\tCELLTOKENMATCH 0.0000\tNUMBER 0.5312\tTIME 0.0000\tall 0.1352, 0.0858\tCPM 0.1216, 0.0000\tCEM 0.2721, 0.1250\tANYCELLMATCH 0.0667, 0.1417\tCELLMATCH 0.0000, 0.3036\tCELLTOKENMATCH 0.1250, 0.0000\tNUMBER 0.0385, 0.1308\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "spider_val_flight_2\tCPM 0.4318\tCEM 0.4091\tANYCELLMATCH 0.2727\tCELLMATCH 0.0455\tCELLTOKENMATCH 0.2727\tNUMBER 0.0000\tTIME 0.0000\tCPM 0.4412\tCEM 0.2059\tANYCELLMATCH 0.9118\tCELLMATCH 0.9118\tCELLTOKENMATCH 0.1765\tNUMBER 0.0000\tTIME 0.0000\tall 0.1600, 0.1707\tCPM 0.1445, 0.1141\tCEM 0.3913, 0.1522\tANYCELLMATCH 0.1569, 0.4052\tCELLMATCH 0.0357, 0.5536\tCELLTOKENMATCH 0.4211, 0.2105\tNUMBER 0.0000, 0.0000\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "spider_val_employee_hire_evaluation\tCPM 0.0385\tCEM 0.7308\tANYCELLMATCH 0.0000\tCELLMATCH 0.0000\tCELLTOKENMATCH 0.0000\tNUMBER 0.0000\tTIME 0.0000\tCPM 0.7000\tCEM 0.4000\tANYCELLMATCH 0.0000\tCELLMATCH 0.0000\tCELLTOKENMATCH 0.0000\tNUMBER 0.4000\tTIME 0.0000\tall 0.2010, 0.0503\tCPM 0.0155, 0.0543\tCEM 0.5507, 0.0580\tANYCELLMATCH 0.0000, 0.0000\tCELLMATCH 0.0000, 0.0000\tCELLTOKENMATCH 0.0000, 0.0000\tNUMBER 0.0000, 0.6667\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "spider_val_cre_Doc_Template_Mgt\tCPM 0.6230\tCEM 0.4508\tANYCELLMATCH 0.0164\tCELLMATCH 0.0000\tCELLTOKENMATCH 0.0164\tNUMBER 0.0164\tTIME 0.0000\tCPM 0.7647\tCEM 0.3824\tANYCELLMATCH 0.8824\tCELLMATCH 0.8824\tCELLTOKENMATCH 0.0588\tNUMBER 0.0588\tTIME 0.0000\tall 0.1125, 0.0360\tCPM 0.1153, 0.0395\tCEM 0.5046, 0.1193\tANYCELLMATCH 0.0323, 0.4839\tCELLMATCH 0.0000, 0.7143\tCELLTOKENMATCH 0.0909, 0.0909\tNUMBER 0.0870, 0.0870\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "spider_val_course_teach\tCPM 0.0000\tCEM 0.8000\tANYCELLMATCH 0.0000\tCELLMATCH 0.0000\tCELLTOKENMATCH 0.0000\tNUMBER 0.0000\tTIME 0.0000\tCPM 0.2000\tCEM 0.4000\tANYCELLMATCH 0.6000\tCELLMATCH 0.6000\tCELLTOKENMATCH 0.0000\tNUMBER 0.0000\tTIME 0.0000\tall 0.2500, 0.0625\tCPM 0.0000, 0.0244\tCEM 0.7273, 0.0909\tANYCELLMATCH 0.0000, 1.0000\tCELLMATCH 0.0000, 1.0000\tCELLTOKENMATCH 0.0000, 0.0000\tNUMBER 0.0000, 0.0000\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "spider_val_museum_visit\tCPM 0.3571\tCEM 0.5000\tANYCELLMATCH 0.1071\tCELLMATCH 0.0000\tCELLTOKENMATCH 0.1071\tNUMBER 0.0357\tTIME 0.0000\tCPM 0.7333\tCEM 0.1333\tANYCELLMATCH 0.4000\tCELLMATCH 0.4000\tCELLTOKENMATCH 0.0667\tNUMBER 0.3333\tTIME 0.0000\tall 0.2500, 0.1354\tCPM 0.2000, 0.2200\tCEM 0.5000, 0.0714\tANYCELLMATCH 0.2500, 0.5000\tCELLMATCH 0.0000, 1.0000\tCELLTOKENMATCH 0.4286, 0.1429\tNUMBER 0.0455, 0.2273\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "spider_val_wta_1\tCPM 0.4286\tCEM 0.4388\tANYCELLMATCH 0.0510\tCELLMATCH 0.0510\tCELLTOKENMATCH 0.0102\tNUMBER 0.0102\tTIME 0.0612\tCPM 0.3889\tCEM 0.1111\tANYCELLMATCH 0.5000\tCELLMATCH 0.5000\tCELLTOKENMATCH 0.0000\tNUMBER 0.2222\tTIME 0.0000\tall 0.0996, 0.0176\tCPM 0.0736, 0.0123\tCEM 0.5658, 0.0263\tANYCELLMATCH 0.1471, 0.2647\tCELLMATCH 0.2000, 0.3600\tCELLTOKENMATCH 0.1000, 0.0000\tNUMBER 0.0182, 0.0727\tTIME 0.0323, 0.0000\t\n",
      "\n",
      "spider_val_battle_death\tCPM 0.0000\tCEM 0.7407\tANYCELLMATCH 0.2222\tCELLMATCH 0.0000\tCELLTOKENMATCH 0.2222\tNUMBER 0.0000\tTIME 0.0000\tCPM 0.4000\tCEM 0.7000\tANYCELLMATCH 0.8000\tCELLMATCH 0.7000\tCELLTOKENMATCH 0.1000\tNUMBER 0.0000\tTIME 0.0000\tall 0.2564, 0.1154\tCPM 0.0000, 0.1111\tCEM 0.4762, 0.1667\tANYCELLMATCH 0.4000, 0.5333\tCELLMATCH 0.0000, 0.8750\tCELLTOKENMATCH 0.8571, 0.1429\tNUMBER 0.0000, 0.0000\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "spider_val_student_transcripts_tracking\tCPM 0.7923\tCEM 0.3000\tANYCELLMATCH 0.0000\tCELLMATCH 0.0000\tCELLTOKENMATCH 0.0000\tNUMBER 0.0538\tTIME 0.0769\tCPM 0.6923\tCEM 0.1923\tANYCELLMATCH 0.5769\tCELLMATCH 0.5000\tCELLTOKENMATCH 0.0769\tNUMBER 0.0000\tTIME 0.0000\tall 0.0792, 0.0162\tCPM 0.0829, 0.0145\tCEM 0.7358, 0.0943\tANYCELLMATCH 0.0000, 0.6522\tCELLMATCH 0.0000, 0.7647\tCELLTOKENMATCH 0.0000, 0.3333\tNUMBER 0.0437, 0.0000\tTIME 0.0427, 0.0000\t\n",
      "\n",
      "spider_val_tvshow\tCPM 0.1047\tCEM 0.6395\tANYCELLMATCH 0.1512\tCELLMATCH 0.0233\tCELLTOKENMATCH 0.1279\tNUMBER 0.0000\tTIME 0.0000\tCPM 0.1579\tCEM 0.6579\tANYCELLMATCH 0.8421\tCELLMATCH 0.8421\tCELLTOKENMATCH 0.2368\tNUMBER 0.0000\tTIME 0.0000\tall 0.2351, 0.1269\tCPM 0.1184, 0.0789\tCEM 0.3333, 0.1515\tANYCELLMATCH 0.1688, 0.4156\tCELLMATCH 0.0526, 0.8421\tCELLTOKENMATCH 0.2292, 0.1875\tNUMBER 0.0000, 0.0000\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "spider_val_poker_player\tCPM 0.0000\tCEM 0.9130\tANYCELLMATCH 0.0000\tCELLMATCH 0.0000\tCELLTOKENMATCH 0.0000\tNUMBER 0.0000\tTIME 0.0000\tCPM 0.2000\tCEM 0.6000\tANYCELLMATCH 0.2000\tCELLMATCH 0.2000\tCELLTOKENMATCH 0.0000\tNUMBER 0.6000\tTIME 0.0000\tall 0.3784, 0.0901\tCPM 0.0000, 0.0377\tCEM 0.7500, 0.1071\tANYCELLMATCH 0.0000, 1.0000\tCELLMATCH 0.0000, 1.0000\tCELLTOKENMATCH 0.0000, 0.0000\tNUMBER 0.0000, 1.0000\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "spider_val_voter_1\tCPM 0.3750\tCEM 0.6667\tANYCELLMATCH 0.0417\tCELLMATCH 0.0417\tCELLTOKENMATCH 0.0000\tNUMBER 0.0000\tTIME 0.1250\tCPM 0.5000\tCEM 0.3750\tANYCELLMATCH 0.6250\tCELLMATCH 0.6250\tCELLTOKENMATCH 0.0000\tNUMBER 0.0000\tTIME 0.0000\tall 0.2667, 0.0933\tCPM 0.2195, 0.0976\tCEM 0.5517, 0.1034\tANYCELLMATCH 0.1250, 0.6250\tCELLMATCH 0.1250, 0.6250\tCELLTOKENMATCH 0.0000, 0.0000\tNUMBER 0.0000, 0.0000\tTIME 0.2000, 0.0000\t\n",
      "\n",
      "spider_val_world_1\tCPM 0.0427\tCEM 0.7012\tANYCELLMATCH 0.1585\tCELLMATCH 0.0000\tCELLTOKENMATCH 0.1585\tNUMBER 0.0732\tTIME 0.0000\tCPM 0.1691\tCEM 0.2868\tANYCELLMATCH 0.5588\tCELLMATCH 0.5515\tCELLTOKENMATCH 0.0662\tNUMBER 0.0735\tTIME 0.0000\tall 0.1731, 0.1518\tCPM 0.0526, 0.1729\tCEM 0.4167, 0.1413\tANYCELLMATCH 0.0935, 0.2734\tCELLMATCH 0.0000, 0.7812\tCELLTOKENMATCH 0.1361, 0.0471\tNUMBER 0.0759, 0.0633\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "spider_val_orchestra\tCPM 0.0652\tCEM 0.7609\tANYCELLMATCH 0.0000\tCELLMATCH 0.0000\tCELLTOKENMATCH 0.0000\tNUMBER 0.0000\tTIME 0.0000\tCPM 0.5714\tCEM 0.2857\tANYCELLMATCH 0.4286\tCELLMATCH 0.4286\tCELLTOKENMATCH 0.0000\tNUMBER 0.2857\tTIME 0.0000\tall 0.1681, 0.0531\tCPM 0.0195, 0.0519\tCEM 0.5303, 0.0606\tANYCELLMATCH 0.0000, 0.6667\tCELLMATCH 0.0000, 1.0000\tCELLTOKENMATCH 0.0000, 0.0000\tNUMBER 0.0000, 0.5000\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "spider_val_network_1\tCPM 0.0441\tCEM 0.6912\tANYCELLMATCH 0.0294\tCELLMATCH 0.0294\tCELLTOKENMATCH 0.0000\tNUMBER 0.0000\tTIME 0.0000\tCPM 0.0000\tCEM 0.5000\tANYCELLMATCH 0.4545\tCELLMATCH 0.4545\tCELLTOKENMATCH 0.0000\tNUMBER 0.1818\tTIME 0.0000\tall 0.3205, 0.1026\tCPM 0.0345, 0.0000\tCEM 0.7344, 0.1719\tANYCELLMATCH 0.2000, 1.0000\tCELLMATCH 0.2000, 1.0000\tCELLTOKENMATCH 0.0000, 0.0000\tNUMBER 0.0000, 1.0000\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "spider_val_dog_kennels\tCPM 0.6486\tCEM 0.5338\tANYCELLMATCH 0.0541\tCELLMATCH 0.0135\tCELLTOKENMATCH 0.0405\tNUMBER 0.0068\tTIME 0.0811\tCPM 0.5000\tCEM 0.3125\tANYCELLMATCH 0.3125\tCELLMATCH 0.2500\tCELLTOKENMATCH 0.0625\tNUMBER 0.0000\tTIME 0.0000\tall 0.1100, 0.0220\tCPM 0.1203, 0.0201\tCEM 0.4593, 0.0581\tANYCELLMATCH 0.1951, 0.2439\tCELLMATCH 0.1176, 0.4706\tCELLTOKENMATCH 0.2500, 0.0833\tNUMBER 0.0476, 0.0000\tTIME 0.0293, 0.0000\t\n",
      "\n",
      "spider_val_singer\tCPM 0.0500\tCEM 0.8250\tANYCELLMATCH 0.0000\tCELLMATCH 0.0000\tCELLTOKENMATCH 0.0000\tNUMBER 0.0000\tTIME 0.0000\tCPM 0.0833\tCEM 0.5833\tANYCELLMATCH 0.0833\tCELLMATCH 0.0833\tCELLTOKENMATCH 0.0000\tNUMBER 0.5000\tTIME 0.0000\tall 0.3153, 0.0721\tCPM 0.0282, 0.0141\tCEM 0.8250, 0.1750\tANYCELLMATCH 0.0000, 1.0000\tCELLMATCH 0.0000, 1.0000\tCELLTOKENMATCH 0.0000, 0.0000\tNUMBER 0.0000, 1.0000\tTIME 0.0000, 0.0000\t\n",
      "\n",
      "spider_val_real_estate_properties\tCPM 0.7500\tCEM 0.5000\tANYCELLMATCH 0.0000\tCELLMATCH 0.0000\tCELLTOKENMATCH 0.0000\tNUMBER 0.0000\tTIME 0.0000\tCPM 1.0000\tCEM 0.0000\tANYCELLMATCH 0.5000\tCELLMATCH 0.5000\tCELLTOKENMATCH 0.0000\tNUMBER 0.0000\tTIME 0.0000\tall 0.0333, 0.0222\tCPM 0.0375, 0.0250\tCEM 1.0000, 0.0000\tANYCELLMATCH 0.0000, 1.0000\tCELLMATCH 0.0000, 1.0000\tCELLTOKENMATCH 0.0000, 0.0000\tNUMBER 0.0000, 0.0000\tTIME 0.0000, 0.0000\t\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spider_train\tCPM 0.2621\tCEM 0.3190\tANYCELLMATCH 0.0357\tCELLMATCH 0.0120\tCELLTOKENMATCH 0.0249\tNUMBER 0.0170\tTIME 0.0209\tCPM 0.2516\tCEM 0.2233\tANYCELLMATCH 0.2728\tCELLMATCH 0.2518\tCELLTOKENMATCH 0.0306\tNUMBER 0.1261\tTIME 0.0151\tall 0.0714, 0.0335\tCPM 0.0535, 0.0224\tCEM 0.2480, 0.0756\tANYCELLMATCH 0.0594, 0.1977\tCELLMATCH 0.0347, 0.3171\tCELLTOKENMATCH 0.0911, 0.0488\tNUMBER 0.0248, 0.0799\tTIME 0.0161, 0.0051\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "link_types = ['CPM','CEM','ANYCELLMATCH','CELLMATCH','CELLTOKENMATCH','NUMBER','TIME']\n",
    "link_analysis_processed = {}\n",
    "for dataset_id in link_analysis:\n",
    "    link_analysis_processed[dataset_id] = {}\n",
    "    where_num = {'all':0, 'any':0}\n",
    "    select_num = {'all':0, 'any':0}\n",
    "    link_num = {'all':[0,0,0]}\n",
    "    for all_links, select_cols, where_cols in link_analysis[dataset_id]:\n",
    "        where_num['all'] += len(where_cols)\n",
    "        select_num['all'] += len(select_cols)\n",
    "        for col, links in all_links.items():\n",
    "            links = set(links)\n",
    "            if 'CELLMATCH' in links or 'CELLTOKENMATCH' in links:\n",
    "                links.add('ANYCELLMATCH')\n",
    "            if links:\n",
    "                link_num['all'][0] += 1\n",
    "                if col in select_cols:\n",
    "                    link_num['all'][1] += 1\n",
    "                    select_num['any'] += 1\n",
    "                if col in where_cols:\n",
    "                    link_num['all'][2] += 1\n",
    "                    where_num['any'] += 1\n",
    "            for link in links:\n",
    "                if link not in link_num:\n",
    "                    link_num[link] = [0,0,0]\n",
    "                link_num[link][0] += 1\n",
    "                if col in select_cols:\n",
    "                    link_num[link][1] += 1\n",
    "                    if link not in select_num:\n",
    "                        select_num[link] = 0\n",
    "                    select_num[link] += 1\n",
    "                if col in where_cols:\n",
    "                    link_num[link][2] += 1\n",
    "                    if link not in where_num:\n",
    "                        where_num[link] = 0\n",
    "                    where_num[link] += 1\n",
    "    print(dataset_id, end='\\t')\n",
    "    for k in link_types:\n",
    "        print(k,'%.4f'%(select_num.get(k,0)/select_num['all']),end='\\t')\n",
    "        link_analysis_processed[dataset_id]['select_%s'%k] = select_num.get(k,0)/select_num['all']\n",
    "    for k in link_types:\n",
    "        print(k,'%.4f'%(where_num.get(k,0)/where_num['all']),end='\\t')\n",
    "        link_analysis_processed[dataset_id]['where_%s'%k] = where_num.get(k,0)/where_num['all']\n",
    "    for k in ['all']+link_types:\n",
    "        v = link_num.get(k,[1,0,0])\n",
    "        print(k,'%.4f, %.4f'%(v[1]/v[0],v[2]/v[0]), end='\\t')\n",
    "        link_analysis_processed[dataset_id]['linked_%s'%k] = (v[1]/v[0],v[2]/v[0])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/t-xiaden/workspace/featurestorage/data/spider-20190205/dev.json','r') as f:\n",
    "    origs = json.load(f)\n",
    "with open('/home/t-xiaden/workspace/NL2CodeOverData/logdirs/bert_value_run_0_true_1_new_nocolvalue-step40000.eval', 'r') as f:\n",
    "    results = json.load(f)\n",
    "db_result = {}\n",
    "for i, per_item in enumerate(results['per_item']):\n",
    "    db_id = origs[i]['db_id']\n",
    "    if db_id not in db_result:\n",
    "        db_result[db_id] = {'exec':[],'exact':[],'select':[],'where':[],'where(with value)':[]}\n",
    "    db_result[db_id]['exec'].append(per_item['exec'])\n",
    "    db_result[db_id]['exact'].append(per_item['exact'])\n",
    "    for k in ['select','where','where(with value)']:\n",
    "        db_result[db_id][k].append(per_item['partial'][k]['acc'])\n",
    "for db_id in db_result:\n",
    "    db_result[db_id]['num'] = len(db_result[db_id]['exec'])\n",
    "    db_result[db_id]['exec'] = sum(db_result[db_id]['exec'])/len(db_result[db_id]['exec'])\n",
    "    db_result[db_id]['exact'] = sum(db_result[db_id]['exact'])/len(db_result[db_id]['exact'])\n",
    "    for k in ['select','where','where(with value)']:\n",
    "        db_result[db_id][k] = sum(db_result[db_id][k])/len(db_result[db_id][k])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('real_estate_properties', {'exec': 0.25, 'exact': 0.25, 'select': 1.0, 'where': 0.75, 'where(with value)': 0.75, 'num': 4})\n",
      "{'select_CPM': 0.75, 'select_CEM': 0.5, 'select_ANYCELLMATCH': 0.0, 'select_CELLMATCH': 0.0, 'select_CELLTOKENMATCH': 0.0, 'select_NUMBER': 0.0, 'select_TIME': 0.0, 'where_CPM': 1.0, 'where_CEM': 0.0, 'where_ANYCELLMATCH': 0.5, 'where_CELLMATCH': 0.5, 'where_CELLTOKENMATCH': 0.0, 'where_NUMBER': 0.0, 'where_TIME': 0.0, 'linked_all': (0.03333333333333333, 0.022222222222222223), 'linked_CPM': (0.0375, 0.025), 'linked_CEM': (1.0, 0.0), 'linked_ANYCELLMATCH': (0.0, 1.0), 'linked_CELLMATCH': (0.0, 1.0), 'linked_CELLTOKENMATCH': (0.0, 0.0), 'linked_NUMBER': (0.0, 0.0), 'linked_TIME': (0.0, 0.0)}\n",
      "('world_1', {'exec': 0.4, 'exact': 0.3333333333333333, 'select': 0.75, 'where': 0.575, 'where(with value)': 0.5416666666666666, 'num': 120})\n",
      "{'select_CPM': 0.042682926829268296, 'select_CEM': 0.7012195121951219, 'select_ANYCELLMATCH': 0.15853658536585366, 'select_CELLMATCH': 0.0, 'select_CELLTOKENMATCH': 0.15853658536585366, 'select_NUMBER': 0.07317073170731707, 'select_TIME': 0.0, 'where_CPM': 0.16911764705882354, 'where_CEM': 0.2867647058823529, 'where_ANYCELLMATCH': 0.5588235294117647, 'where_CELLMATCH': 0.5514705882352942, 'where_CELLTOKENMATCH': 0.0661764705882353, 'where_NUMBER': 0.07352941176470588, 'where_TIME': 0.0, 'linked_all': (0.17310252996005326, 0.151797603195739), 'linked_CPM': (0.05263157894736842, 0.17293233082706766), 'linked_CEM': (0.4166666666666667, 0.14130434782608695), 'linked_ANYCELLMATCH': (0.09352517985611511, 0.2733812949640288), 'linked_CELLMATCH': (0.0, 0.78125), 'linked_CELLTOKENMATCH': (0.13612565445026178, 0.04712041884816754), 'linked_NUMBER': (0.0759493670886076, 0.06329113924050633), 'linked_TIME': (0.0, 0.0)}\n",
      "('museum_visit', {'exec': 0.5555555555555556, 'exact': 0.5, 'select': 0.8333333333333334, 'where': 0.8333333333333334, 'where(with value)': 0.8333333333333334, 'num': 18})\n",
      "{'select_CPM': 0.35714285714285715, 'select_CEM': 0.5, 'select_ANYCELLMATCH': 0.10714285714285714, 'select_CELLMATCH': 0.0, 'select_CELLTOKENMATCH': 0.10714285714285714, 'select_NUMBER': 0.03571428571428571, 'select_TIME': 0.0, 'where_CPM': 0.7333333333333333, 'where_CEM': 0.13333333333333333, 'where_ANYCELLMATCH': 0.4, 'where_CELLMATCH': 0.4, 'where_CELLTOKENMATCH': 0.06666666666666667, 'where_NUMBER': 0.3333333333333333, 'where_TIME': 0.0, 'linked_all': (0.25, 0.13541666666666666), 'linked_CPM': (0.2, 0.22), 'linked_CEM': (0.5, 0.07142857142857142), 'linked_ANYCELLMATCH': (0.25, 0.5), 'linked_CELLMATCH': (0.0, 1.0), 'linked_CELLTOKENMATCH': (0.42857142857142855, 0.14285714285714285), 'linked_NUMBER': (0.045454545454545456, 0.22727272727272727), 'linked_TIME': (0.0, 0.0)}\n",
      "('battle_death', {'exec': 0.5625, 'exact': 0.625, 'select': 0.75, 'where': 0.9375, 'where(with value)': 0.8125, 'num': 16})\n",
      "{'select_CPM': 0.0, 'select_CEM': 0.7407407407407407, 'select_ANYCELLMATCH': 0.2222222222222222, 'select_CELLMATCH': 0.0, 'select_CELLTOKENMATCH': 0.2222222222222222, 'select_NUMBER': 0.0, 'select_TIME': 0.0, 'where_CPM': 0.4, 'where_CEM': 0.7, 'where_ANYCELLMATCH': 0.8, 'where_CELLMATCH': 0.7, 'where_CELLTOKENMATCH': 0.1, 'where_NUMBER': 0.0, 'where_TIME': 0.0, 'linked_all': (0.2564102564102564, 0.11538461538461539), 'linked_CPM': (0.0, 0.1111111111111111), 'linked_CEM': (0.47619047619047616, 0.16666666666666666), 'linked_ANYCELLMATCH': (0.4, 0.5333333333333333), 'linked_CELLMATCH': (0.0, 0.875), 'linked_CELLTOKENMATCH': (0.8571428571428571, 0.14285714285714285), 'linked_NUMBER': (0.0, 0.0), 'linked_TIME': (0.0, 0.0)}\n",
      "('student_transcripts_tracking', {'exec': 0.6025641025641025, 'exact': 0.5769230769230769, 'select': 0.8076923076923077, 'where': 0.8717948717948718, 'where(with value)': 0.8333333333333334, 'num': 78})\n",
      "{'select_CPM': 0.7923076923076923, 'select_CEM': 0.3, 'select_ANYCELLMATCH': 0.0, 'select_CELLMATCH': 0.0, 'select_CELLTOKENMATCH': 0.0, 'select_NUMBER': 0.05384615384615385, 'select_TIME': 0.07692307692307693, 'where_CPM': 0.6923076923076923, 'where_CEM': 0.19230769230769232, 'where_ANYCELLMATCH': 0.5769230769230769, 'where_CELLMATCH': 0.5, 'where_CELLTOKENMATCH': 0.07692307692307693, 'where_NUMBER': 0.0, 'where_TIME': 0.0, 'linked_all': (0.07916102841677942, 0.016238159675236806), 'linked_CPM': (0.08286403861625101, 0.014481094127111826), 'linked_CEM': (0.7358490566037735, 0.09433962264150944), 'linked_ANYCELLMATCH': (0.0, 0.6521739130434783), 'linked_CELLMATCH': (0.0, 0.7647058823529411), 'linked_CELLTOKENMATCH': (0.0, 0.3333333333333333), 'linked_NUMBER': (0.04375, 0.0), 'linked_TIME': (0.042735042735042736, 0.0)}\n",
      "('car_1', {'exec': 0.6195652173913043, 'exact': 0.532608695652174, 'select': 0.75, 'where': 0.8043478260869565, 'where(with value)': 0.7608695652173914, 'num': 92})\n",
      "{'select_CPM': 0.14285714285714285, 'select_CEM': 0.5873015873015873, 'select_ANYCELLMATCH': 0.06349206349206349, 'select_CELLMATCH': 0.0, 'select_CELLTOKENMATCH': 0.06349206349206349, 'select_NUMBER': 0.07936507936507936, 'select_TIME': 0.0, 'where_CPM': 0.0, 'where_CEM': 0.53125, 'where_ANYCELLMATCH': 0.265625, 'where_CELLMATCH': 0.265625, 'where_CELLTOKENMATCH': 0.0, 'where_NUMBER': 0.53125, 'where_TIME': 0.0, 'linked_all': (0.13517441860465115, 0.08575581395348837), 'linked_CPM': (0.12162162162162163, 0.0), 'linked_CEM': (0.27205882352941174, 0.125), 'linked_ANYCELLMATCH': (0.06666666666666667, 0.14166666666666666), 'linked_CELLMATCH': (0.0, 0.30357142857142855), 'linked_CELLTOKENMATCH': (0.125, 0.0), 'linked_NUMBER': (0.038461538461538464, 0.13076923076923078), 'linked_TIME': (0.0, 0.0)}\n",
      "('network_1', {'exec': 0.6428571428571429, 'exact': 0.6607142857142857, 'select': 0.9107142857142857, 'where': 0.8571428571428571, 'where(with value)': 0.8571428571428571, 'num': 56})\n",
      "{'select_CPM': 0.04411764705882353, 'select_CEM': 0.6911764705882353, 'select_ANYCELLMATCH': 0.029411764705882353, 'select_CELLMATCH': 0.029411764705882353, 'select_CELLTOKENMATCH': 0.0, 'select_NUMBER': 0.0, 'select_TIME': 0.0, 'where_CPM': 0.0, 'where_CEM': 0.5, 'where_ANYCELLMATCH': 0.45454545454545453, 'where_CELLMATCH': 0.45454545454545453, 'where_CELLTOKENMATCH': 0.0, 'where_NUMBER': 0.18181818181818182, 'where_TIME': 0.0, 'linked_all': (0.32051282051282054, 0.10256410256410256), 'linked_CPM': (0.034482758620689655, 0.0), 'linked_CEM': (0.734375, 0.171875), 'linked_ANYCELLMATCH': (0.2, 1.0), 'linked_CELLMATCH': (0.2, 1.0), 'linked_CELLTOKENMATCH': (0.0, 0.0), 'linked_NUMBER': (0.0, 1.0), 'linked_TIME': (0.0, 0.0)}\n",
      "('dog_kennels', {'exec': 0.6463414634146342, 'exact': 0.6097560975609756, 'select': 0.8170731707317073, 'where': 0.8902439024390244, 'where(with value)': 0.8292682926829268, 'num': 82})\n",
      "{'select_CPM': 0.6486486486486487, 'select_CEM': 0.5337837837837838, 'select_ANYCELLMATCH': 0.05405405405405406, 'select_CELLMATCH': 0.013513513513513514, 'select_CELLTOKENMATCH': 0.04054054054054054, 'select_NUMBER': 0.006756756756756757, 'select_TIME': 0.08108108108108109, 'where_CPM': 0.5, 'where_CEM': 0.3125, 'where_ANYCELLMATCH': 0.3125, 'where_CELLMATCH': 0.25, 'where_CELLTOKENMATCH': 0.0625, 'where_NUMBER': 0.0, 'where_TIME': 0.0, 'linked_all': (0.10997643362136685, 0.02199528672427337), 'linked_CPM': (0.12030075187969924, 0.020050125313283207), 'linked_CEM': (0.45930232558139533, 0.05813953488372093), 'linked_ANYCELLMATCH': (0.1951219512195122, 0.24390243902439024), 'linked_CELLMATCH': (0.11764705882352941, 0.47058823529411764), 'linked_CELLTOKENMATCH': (0.25, 0.08333333333333333), 'linked_NUMBER': (0.047619047619047616, 0.0), 'linked_TIME': (0.02926829268292683, 0.0)}\n",
      "('wta_1', {'exec': 0.6612903225806451, 'exact': 0.6774193548387096, 'select': 0.8387096774193549, 'where': 0.9516129032258065, 'where(with value)': 0.9516129032258065, 'num': 62})\n",
      "{'select_CPM': 0.42857142857142855, 'select_CEM': 0.4387755102040816, 'select_ANYCELLMATCH': 0.05102040816326531, 'select_CELLMATCH': 0.05102040816326531, 'select_CELLTOKENMATCH': 0.01020408163265306, 'select_NUMBER': 0.01020408163265306, 'select_TIME': 0.061224489795918366, 'where_CPM': 0.3888888888888889, 'where_CEM': 0.1111111111111111, 'where_ANYCELLMATCH': 0.5, 'where_CELLMATCH': 0.5, 'where_CELLTOKENMATCH': 0.0, 'where_NUMBER': 0.2222222222222222, 'where_TIME': 0.0, 'linked_all': (0.09964830011723329, 0.017584994138335287), 'linked_CPM': (0.07355516637478109, 0.012259194395796848), 'linked_CEM': (0.5657894736842105, 0.02631578947368421), 'linked_ANYCELLMATCH': (0.14705882352941177, 0.2647058823529412), 'linked_CELLMATCH': (0.2, 0.36), 'linked_CELLTOKENMATCH': (0.1, 0.0), 'linked_NUMBER': (0.01818181818181818, 0.07272727272727272), 'linked_TIME': (0.03225806451612903, 0.0)}\n",
      "('pets_1', {'exec': 0.6666666666666666, 'exact': 0.35714285714285715, 'select': 0.8333333333333334, 'where': 0.7619047619047619, 'where(with value)': 0.7142857142857143, 'num': 42})\n",
      "{'select_CPM': 0.296875, 'select_CEM': 0.484375, 'select_ANYCELLMATCH': 0.0, 'select_CELLMATCH': 0.0, 'select_CELLTOKENMATCH': 0.0, 'select_NUMBER': 0.0, 'select_TIME': 0.0, 'where_CPM': 0.5294117647058824, 'where_CEM': 0.17647058823529413, 'where_ANYCELLMATCH': 0.4117647058823529, 'where_CELLMATCH': 0.4117647058823529, 'where_CELLTOKENMATCH': 0.0, 'where_NUMBER': 0.23529411764705882, 'where_TIME': 0.0, 'linked_all': (0.18181818181818182, 0.10984848484848485), 'linked_CPM': (0.0871559633027523, 0.08256880733944955), 'linked_CEM': (0.7380952380952381, 0.14285714285714285), 'linked_ANYCELLMATCH': (0.0, 0.8235294117647058), 'linked_CELLMATCH': (0.0, 0.8235294117647058), 'linked_CELLTOKENMATCH': (0.0, 0.0), 'linked_NUMBER': (0.0, 1.0), 'linked_TIME': (0.0, 0.0)}\n",
      "('flight_2', {'exec': 0.75, 'exact': 0.7625, 'select': 0.95, 'where': 0.95, 'where(with value)': 0.8125, 'num': 80})\n",
      "{'select_CPM': 0.4318181818181818, 'select_CEM': 0.4090909090909091, 'select_ANYCELLMATCH': 0.2727272727272727, 'select_CELLMATCH': 0.045454545454545456, 'select_CELLTOKENMATCH': 0.2727272727272727, 'select_NUMBER': 0.0, 'select_TIME': 0.0, 'where_CPM': 0.4411764705882353, 'where_CEM': 0.20588235294117646, 'where_ANYCELLMATCH': 0.9117647058823529, 'where_CELLMATCH': 0.9117647058823529, 'where_CELLTOKENMATCH': 0.17647058823529413, 'where_NUMBER': 0.0, 'where_TIME': 0.0, 'linked_all': (0.16, 0.17066666666666666), 'linked_CPM': (0.1444866920152091, 0.11406844106463879), 'linked_CEM': (0.391304347826087, 0.15217391304347827), 'linked_ANYCELLMATCH': (0.1568627450980392, 0.40522875816993464), 'linked_CELLMATCH': (0.03571428571428571, 0.5535714285714286), 'linked_CELLTOKENMATCH': (0.42105263157894735, 0.21052631578947367), 'linked_NUMBER': (0.0, 0.0), 'linked_TIME': (0.0, 0.0)}\n",
      "('tvshow', {'exec': 0.7741935483870968, 'exact': 0.8225806451612904, 'select': 0.9516129032258065, 'where': 0.9516129032258065, 'where(with value)': 0.8387096774193549, 'num': 62})\n",
      "{'select_CPM': 0.10465116279069768, 'select_CEM': 0.6395348837209303, 'select_ANYCELLMATCH': 0.1511627906976744, 'select_CELLMATCH': 0.023255813953488372, 'select_CELLTOKENMATCH': 0.12790697674418605, 'select_NUMBER': 0.0, 'select_TIME': 0.0, 'where_CPM': 0.15789473684210525, 'where_CEM': 0.6578947368421053, 'where_ANYCELLMATCH': 0.8421052631578947, 'where_CELLMATCH': 0.8421052631578947, 'where_CELLTOKENMATCH': 0.23684210526315788, 'where_NUMBER': 0.0, 'where_TIME': 0.0, 'linked_all': (0.23507462686567165, 0.12686567164179105), 'linked_CPM': (0.11842105263157894, 0.07894736842105263), 'linked_CEM': (0.3333333333333333, 0.15151515151515152), 'linked_ANYCELLMATCH': (0.16883116883116883, 0.4155844155844156), 'linked_CELLMATCH': (0.05263157894736842, 0.8421052631578947), 'linked_CELLTOKENMATCH': (0.22916666666666666, 0.1875), 'linked_NUMBER': (0.0, 0.0), 'linked_TIME': (0.0, 0.0)}\n",
      "('voter_1', {'exec': 0.8, 'exact': 0.9333333333333333, 'select': 0.9333333333333333, 'where': 1.0, 'where(with value)': 0.8, 'num': 15})\n",
      "{'select_CPM': 0.375, 'select_CEM': 0.6666666666666666, 'select_ANYCELLMATCH': 0.041666666666666664, 'select_CELLMATCH': 0.041666666666666664, 'select_CELLTOKENMATCH': 0.0, 'select_NUMBER': 0.0, 'select_TIME': 0.125, 'where_CPM': 0.5, 'where_CEM': 0.375, 'where_ANYCELLMATCH': 0.625, 'where_CELLMATCH': 0.625, 'where_CELLTOKENMATCH': 0.0, 'where_NUMBER': 0.0, 'where_TIME': 0.0, 'linked_all': (0.26666666666666666, 0.09333333333333334), 'linked_CPM': (0.21951219512195122, 0.0975609756097561), 'linked_CEM': (0.5517241379310345, 0.10344827586206896), 'linked_ANYCELLMATCH': (0.125, 0.625), 'linked_CELLMATCH': (0.125, 0.625), 'linked_CELLTOKENMATCH': (0.0, 0.0), 'linked_NUMBER': (0.0, 0.0), 'linked_TIME': (0.2, 0.0)}\n",
      "('cre_Doc_Template_Mgt', {'exec': 0.8571428571428571, 'exact': 0.8333333333333334, 'select': 0.9761904761904762, 'where': 0.9761904761904762, 'where(with value)': 0.9285714285714286, 'num': 84})\n",
      "{'select_CPM': 0.6229508196721312, 'select_CEM': 0.45081967213114754, 'select_ANYCELLMATCH': 0.01639344262295082, 'select_CELLMATCH': 0.0, 'select_CELLTOKENMATCH': 0.01639344262295082, 'select_NUMBER': 0.01639344262295082, 'select_TIME': 0.0, 'where_CPM': 0.7647058823529411, 'where_CEM': 0.38235294117647056, 'where_ANYCELLMATCH': 0.8823529411764706, 'where_CELLMATCH': 0.8823529411764706, 'where_CELLTOKENMATCH': 0.058823529411764705, 'where_NUMBER': 0.058823529411764705, 'where_TIME': 0.0, 'linked_all': (0.1124859392575928, 0.0359955005624297), 'linked_CPM': (0.11532625189681335, 0.03945371775417299), 'linked_CEM': (0.5045871559633027, 0.11926605504587157), 'linked_ANYCELLMATCH': (0.03225806451612903, 0.4838709677419355), 'linked_CELLMATCH': (0.0, 0.7142857142857143), 'linked_CELLTOKENMATCH': (0.09090909090909091, 0.09090909090909091), 'linked_NUMBER': (0.08695652173913043, 0.08695652173913043), 'linked_TIME': (0.0, 0.0)}\n",
      "('singer', {'exec': 0.8666666666666667, 'exact': 0.8666666666666667, 'select': 0.9666666666666667, 'where': 1.0, 'where(with value)': 0.9666666666666667, 'num': 30})\n",
      "{'select_CPM': 0.05, 'select_CEM': 0.825, 'select_ANYCELLMATCH': 0.0, 'select_CELLMATCH': 0.0, 'select_CELLTOKENMATCH': 0.0, 'select_NUMBER': 0.0, 'select_TIME': 0.0, 'where_CPM': 0.08333333333333333, 'where_CEM': 0.5833333333333334, 'where_ANYCELLMATCH': 0.08333333333333333, 'where_CELLMATCH': 0.08333333333333333, 'where_CELLTOKENMATCH': 0.0, 'where_NUMBER': 0.5, 'where_TIME': 0.0, 'linked_all': (0.3153153153153153, 0.07207207207207207), 'linked_CPM': (0.028169014084507043, 0.014084507042253521), 'linked_CEM': (0.825, 0.175), 'linked_ANYCELLMATCH': (0.0, 1.0), 'linked_CELLMATCH': (0.0, 1.0), 'linked_CELLTOKENMATCH': (0.0, 0.0), 'linked_NUMBER': (0.0, 1.0), 'linked_TIME': (0.0, 0.0)}\n",
      "('concert_singer', {'exec': 0.9111111111111111, 'exact': 0.8222222222222222, 'select': 0.9555555555555556, 'where': 0.9333333333333333, 'where(with value)': 0.8444444444444444, 'num': 45})\n",
      "{'select_CPM': 0.07792207792207792, 'select_CEM': 0.6363636363636364, 'select_ANYCELLMATCH': 0.09090909090909091, 'select_CELLMATCH': 0.0, 'select_CELLTOKENMATCH': 0.09090909090909091, 'select_NUMBER': 0.025974025974025976, 'select_TIME': 0.0, 'where_CPM': 0.17391304347826086, 'where_CEM': 0.43478260869565216, 'where_ANYCELLMATCH': 0.43478260869565216, 'where_CELLMATCH': 0.34782608695652173, 'where_CELLTOKENMATCH': 0.08695652173913043, 'where_NUMBER': 0.13043478260869565, 'where_TIME': 0.0, 'linked_all': (0.16326530612244897, 0.052478134110787174), 'linked_CPM': (0.030927835051546393, 0.020618556701030927), 'linked_CEM': (0.4537037037037037, 0.09259259259259259), 'linked_ANYCELLMATCH': (0.22580645161290322, 0.3225806451612903), 'linked_CELLMATCH': (0.0, 0.4), 'linked_CELLTOKENMATCH': (0.6363636363636364, 0.18181818181818182), 'linked_NUMBER': (0.06060606060606061, 0.09090909090909091), 'linked_TIME': (0.0, 0.0)}\n",
      "('employee_hire_evaluation', {'exec': 0.9473684210526315, 'exact': 0.8947368421052632, 'select': 1.0, 'where': 1.0, 'where(with value)': 1.0, 'num': 38})\n",
      "{'select_CPM': 0.038461538461538464, 'select_CEM': 0.7307692307692307, 'select_ANYCELLMATCH': 0.0, 'select_CELLMATCH': 0.0, 'select_CELLTOKENMATCH': 0.0, 'select_NUMBER': 0.0, 'select_TIME': 0.0, 'where_CPM': 0.7, 'where_CEM': 0.4, 'where_ANYCELLMATCH': 0.0, 'where_CELLMATCH': 0.0, 'where_CELLTOKENMATCH': 0.0, 'where_NUMBER': 0.4, 'where_TIME': 0.0, 'linked_all': (0.20100502512562815, 0.05025125628140704), 'linked_CPM': (0.015503875968992248, 0.05426356589147287), 'linked_CEM': (0.5507246376811594, 0.057971014492753624), 'linked_ANYCELLMATCH': (0.0, 0.0), 'linked_CELLMATCH': (0.0, 0.0), 'linked_CELLTOKENMATCH': (0.0, 0.0), 'linked_NUMBER': (0.0, 0.6666666666666666), 'linked_TIME': (0.0, 0.0)}\n",
      "('course_teach', {'exec': 0.9666666666666667, 'exact': 0.9333333333333333, 'select': 0.9666666666666667, 'where': 1.0, 'where(with value)': 0.8666666666666667, 'num': 30})\n",
      "{'select_CPM': 0.0, 'select_CEM': 0.8, 'select_ANYCELLMATCH': 0.0, 'select_CELLMATCH': 0.0, 'select_CELLTOKENMATCH': 0.0, 'select_NUMBER': 0.0, 'select_TIME': 0.0, 'where_CPM': 0.2, 'where_CEM': 0.4, 'where_ANYCELLMATCH': 0.6, 'where_CELLMATCH': 0.6, 'where_CELLTOKENMATCH': 0.0, 'where_NUMBER': 0.0, 'where_TIME': 0.0, 'linked_all': (0.25, 0.0625), 'linked_CPM': (0.0, 0.024390243902439025), 'linked_CEM': (0.7272727272727273, 0.09090909090909091), 'linked_ANYCELLMATCH': (0.0, 1.0), 'linked_CELLMATCH': (0.0, 1.0), 'linked_CELLTOKENMATCH': (0.0, 0.0), 'linked_NUMBER': (0.0, 0.0), 'linked_TIME': (0.0, 0.0)}\n",
      "('orchestra', {'exec': 0.975, 'exact': 0.975, 'select': 1.0, 'where': 1.0, 'where(with value)': 1.0, 'num': 40})\n",
      "{'select_CPM': 0.06521739130434782, 'select_CEM': 0.7608695652173914, 'select_ANYCELLMATCH': 0.0, 'select_CELLMATCH': 0.0, 'select_CELLTOKENMATCH': 0.0, 'select_NUMBER': 0.0, 'select_TIME': 0.0, 'where_CPM': 0.5714285714285714, 'where_CEM': 0.2857142857142857, 'where_ANYCELLMATCH': 0.42857142857142855, 'where_CELLMATCH': 0.42857142857142855, 'where_CELLTOKENMATCH': 0.0, 'where_NUMBER': 0.2857142857142857, 'where_TIME': 0.0, 'linked_all': (0.168141592920354, 0.05309734513274336), 'linked_CPM': (0.01948051948051948, 0.05194805194805195), 'linked_CEM': (0.5303030303030303, 0.06060606060606061), 'linked_ANYCELLMATCH': (0.0, 0.6666666666666666), 'linked_CELLMATCH': (0.0, 1.0), 'linked_CELLTOKENMATCH': (0.0, 0.0), 'linked_NUMBER': (0.0, 0.5), 'linked_TIME': (0.0, 0.0)}\n",
      "('poker_player', {'exec': 1.0, 'exact': 1.0, 'select': 1.0, 'where': 1.0, 'where(with value)': 1.0, 'num': 40})\n",
      "{'select_CPM': 0.0, 'select_CEM': 0.9130434782608695, 'select_ANYCELLMATCH': 0.0, 'select_CELLMATCH': 0.0, 'select_CELLTOKENMATCH': 0.0, 'select_NUMBER': 0.0, 'select_TIME': 0.0, 'where_CPM': 0.2, 'where_CEM': 0.6, 'where_ANYCELLMATCH': 0.2, 'where_CELLMATCH': 0.2, 'where_CELLTOKENMATCH': 0.0, 'where_NUMBER': 0.6, 'where_TIME': 0.0, 'linked_all': (0.3783783783783784, 0.09009009009009009), 'linked_CPM': (0.0, 0.03773584905660377), 'linked_CEM': (0.75, 0.10714285714285714), 'linked_ANYCELLMATCH': (0.0, 1.0), 'linked_CELLMATCH': (0.0, 1.0), 'linked_CELLTOKENMATCH': (0.0, 0.0), 'linked_NUMBER': (0.0, 1.0), 'linked_TIME': (0.0, 0.0)}\n"
     ]
    }
   ],
   "source": [
    "for x in sorted(db_result.items(), key=lambda x:x[1]['exec']):\n",
    "    print(x)\n",
    "    print(link_analysis_processed['spider_val_'+x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predicted': 'SELECT Properties.property_name FROM Properties WHERE Properties.room_count > 1.0 OR Properties.room_count > 1.0',\n",
       " 'gold': 'SELECT property_name FROM Properties WHERE property_type_code  =  \"House\" UNION SELECT property_name FROM Properties WHERE property_type_code  =  \"Apartment\" AND room_count  >  1',\n",
       " 'predicted_parse_error': False,\n",
       " 'hardness': 'hard',\n",
       " 'exact': 0,\n",
       " 'exact (with val)': 0,\n",
       " 'partial': {'select': {'acc': 1,\n",
       "   'rec': 1,\n",
       "   'f1': 1,\n",
       "   'label_total': 1,\n",
       "   'pred_total': 1},\n",
       "  'select(no AGG)': {'acc': 1,\n",
       "   'rec': 1,\n",
       "   'f1': 1,\n",
       "   'label_total': 1,\n",
       "   'pred_total': 1},\n",
       "  'where': {'acc': 0, 'rec': 0, 'f1': 0, 'label_total': 1, 'pred_total': 2},\n",
       "  'where(no OP)': {'acc': 0,\n",
       "   'rec': 0,\n",
       "   'f1': 0,\n",
       "   'label_total': 1,\n",
       "   'pred_total': 2},\n",
       "  'where(with value)': {'acc': 0,\n",
       "   'rec': 0,\n",
       "   'f1': 0,\n",
       "   'label_total': 1,\n",
       "   'pred_total': 2},\n",
       "  'group(no Having)': {'acc': 1,\n",
       "   'rec': 1,\n",
       "   'f1': 1,\n",
       "   'label_total': 0,\n",
       "   'pred_total': 0},\n",
       "  'group': {'acc': 1, 'rec': 1, 'f1': 1, 'label_total': 0, 'pred_total': 0},\n",
       "  'order': {'acc': 1, 'rec': 1, 'f1': 1, 'label_total': 0, 'pred_total': 0},\n",
       "  'and/or': {'acc': 0, 'rec': 0, 'f1': 0, 'label_total': 1, 'pred_total': 0},\n",
       "  'IUEN': {'acc': 0, 'rec': 0, 'f1': 0, 'label_total': 1, 'pred_total': 0},\n",
       "  'keywords': {'acc': 0,\n",
       "   'rec': 0,\n",
       "   'f1': 0,\n",
       "   'label_total': 2,\n",
       "   'pred_total': 2}},\n",
       " 'exec': False}"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit88cc932d8db248f1babfdc8585b6adeb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
